| üåü **Benchmark** | üìú **Paper** | ‚ö†Ô∏è **Risk** | üìå **Subrisk** |
|------------------|-------------|-------------|----------------|
| [BBQ](https://github.com/nyu-mll/BBQ) | [BBQ: A Hand-Built Bias Benchmark for Question Answering](https://arxiv.org/abs/2110.08193) | Fairness | Decision bias |
| [KoBBQ](https://jinjh0123.github.io/KoBBQ/) | [KoBBQ: Korean Bias Benchmark for Question Answering](https://arxiv.org/abs/2307.16778) | Fairness | Decision bias |
| [TrustGPT](https://github.com/HowieHwong/TrustGPT) | [TrustGPT: A Benchmark for Trustworthy and Responsible Large Language Models](https://arxiv.org/abs/2306.11507) | Fairness | Decision bias |
| RACE | [RACE: Large-scale ReAding Comprehension Dataset From Examinations](https://arxiv.org/abs/1704.04683) | Fairness | Decision bias |
| [FairPrism](https://github.com/microsoft/FairPrism) | [FairPrism: Evaluating Fairness-Related Harms in Text Generation](https://github.com/microsoft/FairPrism/blob/main/FairPrism_paper.pdf) | Fairness | Decision bias |
| [CEAT](https://github.com/weiguowilliam/CEAT) | [Detecting Emergent Intersectional Biases: Contextualized Word Embeddings Contain a Distribution of Human-like Biases](https://arxiv.org/abs/2006.03955) | Fairness | Decision bias |
| [CrowS-Pairs](https://github.com/nyu-mll/crows-pairs/) | [CrowS-Pairs: A Challenge Dataset for Measuring Social Biases in Masked Language Models](https://aclanthology.org/2020.emnlp-main.154.pdf) | Fairness | Decision bias |
| [StereoSet](https://github.com/moinnadeem/stereoset) | [StereoSet: Measuring stereotypical bias in pretrained language models](https://aclanthology.org/2021.acl-long.416.pdf) | Fairness | Decision bias |
| [EEC](https://www.svkir.com/resources.html) | [Examining Gender and Race Bias in Two Hundred Sentiment Analysis Systems](https://arxiv.org/abs/1805.04508) | Fairness | Decision bias |
| [GAP](https://github.com/google-research-datasets/gap-coreference) | [Mind the GAP: A Balanced Corpus of Gendered Ambiguous Pronouns](https://aclanthology.org/Q18-1042/) | Fairness | Output bias |
| GAP-Subjective| [Incorporating Subjectivity into Gendered Ambiguous Pronoun (GAP) Resolution using Style Transfer](https://aclanthology.org/2022.gebnlp-1.28.pdf) | Fairness | Output bias |
| [Gender-GAP](https://github.com/facebookresearch/ResponsibleNLP?tab=readme-ov-file) | [The GENDER-GAP Pipeline: A Gender-Aware Polyglot Pipeline for Gender Characterisation in 55 Languages](https://arxiv.org/pdf/2308.16871) | Fairness | Decision bias |
| [TruthfulQA](https://github.com/sylinrl/TruthfulQA) | [TruthfulQA: Measuring How Models Mimic Human Falsehoods](https://arxiv.org/abs/2109.07958) | Fairness | Decision bias |
| [WEAT](https://github.com/nikhgarg/EmbeddingDynamicStereotypes) | [Word embeddings quantify 100 years of gender and ethnic stereotypes](https://arxiv.org/abs/1711.08412) | Fairness | Decision bias |
| [SEAT](https://github.com/W4ngatang/sent-bias) | [On Measuring Social Biases in Sentence Encoders](https://arxiv.org/abs/1903.10561) | Fairness | Decision bias |
| [HONEST](https://github.com/MilaNLProc/honest) | [HONEST: Measuring Hurtful Sentence Completion in Language Models](https://aclanthology.org/2021.naacl-main.191.pdf) | Fairness | Decision bias |
| Cul_Opponents*| [John vs. Ahmed: Debate-Induced Bias in Multilingual LLMs](https://aclanthology.org/2024.arabicnlp-1.18.pdf) | Fairness | Decision bias |
| [RedditBias](https://github.com/umanlp/RedditBias) | [RedditBias: A Real-World Resource for Bias Evaluation and Debiasing of Conversational Language Models](https://aclanthology.org/2021.acl-long.151.pdf) | Fairness | Decision bias |
| [HeteroCorpus](https://github.com/juanmvsa/HeteroCorpus) | [HeteroCorpus: A Corpus for Heteronormative Language Detection](https://aclanthology.org/2022.gebnlp-1.23.pdf) | Intellectual property | Data usage rights |
| Gender_Bias* | [Identifying and reducing gender bias in word-level language models](https://aclanthology.org/N19-3002.pdf) | Intellectual property | Copyright infringement |
| [WinoBias](https://github.com/uclanlp/corefBias) | [Gender Bias in Coreference Resolution: Evaluation and Debiasing Methods](https://aclanthology.org/N18-2003.pdf) | Intellectual property | Copyright infringement |
| [WinoBias+](https://github.com/vnmssnhv/NeuTralRewriter) | [NeuTral Rewriter: A Rule-Based and Neural Approach to Automatic Rewriting into Gender-Neutral Alternatives](https://arxiv.org/abs/2109.06105) | Intellectual property | Copyright infringement |
| [WinoGender](https://github.com/rudinger/winogender-schemas) | [Gender Bias in Coreference Resolution](https://arxiv.org/abs/1804.09301) | Intellectual property | Revealing confidential information |
| [WinoQueer](https://github.com/katyfelkner/winoqueer) | [WinoQueer: A Community-in-the-Loop Benchmark for Anti-LGBTQ+ Bias in Large Language Models](https://arxiv.org/abs/2306.15087) | Intellectual property | Revealing confidential information |
| [WinoPron](https://github.com/uds-lsv/winopron) | [WinoPron: Revisiting English Winogender Schemas for Consistency, Coverage, and Grammatical Case](https://arxiv.org/abs/2409.05653) | Intellectual property | Revealing confidential information |
| [SweWinogender](https://github.com/aidotse/superlim-baselines) | [The Swedish Winogender Dataset](https://aclanthology.org/2021.nodalida-main.52.pdf) | Intellectual property | Revealing confidential information |
| [BEC-Pro](https://github.com/marionbartl/gender-bias-BERT) | [Unmasking Contextual Stereotypes: Measuring and Mitigating BERT‚Äôs Gender Bias](https://arxiv.org/abs/2010.14534) | Intellectual property | Revealing confidential information |
| [BUG](https://github.com/SLAB-NLP/BUG) | [Collecting a Large-Scale Gender Bias Dataset for Coreference Resolution and Machine Translation](https://arxiv.org/abs/2109.03858) | Fairness | Output bias |
| [Bias-NLI](https://github.com/sunipa/On-Measuring-and-Mitigating-Biased-Inferences-of-Word-Embeddings) | [On Measuring and Mitigating Biased Inferences of Word Embeddings](https://arxiv.org/abs/1908.09369) | Fairness | Output bias |
| [PANDA](https://github.com/facebookresearch/ResponsibleNLP) | [Perturbation Augmentation for Fairer NLP](https://arxiv.org/abs/2205.12586) | Fairness | Output bias |
| [BOLD](https://github.com/amazon-science/bold) | [BOLD: Dataset and Metrics for Measuring Biases in Open-Ended Language Generation](https://arxiv.org/abs/2101.11718) | Legal compliance | Legal accountability |
| [RealToxicityPrompts](https://allenai.org/data/real-toxicity-prompts) | [RealToxicityPrompts: Evaluating Neural Toxic Degeneration in Language Models](https://arxiv.org/abs/2009.11462) | Value alignment | Hallucination |
| [HolisticBias](https://github.com/facebookresearch/ResponsibleNLP) | [‚ÄúI‚Äôm sorry to hear that‚Äù: Finding New Biases in Language Models with a Holistic Descriptor Dataset](https://arxiv.org/pdf/2205.09209) | Legal compliance | Legal accountability |
| [UnQover](https://github.com/allenai/unqover) | [UnQovering Stereotyping Biases via Underspecified Questions](https://arxiv.org/abs/2010.02428) | Legal compliance | Legal accountability |
| [CEB](https://github.com/SongW-SW/CEB) | [CEB: Compositional Evaluation Benchmark for Fairness in Large Language Models](https://arxiv.org/pdf/2407.02408) | Legal compliance | Legal accountability |
| [HarmfulQA](https://huggingface.co/datasets/declare-lab/HarmfulQA) | [Red-Teaming Large Language Models using Chain of Utterances for Safety-Alignment](https://arxiv.org/abs/2308.09662) | Fairness | Output bias |
| [CategoricalHarmfulQA](https://huggingface.co/datasets/declare-lab/CategoricalHarmfulQA) | [Language Models are Homer Simpson! Safety Re-Alignment of Fine-tuned Language Models through Task Arithmetic](https://arxiv.org/pdf/2402.11746) | Fairness | Output bias |
| [Pile](https://pile.eleuther.ai/) | [The Pile: An 800GB Dataset of Diverse Text for Language Modeling](https://arxiv.org/pdf/2101.00027) | Fairness | Output bias |
| [Detoxify](https://docs.unitary.ai/detoxify) | no paper | Fairness | Output bias |
| [HELM](https://crfm.stanford.edu/helm/latest/) | [Holistic Evaluation of Text-to-Image Models](https://arxiv.org/pdf/2311.04287) | Legal compliance | Generated content ownership |
| [DecodingTrust](https://github.com/AI-secure/DecodingTrust) | [DecodingTrust: A Comprehensive Assessment of Trustworthiness in GPT Models](https://arxiv.org/abs/2306.11698) | Fairness | Output bias |
| [GEM](https://gem-benchmark.com/) | [The GEM Benchmark: Natural Language Generation, its Evaluation and Metrics](https://aclanthology.org/2021.gem-1.10.pdf) | Misuse | Spreading disinformation |
| GEM | [GEMv2: Multilingual NLG Benchmarking in a Single Line of Code](https://arxiv.org/abs/2201.07754) | Misuse | Spreading disinformation |
| [Grep-BiasIR](https://perspectiveapi.com/) | [Grep-BiasIR: A Dataset for Investigating Gender Representation-Bias in Information Retrieval Results](https://arxiv.org/pdf/2402.06900) | Misuse | Spreading disinformation |
| [LATTE](https://huggingface.co/datasets/holistic-ai/job-fair-resume) | [Can LLMs Recognize Toxicity? Definition-Based Toxicity Metric](https://arxiv.org/abs/2406.15484) | Value alignment | Hallucination |
| JobFair| [JobFair: A Framework for Benchmarking Gender Hiring Bias in Large Language Models](https://research.google/pubs/measuring-and-reducing-gendered-correlations-in-pre-trained-models/) | Misuse | Spreading disinformation |
| [Gender-Cor*](https://github.com/facebookresearch/ResponsibleNLP) | [Measuring and Reducing Gendered Correlations in Pre-trained Models](https://scontent-ord5-1.xx.fbcdn.net/v/t39.2365-6/391609697_1000557111201317_8257723889042893875_n.pdf?_nc_cat=108&ccb=1-7&_nc_sid=3c67a6&_nc_ohc=Z2m2xt_gjrsQ7kNvgHisBWG&_nc_ht=scontent-ord5-1.xx&_nc_gid=A9rZAPHmVp3tlM-zYtqwZsC&oh=00_AYC-l8-N22ivRErXurgqkDkzjevgySxqQ9rjovUcRMRsRw&oe=67129E60) | Misuse | Improper usage |
| [ROBBIE](https://github.com/facebookresearch/ResponsibleNLP/tree/main/AdvPromptSet) | [ROBBIE: Robust Bias Evaluation of Large Generative Language Models](https://scontent-ord5-1.xx.fbcdn.net/v/t39.2365-6/391609697_1000557111201317_8257723889042893875_n.pdf?_nc_cat=108&ccb=1-7&_nc_sid=3c67a6&_nc_ohc=Z2m2xt_gjrsQ7kNvgHisBWG&_nc_ht=scontent-ord5-1.xx&_nc_gid=A9rZAPHmVp3tlM-zYtqwZsC&oh=00_AYC-l8-N22ivRErXurgqkDkzjevgySxqQ9rjovUcRMRsRw&oe=67129E60) | Multi-category | Prompt priming |
| [AdvPromptSet](https://github.com/facebookresearch/ResponsibleNLP/blob/main/holistic_bias/README.md) | [ROBBIE: Robust Bias Evaluation of Large Generative Language Models](https://scontent-ord5-1.xx.fbcdn.net/v/t39.2365-6/391609697_1000557111201317_8257723889042893875_n.pdf?_nc_cat=108&ccb=1-7&_nc_sid=3c67a6&_nc_ohc=Z2m2xt_gjrsQ7kNvgHisBWG&_nc_ht=scontent-ord5-1.xx&_nc_gid=A9rZAPHmVp3tlM-zYtqwZsC&oh=00_AYC-l8-N22ivRErXurgqkDkzjevgySxqQ9rjovUcRMRsRw&oe=67129E60) | Value alignment | Hallucination |
| [HolisticBiasR](https://github.com/ewsheng/nlg-bias) | [ROBBIE: Robust Bias Evaluation of Large Generative Language Models](https://aclanthology.org/D19-1339.pdf) | Value alignment | Hallucination |
| Regard| [The Woman Worked as a Babysitter: On Biases in Language Generation](https://arxiv.org/abs/2203.09509) | Value alignment | Hallucination |
| ToxiGen | [ToxiGen: A Large-Scale Machine-Generated Dataset for Adversarial and Implicit Hate Speech Detection](https://arxiv.org/pdf/2404.08517) | Value alignment | Hallucination |
| [UnsafeBench](https://github.com/JailbreakBench/jailbreakbench) | [UnsafeBench: Benchmarking Image Safety Classifiers on Real-World and AI-Generated Images](https://arxiv.org/abs/2404.01318) | nan | nan |
| [Online_Safety* ](https://eddyluo1232.github.io/JailBreakV28K/) | [Online Safety Analysis for LLMs: a Benchmark, an Assessment, and a Path Forward](https://arxiv.org/abs/2404.03027) | Fairness | Output bias |
| [JailbreakBench](https://github.com/usail-hkust/JailTrickBench) | [JailbreakBench: An Open Robustness Benchmark for Jailbreaking Large Language Models](https://arxiv.org/abs/2406.09324) | Fairness | Output bias |
| JailBreakV-28K | [JailBreakV-28K: A Benchmark for Assessing the Robustness of MultiModal Large Language Models against Jailbreak Attacks](https://arxiv.org/pdf/2402.05668) | Fairness | Output bias |
| [Bag of Tricks](https://github.com/llm-attacks/llm-attacks) | [Bag of Tricks: Benchmarking of Jailbreak Attacks on LLMs](https://arxiv.org/abs/2307.15043) | Fairness | Output bias |
| [Comp_Jailbreak*](https://github.com/centerforaisafety/HarmBench) | [Comprehensive Assessment of Jailbreak Attacks Against LLMs](https://arxiv.org/pdf/2402.04249) | Fairness | Output bias |
| AdvBench | [Universal and Transferable Adversarial Attacks on Aligned Language Models](https://arxiv.org/abs/2402.13457) | Fairness | Output bias |
| [HarmBench](https://github.com/LLM-Integrity-Guard/JailMine) | [HarmBench: A Standardized Evaluation Framework for Automated Red Teaming and Robust Refusal](https://arxiv.org/abs/2405.13068) | Fairness | Output bias |
| Attack_vs_Defence* | [LLM Jailbreak Attack versus Defense Techniques--A Comprehensive Study](https://arxiv.org/pdf/2406.01364) | Fairness | Output bias |
| JAILMINE | [Lockpicking LLMs: A Logit-Based Jailbreak Using Token-level Manipulation](https://arxiv.org/abs/2311.17600) | Fairness | Decision bias |
| [BELLS](https://gentellab.github.io/gentel-safe.github.io/) | [BELLS: A Framework Towards Future Proof Benchmarks for the Evaluation of LLM Safeguards](https://arxiv.org/abs/2409.19521) | Fairness | Output bias |
| [MM-SafetyBench](https://github.com/uiuc-kang-lab/InjecAgent) | [MM-SafetyBench: A Benchmark for Safety Evaluation of Multimodal Large Language Models](https://arxiv.org/abs/2403.02691) | Fairness | Output bias |
| GenTel-Safe | [GenTel-Safe: A Unified Benchmark and Shielding Framework for Defending Against Prompt Injection Attacks](https://arxiv.org/abs/2403.17218) | Fairness | Output bias |
| [InjecAgent](https://github.com/nayeon7lee/FactualityPrompt) | [InjecAgent: Benchmarking Indirect Prompt Injections in Tool-Integrated Large Language Model Agents](https://arxiv.org/pdf/2206.04624) | Fairness | Output bias |
| [DbgBench](https://github.com/shmsw25/FActScore) | [A Comprehensive Study of the Capabilities of Large Language Models for Vulnerability Detection](https://arxiv.org/pdf/2206.04624) | Fairness | Output bias |
| FactualityPrompt | [Factuality Enhanced Language Models for Open-Ended Text Generation](https://arxiv.org/abs/2306.09296) | Robustness | Prompt injection |
| [FACTSCORE](https://github.com/RUCAIBox/HaluEval) | [FACTSCORE: Fine-grained Atomic Evaluation of Factual Precision in Long Form Text Generation](https://arxiv.org/abs/2305.11747) | Robustness | Prompt injection |
| KoLA-KC| [KoLA: Carefully Benchmarking World Knowledge of Large Language Models](https://arxiv.org/abs/2307.06908) | Robustness | Prompt injection |
| [HaluEval](https://github.com/HaluEval-Wild/HaluEval-Wild) | [HaluEval: A Large-Scale Hallucination Evaluation Benchmark for Large Language Models](https://arxiv.org/pdf/2403.04307) | Robustness | Prompt injection |
| [FACTOR](https://github.com/thu-coai/Safety-Prompts) | [Generating Benchmarks for Factuality Evaluation of Language Models](https://arxiv.org/pdf/2304.10436) | Robustness | Prompt leaking |
| [HaluEval-Wild](https://github.com/GAIR-NLP/factool) | [HaluEval-Wild:Evaluating Hallucinations of Language Models in the Wild](https://arxiv.org/pdf/2307.13528) | Robustness | Prompt leaking |
| Safety-Prompts | [Safety Assessment of Chinese Large Language Models](https://arxiv.org/pdf/2304.13734) | Fairness | Output bias |
| FACTOOL | [FACTOOL: Factuality Detection in Generative AI A Tool Augmented Framework for Multi-Task and Multi-Domain Scenarios](https://arxiv.org/pdf/2404.00971) | Societal impact | Job loss |
| HELM | [Unsupervised Real-Time Hallucination Detection based on the Internal States of Large Language Models](https://arxiv.org/pdf/2305.10355) | Legal compliance | Generated content ownership |
| True-False| [The Internal State of an LLM Knows When It‚Äôs Lying](https://arxiv.org/abs/2311.16479) | Societal impact | Impact on cultural diversity |
| [FRANK](https://github.com/Anonymousanoy/FOHE) | [Understanding Factuality in Abstractive Summarization with FRANK: A Benchmark for Factuality Metrics
| [HALLUCODE](https://github.com/FuxiaoLiu/LRV-Instruction) | [Exploring and Evaluating Hallucinations in LLM-Powered Code Generation](https://arxiv.org/abs/2306.14565) | Societal impact | Impact on cultural diversity |
| [POPE](https://github.com/bcdnlp/FAITHSCORE) | [Evaluating Object Hallucination in Large Vision-Language Model](https://arxiv.org/abs/2311.01477) | Societal impact | Impact on cultural diversity |
| [RAH-Bench](https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models/tree/Evaluation) | [Mitigating Hallucination in Visual Language Models with Visual Supervision](https://arxiv.org/abs/2306.13394) | Societal impact | Impact on cultural diversity |
| FGHE | [Mitigating Fine-Grained Hallucination by Fine-Tuning Large Vision-Language Models with Caption Rewrites](https://arxiv.org/abs/2309.02301) | Societal impact | Impact on cultural diversity |
| [GAVIE](https://llava-rlhf.github.io/) | [Mitigating Hallucination in Large Multi-Modal Models via Robust Instruction Tuning](https://arxiv.org/abs/2309.14525) | Societal impact | Impact on cultural diversity |
| [Faith-Score](https://github.com/gzcch/Bingo) | [FaithScore: Fine-grained Evaluations of Hallucinations in Large Vision-Language Models](https://arxiv.org/abs/2311.03287) | Societal impact | Impact on cultural diversity |
| [MME](https://github.com/bronyayang/HallE_Control) | [MME: A Comprehensive Evaluation Benchmark for Multimodal Large Language Models](https://arxiv.org/pdf/2310.01779) | Fairness | Output bias |
| [CIEM](https://github.com/technion-cs-nlp/hallucination-mitigation) | [CIEM: Contrastive Instruction Evaluation Method for Better Instruction Tuning](https://arxiv.org/abs/2404.09971) | Transparency | Data transparency |
| [MMHal-Bench](https://github.com/tianyi-lab/HallusionBench) | [Aligning Large Multimodal Models with Factually Augmented RLHF](https://arxiv.org/abs/2310.14566) | Value alignment | Data curation |
| [BINGO](https://github.com/DILAB-KAIST/ERBench) | [Holistic Analysis of Hallucination in GPT-4V(ision): Bias and Interference Challenges](https://arxiv.org/pdf/2403.05266) | Fairness | Output bias |
| [HallE-Control](https://github.com/ECNU-ICALK/DiaHalu) | [HallE-Control: Controlling Object Hallucination in Large Multimodal Models](https://arxiv.org/pdf/2403.00896) | Fairness | Output bias |
| [WACK](https://github.com/hendryx-scale/mhal-detect) | [Constructing Benchmarks and Interventions for Combating Hallucinations in LLMs](https://arxiv.org/abs/2308.06394) | Value alignment | Data curation |
| [HallusionBench](https://iaar-shanghai.github.io/UHGEval/) | [HallusionBench: An Advanced Diagnostic Suite for Entangled Language Hallucination and Visual Illusion in Large Vision-Language Models](https://arxiv.org/pdf/2311.15296) | Value alignment | Data curation |
| [ERBench](https://github.com/manyoso/haltt4llm) | [ERBench: An Entity-Relationship based Automatically Verifiable Hallucination Benchmark for Large Language Models](https://arxiv.org/pdf/2402.10412) | Value alignment | Data curation |
| DiaHalu | [DiaHalu: A Dialogue-level Hallucination Evaluation Benchmark for Large Language Models](https://arxiv.org/html/2406.07070v1) | Value alignment | Data curation |
| M-HalDetect | [Detecting and Preventing Hallucinations in Large Vision Language Models](https://arxiv.org/pdf/2310.03003) | Value alignment | Data curation |
| [UHGEval](https://github.com/chentong0/copy-bench) | [UHGEval: Benchmarking the Hallucination of Chinese Large Language Models via Unconstrained Generation](https://linnk.ai/pdf-summarizer/) | Value alignment | Downstream retraining |
| [FEWL](https://github.com/xz-liu/SHIELD) | [Measuring and Reducing LLM Hallucination without Gold-Standard Answers](https://arxiv.org/html/2406.12975v1) | Value alignment | Hallucination |
| [HalluDial](https://anonymous.4open.science/r/DIGGER-86CE/README.md) | [HalluDial: A Large-Scale Benchmark for Automatic Dialogue-Level Hallucination Evaluation](https://arxiv.org/abs/2401.00676) | Value alignment | Hallucination |
| [Watts*](https://github.com/microsoft/private-benchmarking) | [From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference](https://arxiv.org/abs/2403.00393) | Robustness | Extraction attack |
| [COPYBENCH](https://genai.owasp.org/llmrisk/llm03-training-data-poisoning/) | [COPYBENCH: A Benchmark for Measuring Literal and Non-Literal Copyright Infringement by Language Models](https://genai.owasp.org/llmrisk/llm03-training-data-poisoning/) | Robustness | Extraction attack |
| [SHIELD](https://github.com/open-compass/LawBench/) | [SHIELD: Evaluation and Defense Strategies for Copyright Compliance in LLM Text Generation](https://arxiv.org/abs/2309.16289) | Robustness | Extraction attack |
| [DIGGER](https://github.com/HazyResearch/legalbench/) | [Digger: Detecting Copyright Content Mis-usage in Large Language Model Training](https://proceedings.neurips.cc/paper_files/paper/2023/hash/89e44582fd28ddfea1ea4dcb0ebbf4b0-Abstract-Datasets_and_Benchmarks.html) | Value alignment | Hallucination |
| [LIAR](https://github.com/CSHaitao/LexEval) | [‚ÄúLiar, Liar Pants on Fire‚Äù: A New Benchmark Dataset for Fake News Detection](http://arxiv.org/abs/2409.20288) | nan | nan |
| [TRUCE](https://github.com/zeroentropy-cc/legalbenchrag) | [TRUCE: Private Benchmarking to Prevent Contamination and Improve Comparative Evaluation of LLMs](https://www.arxiv.org/abs/2408.10343) | Fairness | Output bias |
| [https://genai.owasp.org/llmrisk/llm03-training-data-poisoning/](https://github.com/Thiqah/ArabLegalEval) | [no paper ](https://arxiv.org/abs/2408.07983) | nan | nan |
| LawBench | [LawBench: Benchmarking Legal Knowledge of Large Language Models](https://arxiv.org/abs/2407.14192) | Value alignment | Hallucination |
| [Legalbench](https://exploration-lab.github.io/IL-TUR/) | [Legalbench: A collaboratively built benchmark for measuring legal reasoning in large language models](https://arxiv.org/abs/2407.05399) | Value alignment | Hallucination |
| [LexEval](https://ipeval.github.io/) | [LexEval: A Comprehensive Chinese Legal Benchmark for Evaluating Large Language Models](https://arxiv.org/abs/2406.12386) | Value alignment | Hallucination |
| [LegalBench-RAG](https://github.com/CSHaitao/LexiLaw) | [LegalBench-RAG: A Benchmark for Retrieval-Augmented Generation in the Legal Domain](https://arxiv.org/abs/2310.05620) | Value alignment | Hallucination |
| ArabLegalEval | [ArabLegalEval: A Multitask Benchmark for Assessing Arabic Legal Knowledge in Large Language Models](https://arxiv.org/abs/2306.09237) | Value alignment | Hallucination |
| [LeKUBE](https://github.com/TingchenFu/PoisonBench) | [LeKUBE: A Legal Knowledge Update BEnchmark](https://arxiv.org/pdf/2410.08811) | Value alignment | Toxic output |
| [IL-TUR](https://github.com/agiresearch/ASB) | [IL-TUR: Benchmark for Indian Legal Text Understanding and Reasoning](https://arxiv.org/pdf/2410.02644) | Value alignment | Toxic output |
| [IPEval](https://github.com/bboylyg/BackdoorLLM) | [IPEval: A Bilingual Intellectual Property Agency Consultation Evaluation Benchmark for Large Language Models](https://arxiv.org/abs/2408.12798) | Fairness | Output bias |
| LAiW | [LAiW: A Chinese Legal Large Language Models Benchmark](https://arxiv.org/html/2406.10802v1) | Value alignment | Toxic output |
| [One_Many*](https://github.com/wang2226/Trojan-Activation-Attack) | [One Law, Many Languages: Benchmarking Multilingual Legal Reasoning for Judicial Support](https://arxiv.org/pdf/2311.09433) | Value alignment | Toxic output |
| PoisonBench | [PoisonBench: Assessing Large Language Model Vulnerability to Data Poisoning](https://arxiv.org/abs/2307.01881v1) | Explainability | Unexplainable output |
| [ASB](https://llm-access-control.github.io/) | [Agent Security Bench (ASB): Formalizing and Benchmarking Attacks and Defenses in LLM-based Agents](https://arxiv.org/abs/2310.02224) | Fairness | Output bias |
| [BackdoorLLM](https://github.com/ThuCCSLab/FigStep) | [BackdoorLLM: A Comprehensive Benchmark for Backdoor Attacks on Large Language Models](https://arxiv.org/pdf/2311.05608) | Fairness | Output bias |
| KGPA | [KGPA: Robustness Evaluation for Large Language Models via Cross-Domain Knowledge Graphs](https://arxiv.org/pdf/2308.06932) | Fairness | Output bias |
| Trojan Activation Attack | [Trojan Activation Attack: Red-Teaming Large Language Models using Activation Steering for Safety-Alignment](https://arxiv.org/abs/2402.16389) | Fairness | Output bias |
| [ProPILE ](https://hf.co/spaces/kellycyy/CulturalBench) | [ProPILE: Probing Privacy Leakage in Large Language Models](https://arxiv.org/pdf/2410.02677) | Value alignment | Hallucination |
| PrivQA | [Can Language Models be Instructed to Protect Personal Information?](https://arxiv.org/pdf/2409.11404) | Value alignment | Hallucination |
| FigStep | [FigStep: Jailbreaking Large Vision-language Models via Typographic Visual Prompts](https://arxiv.org/pdf/2404.10508) | Fairness | Decision bias |
| [DIVAS](https://gentellab.github.io/gentel-safe.github.io/) | [DIVAS: An LLM-based End-to-End Framework for SoC Security Analysis and Policy-based Protection](https://arxiv.org/pdf/2409.19521) | Fairness | Output bias |
| [MOZIP](https://github.com/JieyuZ2/wrench) | [PatentGPT: A Large Language Model for Intellectual Property](https://arxiv.org/abs/2109.11377) | Value alignment | Toxic output |
| [CULTURALBENCH](https://github.com/BHui97/PLeak) | [CulturalBench: a Robust, Diverse and Challenging Benchmark on Measuring the (Lack of) Cultural Knowledge of LLMs](https://arxiv.org/abs/2405.06823) | Value alignment | Toxic output |
| AraDiCE| [AraDiCE: Benchmarks for Dialectal and Cultural Capabilities in LLMs](no link) | Value alignment | Toxic output |
| LABE | [White Men Lead, Black Women Help? Benchmarking Language Agency Social Biases in LLMs](no link) | Value alignment | Over or under reliance |
| GenTel-Safe | [GenTel-Safe: A Unified Benchmark and Shielding Framework for Defending Against Prompt Injection Attacks](no link) | Value alignment | Over or under reliance |
| WRENCH| [WRENCH: A Comprehensive Benchmark for Weak Supervision](no link) | Fairness | Output bias |
| PLeak| [PLeak: Prompt Leaking Attacks against Large Language Model Applications](no link) | Fairness | Output bias |
