# AI Risk Atlas
**Last Updated: 2025-02-07**

Explore this atlas to understand some of the risks of working with generative AI, foundation models, and machine learning models.

## Risk Categories
Risks are categorized with one of these tags:
- **Traditional AI risks** (applies to traditional models as well as generative AI)
- **Risks amplified by generative AI** (might also apply to traditional models)
- **New risks specifically associated with generative AI**

## Training Data Risks
### Transparency
- **Lack of training data transparency** *(Amplified)*
- **Uncertain data provenance** *(Amplified)*

### Data Laws
- **Data usage restrictions** *(Traditional)*
- **Data acquisition restrictions** *(Amplified)*
- **Data transfer restrictions** *(Traditional)*

### Privacy
- **Personal information in data** *(Traditional)*
- **Data privacy rights alignment** *(Amplified)*
- **Reidentification** *(Traditional)*

### Fairness
- **Data bias** *(Amplified)*

### Intellectual Property
- **Data usage rights restrictions** *(Amplified)*
- **Confidential information in data** *(Amplified)*

### Accuracy
- **Data contamination** *(Amplified)*
- **Unrepresentative data** *(Traditional)*

### Value Alignment
- **Improper retraining** *(Amplified)*
- **Improper data curation** *(Amplified)*

### Robustness
- **Data poisoning** *(Traditional)*

## Inference Risks
### Robustness
- **Prompt injection attack** *(Specific)*
- **Extraction attack** *(Amplified)*
- **Evasion attack** *(Amplified)*
- **Prompt leaking** *(Specific)*

### Multi-category
- **Jailbreaking** *(Specific)*
- **Prompt priming** *(Specific)*

### Privacy
- **Membership inference attack** *(Amplified)*
- **Attribute inference attack** *(Amplified)*
- **Personal information in prompt** *(Specific)*

### Intellectual Property
- **Confidential data in prompt** *(Specific)*
- **IP information in prompt** *(Specific)*

### Accuracy
- **Poor model accuracy** *(Amplified)*

## Output Risks
### Misuse
- **Non-disclosure** *(Specific)*
- **Improper usage** *(Amplified)*
- **Spreading toxicity** *(Specific)*
- **Dangerous use** *(Specific)*
- **Nonconsensual use** *(Amplified)*
- **Spreading disinformation** *(Specific)*

### Value Alignment
- **Incomplete advice** *(Specific)*
- **Harmful code generation** *(Specific)*
- **Over- or under-reliance** *(Amplified)*
- **Toxic output** *(Specific)*
- **Harmful output** *(Specific)*

### Intellectual Property
- **Copyright infringement** *(Specific)*
- **Revealing confidential information** *(Amplified)*

### Explainability
- **Inaccessible training data** *(Amplified)*
- **Untraceable attribution** *(Amplified)*
- **Unexplainable output** *(Amplified)*
- **Unreliable source attribution** *(Specific)*

### Robustness
- **Hallucination** *(Specific)*

### Fairness
- **Output bias** *(Specific)*
- **Decision bias** *(Traditional)*

### Privacy
- **Exposing personal information** *(Amplified)*

## Non-Technical Risks
### Legal Compliance
- **Model usage rights restrictions** *(Traditional)*
- **Legal accountability** *(Amplified)*
- **Generated content ownership and IP** *(Specific)*

### Governance
- **Lack of system transparency** *(Traditional)*
- **Unrepresentative risk testing** *(Amplified)*
- **Incomplete usage definition** *(Specific)*
- **Lack of data transparency** *(Amplified)*
- **Incorrect risk testing** *(Amplified)*
- **Lack of model transparency** *(Traditional)*
- **Lack of testing diversity** *(Amplified)*

### Societal Impact
- **Impact on cultural diversity** *(Specific)*
- **Impact on education: plagiarism** *(Specific)*
- **Impact on Jobs** *(Amplified)*
- **Impact on affected communities** *(Traditional)*
- **Impact on education: bypassing learning** *(Specific)*
- **Impact on the environment** *(Amplified)*
- **Human exploitation** *(Amplified)*
- **Impact on human agency** *(Amplified)*
