# 📊 BenchmarkCards: Large Language Model and Risk Reporting 📊

Welcome to the **BenchmarkCards** repository! This is where we introduce a structured framework for documenting benchmarks used in large language models (LLMs). Our goal? To make benchmarking LLMs easier, transparent, and more informative by focusing on risks and key metrics.  🚀

---

## 📄 Abstract 

LLMs are powerful tools—but with great power comes great responsibility. These models can also pose risks if not evaluated thoroughly. 

**BenchmarkCards** provides a structured way to document benchmarks for LLMs. This framework captures essential details like datasets, metrics, and risks, helping researchers make informed decisions when choosing benchmarks for model evaluations. 

In other words, we’re not telling you how to measure results, but how to document the essential properties of benchmarks. With **BenchmarkCards**, evaluating the risks and transparency of LLM benchmarks is easier, making it more consistent and reproducible.

---

## 🛠️ How to Use this Repository

### 🔗 Key Contents

- **BenchmarkCard_Template.md**: 📋 A handy template for creating your very own BenchmarkCard for LLM documentation. Keep it consistent and organized!
- **ComparisonBenchmarks.md**: ⚖️ Curious how different benchmarks stack up? This file compares various benchmarks used for LLM risk evaluation.
- **Benchmarks_and_Risk_Table.md**: 🗂 A detailed table with benchmarks found in literature, including key references.
- **Benchmark_Network.md**: 🌐 A visual network that shows how benchmarks connect and relate, especially when it comes to evaluating LLM risks.



