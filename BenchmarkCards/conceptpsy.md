# ConceptPsy

## üìä Benchmark Details

**Name**: ConceptPsy

**Overview**: ConceptPsy is a comprehensive benchmark designed to evaluate the complex reasoning and knowledge of Chinese Large Language Models (LLMs) in the field of psychology, encompassing 12 core subjects and 1,383 concepts generated from official exams.

**Data Type**: multiple-choice questions

**Domains**:
- Natural Language Processing

**Languages**:
- Chinese

**Similar Benchmarks**:
- CMMLU
- C-EVAL

**Resources**:
- [Resource](https://arxiv.org/abs/2311.09861)

## üéØ Purpose and Intended Users

**Goal**: To provide a comprehensive evaluation of LLMs' knowledge and reasoning abilities in psychology, addressing concept bias present in previous benchmarks.

**Target Audience**:
- ML Researchers
- Psychology Professionals
- Model Developers

**Tasks**:
- Question Answering

**Limitations**: N/A

## üíæ Data

**Source**: Generated questions based on concepts from the National Post-graduate Entrance Examination in China, validated by psychology professionals.

**Size**: 4,573 questions

**Format**: JSON

**Annotation**: Questions validated by professional psychologists.

## üî¨ Methodology

**Methods**:
- Human evaluation
- Automated metrics

**Metrics**:
- Accuracy

**Calculation**: Metrics are calculated as the average accuracy across different conceptual topics evaluated.

**Interpretation**: Higher accuracy scores indicate better understanding and reasoning ability across psychology concepts.

**Validation**: Reviewed by psychology professionals for content accuracy.

## ‚ö†Ô∏è Targeted Risks

**Risk Categories**:
- Bias
- Accuracy

**Atlas Risks**:
- **Fairness**: Data bias, Output bias
- **Accuracy**: Unrepresentative data

**Demographic Analysis**: N/A

## üîí Ethical and Legal Considerations

**Privacy And Anonymity**: Not Applicable

**Data Licensing**: Not Applicable

**Consent Procedures**: Not Applicable

**Compliance With Regulations**: Not Applicable
