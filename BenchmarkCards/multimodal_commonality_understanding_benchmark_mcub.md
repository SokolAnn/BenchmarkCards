# Multimodal Commonality Understanding Benchmark (MCUB)

## üìä Benchmark Details

**Name**: Multimodal Commonality Understanding Benchmark (MCUB)

**Overview**: MCUB is a benchmark designed to evaluate the capability of Multimodal Large Language Models to identify commonalities among input entities from diverse modalities and select the most appropriate answer from four given candidates. MCUB-4 includes inputs from four modalities (image, video, audio, point cloud) and MCUB-3 consists of subsets with inputs from any three of these modalities. MCUB is created by leveraging GPT-4 to generate questions, options, and correct answers from groups of modality captioning data selected by caption similarity.

**Data Type**: multimodal (image, video, audio, point cloud) - multiple-choice question-answer pairs

**Domains**:
- Natural Language Processing
- Computer Vision
- Audio Processing

**Similar Benchmarks**:
- X-InstructBLIP
- MME
- MUSIC-A VQA
- A VQA
- ModelNet40
- Objaverse

**Resources**:
- [GitHub Repository](https://github.com/THUNLP-MT/ModelCompose)
- [Resource](https://arxiv.org/abs/2402.12750)

## üéØ Purpose and Intended Users

**Goal**: To measure the ability of models to identify commonalities among input entities from diverse modalities and select the most appropriate answer from four given candidates, facilitating research in model composition for Multimodal Large Language Models.

**Target Audience**:
- ML Researchers

**Tasks**:
- Question Answering (Multiple-choice)
- Multimodal Commonality Understanding

**Limitations**: Our exploration is restricted to four commonly used modalities (image, audio, video, point cloud), omitting a comprehensive examination across the entire spectrum of potential modalities. Additionally, our proposed approach has been tested on models of specific sizes, leaving the applicability of the methods on larger-scale MLLMs an open question for future research.

## üíæ Data

**Source**: Generated from captioning datasets per modality: COCO2017 val set (image), MSRVTT test set (video), AudioCaps test set (audio), Cap3D (3000 subset) paired with Objaverse point clouds (point cloud). Groups of inputs were selected by averaging caption similarities computed with all-MiniLM-L6-v2 and GPT-4 was used to generate questions, options, and correct answers.

**Size**: N/A

**Format**: Multiple-choice QA entries: multimodal inputs (image, video, audio, point cloud), a question, four candidate options, and a correct answer (MCUB-3 and MCUB-4 variants defined by modality combinations).

**Annotation**: Questions, options, and correct answers were generated by GPT-4 prompted with in-context examples. Candidate groups were selected based on averaged caption similarity computed by all-MiniLM-L6-v2.

## üî¨ Methodology

**Methods**:
- Automated evaluation on MCUB and existing multimodal benchmarks (zero-shot evaluation across modality combinations)

**Calculation**: N/A

**Interpretation**: N/A

**Baseline Results**: Selected reported results from the paper: MCUB results (Table 3) - ImageBind-LLM: MCUB-3 32.95, MCUB-4 32.93; X-InstructBLIP: 29.30, 27.94; Proj-only: 44.15, 43.00; NaiveMC: 54.70, 54.03; DAMC: 59.80, 60.08. (Additional baseline and task results are provided in the paper for MUSIC-A VQA, A VQA, ModelNet40, Objaverse, and MME.)

**Validation**: MCUB entries were formed by randomly selecting groups of inputs from each modality and retaining groups with the highest semantic similarity (averaged caption similarities via all-MiniLM-L6-v2). GPT-4 was then prompted with in-context examples to generate questions, options, and correct answers. No further validation procedure for MCUB is described beyond these steps in the main text (see Appendix C for more details).

## ‚ö†Ô∏è Targeted Risks

**Atlas Risks**:
No specific atlas risks defined

## üîí Ethical and Legal Considerations

**Privacy And Anonymity**: Not Applicable

**Data Licensing**: Not Applicable

**Consent Procedures**: Not Applicable

**Compliance With Regulations**: Not Applicable
