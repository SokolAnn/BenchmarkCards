# SD-Eval (Spoken Dialogue Evaluation)

## 📊 Benchmark Details

**Name**: SD-Eval (Spoken Dialogue Evaluation)

**Overview**: SD-Eval is a benchmark dataset aimed at the multidimensional evaluation of spoken dialogue understanding and generation, focusing on paralinguistic and environmental information.

**Data Type**: utterances

**Domains**:
- Natural Language Processing

**Languages**:
- English

**Resources**:
- [GitHub Repository](https://github.com/amphionspace/SD-Eval)

## 🎯 Purpose and Intended Users

**Goal**: To advance the development of more empathetic and intelligent spoken dialogue systems that can generate appropriate responses based on paralinguistic and environmental information.

**Target Audience**:
- ML Researchers
- Model Developers

**Tasks**:
- Dialogue Understanding
- Dialogue Generation

**Limitations**: N/A

## 💾 Data

**Source**: Aggregated from eight public datasets focused on speech interaction with emotional and contextual variations.

**Size**: 7,303 utterances, 8.76 hours of speech data

**Format**: audio

**Annotation**: Generated responses based on paralinguistic and environmental information

## 🔬 Methodology

**Methods**:
- Human evaluation
- Automated metrics
- LLM-based evaluation

**Metrics**:
- BLEU Score
- ROUGE-L
- Human Evaluator Scores

**Calculation**: Metrics calculated based on automated evaluations and comparisons with human judgment.

**Interpretation**: Higher scores indicate better alignment with human understandings of responses in context.

**Validation**: Evaluation through subjective and objective measures

## ⚠️ Targeted Risks

**Risk Categories**:
- Accuracy
- Fairness
- Robustness

**Atlas Risks**:
No specific atlas risks defined

## 🔒 Ethical and Legal Considerations

**Privacy And Anonymity**: Not Applicable

**Data Licensing**: Not Applicable

**Consent Procedures**: Not Applicable

**Compliance With Regulations**: Not Applicable
