# Large Language Models are not Fair Evaluators

## üìä Benchmark Details

**Name**: Large Language Models are not Fair Evaluators

**Overview**: This paper uncovers systematic bias in the evaluation paradigm when using large language models (LLMs) as referees to score and compare responses generated by other models. It identifies that the quality ranking can be skewed by simply altering the order of responses presented to the evaluator, leading to unreliable evaluations. The authors propose a novel calibration framework with three strategies to mitigate this bias.

**Data Type**: Comparative evaluation of AI responses

**Domains**:
- Natural Language Processing
- AI Evaluator Bias

**Languages**:
- English

**Similar Benchmarks**:
- Vicuna Benchmark
- MT-Bench

**Resources**:
- [GitHub Repository](https://github.com/i-Eval/FairEval)

## üéØ Purpose and Intended Users

**Goal**: To reveal the positional bias in LLM evaluation and provide solutions to mitigate it.

**Target Audience**:
- Researchers in AI and NLP
- Developers of LLMs
- AI ethics researchers

**Tasks**:
- Evaluating AI assistant performance
- Understanding evaluator biases
- Improving LLM evaluation methodologies

**Limitations**: The proposed methods may not generalize beyond the tested models and benchmarks.

**Out of Scope Uses**:
- Real-time evaluation of untrained models
- Evaluation outside of controlled conditions

## üíæ Data

**Source**: Vicuna Benchmark

**Size**: 80 examples with 9 distinct question categories

**Format**: Question and answer pairs with annotations

**Annotation**: Manual 'win/tie/lose' outcomes of responses annotated by experts

## üî¨ Methodology

**Methods**:
- Multiple Evidence Calibration (MEC)
- Balanced Position Calibration (BPC)
- Human-in-the-Loop Calibration (HITLC)

**Metrics**:
- Accuracy
- Kappa correlation coefficient
- Conflict Rate

**Calculation**: Average score based on multiple evaluations and human annotations

**Interpretation**: Higher scores indicate better alignment with human judgments

**Baseline Results**: Performance compared to standard evaluation methods (VANILLA)

**Validation**: Methods validated against human annotations across various tasks

## ‚ö†Ô∏è Targeted Risks

**Risk Categories**:
- Evaluation Bias
- Positional Bias
- Evaluation Reliability

**Atlas Risks**:
- **Fairness**: Output bias
- **Robustness**: Prompt injection attack
- **Explainability**: Untraceable attribution

**Demographic Analysis**: Not specifically provided

**Potential Harm**: Potential reliance on biased evaluations leading to inaccurate model assessments.

## üîí Ethical and Legal Considerations

**Privacy And Anonymity**: Not Applicable

**Data Licensing**: Not Applicable

**Consent Procedures**: Not Applicable

**Compliance With Regulations**: Not Applicable
