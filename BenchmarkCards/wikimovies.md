# WIKIMOVIES

## üìä Benchmark Details

**Name**: WIKIMOVIES

**Overview**: WIKIMOVIES is a QA dataset in the movie domain that contains raw text alongside a preprocessed KB, designed to be answerable by using either a perfect KB (based on OMDb), Wikipedia pages or an imperfect KB obtained through running an engineered IE pipeline on those pages. It contains ‚âà100k questions and was constructed to allow measuring the performance of QA systems when the knowledge source is switched from a KB to unstructured documents and to analyze performance by question type.

**Data Type**: question-answering pairs; raw Wikipedia articles (text); knowledge base triples (graph)

**Domains**:
- Natural Language Processing

**Similar Benchmarks**:
- WIKIQA
- TRECQA
- SimpleQuestions
- WebQuestions

**Resources**:
- [Resource](http://fb.ai/babi)
- [Resource](https://arxiv.org/abs/1606.03126)

## üéØ Purpose and Intended Users

**Goal**: To provide an analysis tool to measure and compare QA system performance when the knowledge source is switched between a human-annotated KB, an automatically extracted KB (IE), and raw Wikipedia documents; and to provide ample training examples and the ability to break down results by question type.

**Tasks**:
- Question Answering

**Limitations**: For some questions there can be multiple correct answers.

## üíæ Data

**Source**: Documents: Wikipedia articles for movies identified by title match to OMDb; KB: triples constructed from OMDb and MovieLens metadata; IE: triples produced by running automatic information extraction including Stanford CoreNLP coreference resolution and SENNA semantic role labeling on Wikipedia pages. Question templates were derived from SimpleQuestions and entities substituted from the KB.

**Size**: Approximately 100,000 question-answer pairs total; split into ‚âà96,000 training, 10,000 development, and 10,000 test examples; ‚âà17,000 Wikipedia documents (movies); KB contains ‚âà43,000 entities, ‚âà10,000 related actors, ‚âà6,000 directors.

**Format**: N/A

**Annotation**: Question-answer pairs generated by substituting entities into human-authored SimpleQuestions question templates; KB entries from OMDb and MovieLens metadata; IE triples produced automatically using Stanford CoreNLP coreference resolution and SENNA semantic role labeling and subsequent cleaning/lemmatization.

## üî¨ Methodology

**Methods**:
- Automated metrics (hits@1, Mean Average Precision (MAP), Mean Reciprocal Rank (MRR))
- Model-based evaluation (compare performance of multiple models: KB QA system, Supervised Embeddings, Memory Network, Key-Value Memory Network)

**Metrics**:
- Hits@1 (top-1 accuracy)
- Mean Average Precision (MAP)
- Mean Reciprocal Rank (MRR)

**Calculation**: Hits@1: accuracy of the top hit (single answer) over all possible answers (all entities), reported in percent. MAP and MRR: mean average precision and mean reciprocal rank of the ranked set of candidate answers as used for answer sentence selection.

**Interpretation**: Higher Hits@1 (percentage) indicates better top-answer accuracy. Higher MAP and MRR indicate better ranked-answer performance.

**Baseline Results**: WIKIMOVIES (test, % hits@1): Bordes et al. (2014) QA system: KB 93.5, IE 56.5, Doc N/A; Supervised Embeddings: KB 54.4, IE 54.4, Doc 54.4; Memory Network: KB 78.5, IE 63.4, Doc 69.9; Key-Value Memory Network: KB 93.9, IE 68.3, Doc 76.2. WIKIQA (test): Key-Value Memory Network MAP 0.7069, MRR 0.7265 (other model scores reported in paper).

**Validation**: Questions split into disjoint training, development and test sets (‚âà96k / 10k / 10k). Hyperparameters and memory representations were optimized on the development set. For WIKIQA, word vectors were pre-trained and dropout and exact-match features tuned on the development set.

## ‚ö†Ô∏è Targeted Risks

**Atlas Risks**:
No specific atlas risks defined

## üîí Ethical and Legal Considerations

**Privacy And Anonymity**: Not Applicable

**Data Licensing**: Not Applicable

**Consent Procedures**: Not Applicable

**Compliance With Regulations**: Not Applicable
