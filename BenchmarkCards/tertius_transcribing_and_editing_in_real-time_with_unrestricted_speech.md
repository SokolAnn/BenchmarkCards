# TERTiUS (Transcribing and Editing in Real-Time with Unrestricted Speech)

## üìä Benchmark Details

**Name**: TERTiUS (Transcribing and Editing in Real-Time with Unrestricted Speech)

**Overview**: Introduces the task of interactive dictation, in which users interleave verbatim dictation and open-ended natural language editing commands in a single uninterrupted speech stream, and introduces a new dataset (TERTiUS) and data-collection interface to support training and evaluation of systems that incrementally segment speech into dictation vs. command, normalize command utterances, and interpret commands as either edited document states or small text-editing programs.

**Data Type**: audio and text: speech audio (recorded demonstrator speech), ASR transcripts (partial and final), normalized utterances, gold document states (pre- and post-state), and annotated edit programs

**Domains**:
- Natural Language Processing
- Speech Recognition
- Accessibility

**Languages**:
- English

**Similar Benchmarks**:
- Dragon NaturallySpeaking (DNS)
- Enron Email Dataset

**Resources**:
- [Resource](https://aka.ms/tertiustask)
- [Resource](https://beta.openai.com/)
- [Resource](https://www.nuance.com/asset/en_us/collateral/dragon/command-cheat-sheet/ct-dragon-naturally-speaking-en-us.pdf)

## üéØ Purpose and Intended Users

**Goal**: To enable and evaluate systems that support interactive dictation: real-time transcription plus open-ended spoken editing commands, requiring incremental segmentation and interpretation; provide a dataset (TERTiUS) and data-collection interface to bootstrap such systems.

**Tasks**:
- Segmentation (partitioning transcripts into dictation vs. command segments)
- Text Normalization (ASR repair / normalization of command utterances)
- Interpretation (predicting edited document state or predicting an edit program)
- Speech Recognition (ASR results as part of the dataset)

**Limitations**: TERTiUS is a pilot dataset. The test set is not large enough to support reliable dialogue-level evaluation metrics. Inter-annotator agreement measurements are not reported. The demonstration collection setting differs from test-time usage (gold trajectories), which may introduce data biases. All demonstrators and annotators were native English speakers; dataset currently only English and supports unformatted plain text.

## üíæ Data

**Source**: Collected via a novel Wizard-of-Oz data-collection interface: demonstrators spoke to Microsoft Speech Services (MSS) ASR while demonstrating gold segmentations, normalized command utterances, and gold document state updates. Tasks used during collection included Replicate doc (replicating emails from the Enron Email Dataset), Elaborate doc, and Replicate segment.

**Size**: Eleven demonstrators collected 1,372 interactive dictation trajectories; dataset contains 4,184 demonstrated segments in total (table totals). Two annotators annotated programs for 868 commands. Train/validation/test split: 991 training trajectories (3,199 demonstrated segments), 173 validation trajectories (562 segments), 156 test trajectories (423 segments).

**Format**: N/A

**Annotation**: Gold segmentations, normalized utterances, and gold state updates were demonstrated by human demonstrators during collection. Programs for commands were annotated post-hoc by human annotators for 868 commands; the remaining programs were auto-generated by GPT3 (per paper).

## üî¨ Methodology

**Methods**:
- Human demonstration / Wizard-of-Oz data collection
- Automated model evaluation (segmentation, normalization, interpretation)
- Model-based evaluation using fine-tuned T5 and prompted GPT3
- Runtime / latency measurement (seconds per example)

**Metrics**:
- Exact match (EM) for segmentation and states
- Macro-averaged labeled F1 for segmentation
- Program Exact Match (Program EM)
- ASR Repair Exact Match (ASR Repair EM)
- Runtime (seconds per example)

**Calculation**: Segmentation EM returns 0 or 1 depending on whether the entire labeled segmentation of the final transcript U is correct. Macro-averaged labeled F1 considers labeled segments equal if they have the same start and end points in U and the same label (dictation or command). State EM measures exact string match between predicted and gold post-states (cursor position disregarded). Program EM measures exact match between predicted program and gold program. ASR Repair EM measures exact match between predicted repaired utterance and ground-truth utterance.

**Interpretation**: Higher EM indicates better reconstruction of the gold post-state or program. The paper highlights a trade-off between accuracy and latency: larger/prompted GPT3 models achieve higher State EM but incur much higher runtime compared to fine-tuned T5. Generating end states directly is often more accurate but slower than predicting short programs.

**Baseline Results**: Segmentation (T5): Macro-averaged labeled F1 90.9%, Segmentation EM 85.3%, runtime 0.097 s/it. Joint ASR repair + interpretation (T5): Program EM 28.3%, State EM 29.5%, runtime approx. 1.28s (prog) / 3.46s (state). Joint ASR repair + interpretation (GPT3 text-davinci-003): Program EM 38.6% (isolated/interp numbers vary in paper), State EM 55.1%, runtime approx. 5.32s (prog) / 6.92s (state). Isolated ASR Repair EM: T5 47.3%, GPT3 70.7%. Isolated Interpretation State EM: T5 33.7%, GPT3 54.2%. (All values reported in paper tables.)

**Validation**: Dataset split into training (991 trajectories, 3,199 segments), validation (173 trajectories, 562 segments), and test (156 trajectories, 423 segments). Segmentation evaluation constructs short transcripts by concatenating up to 4 consecutive gold segments from full transcripts; evaluation is performed on transcripts ending in final ASR results. Evaluation metrics and procedures are described for isolated and joint evaluations of components (segmentation, normalization, interpretation).

## ‚ö†Ô∏è Targeted Risks

**Risk Categories**:
- Fairness
- Societal Impact

**Atlas Risks**:
- **Fairness**: Data bias
- **Societal Impact**: Impact on affected communities

**Demographic Analysis**: All demonstrators and annotators were native English speakers; dataset is currently only English.

**Potential Harm**: The paper states: "The fact that all speakers in our dataset were native speakers of American English could contribute to exacerbating the already present disparity in usability for English vs. non-English speakers."

## üîí Ethical and Legal Considerations

**Privacy And Anonymity**: Not Applicable

**Data Licensing**: Not Applicable

**Consent Procedures**: Demonstrators were trained to use the interface and were told during training how their data would be used (per paper).

**Compliance With Regulations**: Not Applicable
