# LLM-Coordination Benchmark

## 📊 Benchmark Details

**Name**: LLM-Coordination Benchmark

**Overview**: The LLM-Coordination Benchmark is introduced to analyze LLMs in the context of Pure Coordination Settings, evaluating their capabilities in Agentic Coordination and Coordination Question Answering (CoordQA).

**Data Type**: question-answering pairs

**Domains**:
- Natural Language Processing

**Languages**:
- English

**Resources**:
- [GitHub Repository](https://github.com/eric-ai-lab/llm_coordination)

## 🎯 Purpose and Intended Users

**Goal**: To evaluate and analyze the multi-agent coordination abilities of Large Language Models (LLMs) in pure coordination games.

**Target Audience**:
- ML Researchers
- AI Practitioners

**Tasks**:
- Agentic Coordination
- Coordination Question Answering

**Limitations**: N/A

## 💾 Data

**Source**: Manually curated edge cases from pure coordination games, specifically selected scenarios for evaluation.

**Size**: 198 unique questions

**Format**: N/A

**Annotation**: Manually developed and labeled.

## 🔬 Methodology

**Methods**:
- Automated metrics

**Metrics**:
- Accuracy

**Calculation**: Calculated based on the total scores achieved by agents and their performances on specific tasks.

**Interpretation**: High scores indicate better coordination and reasoning capabilities.

**Validation**: Comparison against RL baselines and analysis through self-play and cross-play settings.

## ⚠️ Targeted Risks

**Risk Categories**:
- Bias
- Accuracy
- Fairness

**Atlas Risks**:
- **Fairness**: Data bias
- **Accuracy**: Unrepresentative data

**Demographic Analysis**: N/A

## 🔒 Ethical and Legal Considerations

**Privacy And Anonymity**: Not Applicable

**Data Licensing**: Not Applicable

**Consent Procedures**: Not Applicable

**Compliance With Regulations**: Not Applicable
