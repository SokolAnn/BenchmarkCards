# RULEARN

## 📊 Benchmark Details

**Name**: RULEARN

**Overview**: RULEARN is a novel benchmark to assess the rule-learning abilities of LLM agents in interactive settings through strategic interactions with simulated environments to gather observations and solve complex problems.

**Data Type**: text-based puzzles

**Domains**:
- Natural Language Processing

**Languages**:
- English

**Resources**:
- [GitHub Repository](https://github.com/KaiyuHe998/RULEARN_IDEA)

## 🎯 Purpose and Intended Users

**Goal**: The goal of the benchmark is to provide a challenging resource for evaluating the rule-learning capabilities of LLM agents in fully interactive environments.

**Target Audience**:
- ML Researchers
- Domain Experts

**Tasks**:
- Rule Learning

**Limitations**: N/A

## 💾 Data

**Source**: 300 high-quality manually created puzzles with hidden rules set in a text-based environment.

**Size**: 300 puzzles

**Format**: JSON

**Annotation**: Manually created patterns and rules.

## 🔬 Methodology

**Methods**:
- Human evaluation
- Automated metrics

**Metrics**:
- Success rate
- Action efficiency

**Calculation**: Metrics are calculated based on the success rate of LLM agents in solving puzzles over benchmark interactions.

**Interpretation**: Higher success rates indicate better rule-learning capabilities.

**Baseline Results**: Compared against baseline agents without the proposed IDEA reasoning framework.

**Validation**: Evaluated through experiments with human participants and various LLMs.

## ⚠️ Targeted Risks

**Risk Categories**:
- Bias
- Safety
- Robustness

**Atlas Risks**:
No specific atlas risks defined

**Demographic Analysis**: N/A

## 🔒 Ethical and Legal Considerations

**Privacy And Anonymity**: All data in RULEARN contains no personal or sensitive information.

**Data Licensing**: Not Applicable

**Consent Procedures**: Informed consent was obtained from all human participants.

**Compliance With Regulations**: The project is approved by the Institutional Review Board (IRB).
