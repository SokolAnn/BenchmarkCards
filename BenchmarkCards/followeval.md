# FollowEval

## üìä Benchmark Details

**Name**: FollowEval

**Overview**: FollowEval is a benchmark designed to evaluate the instruction-following capabilities of large language models (LLMs) across diverse dimensions such as string manipulation, commonsense reasoning, logical reasoning, spatial reasoning, and adherence to response constraints. It features 200 manually curated test instances in English and Chinese.

**Data Type**: text

**Domains**:
- Natural Language Processing

**Languages**:
- English
- Chinese

**Resources**:
- [Resource](https://arxiv.org/abs/2311.09829)

## üéØ Purpose and Intended Users

**Goal**: To provide a comprehensive evaluation of the instruction-following capabilities of LLMs in both English and Chinese.

**Target Audience**:
- ML Researchers
- Model Developers

**Tasks**:
- Text Classification
- Logical Reasoning

**Limitations**: N/A

## üíæ Data

**Source**: Manually curated by human experts.

**Size**: 200 examples

**Format**: N/A

**Annotation**: Manual curation and regex-based verification.

## üî¨ Methodology

**Methods**:
- Human evaluation
- Automated metrics

**Metrics**:
- Accuracy

**Calculation**: Accuracy is calculated as the proportion of correctly answered test instances.

**Interpretation**: Higher accuracy indicates better instruction-following capabilities of the LLMs.

**Baseline Results**: Humans achieved 100% accuracy; GPT-4 is the top performer among evaluated models.

**Validation**: N/A

## ‚ö†Ô∏è Targeted Risks

**Risk Categories**:
- Accuracy

**Atlas Risks**:
No specific atlas risks defined

**Demographic Analysis**: N/A

**Potential Harm**: N/A

## üîí Ethical and Legal Considerations

**Privacy And Anonymity**: Not Applicable

**Data Licensing**: Not Applicable

**Consent Procedures**: Not Applicable

**Compliance With Regulations**: Not Applicable
