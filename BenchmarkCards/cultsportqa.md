# CultSportQA

## üìä Benchmark Details

**Name**: CultSportQA

**Overview**: CultSportQA is a comprehensive benchmark designed to evaluate language models‚Äô understanding of traditional sports across 60 countries and 6 continents, encompassing four distinct cultural categories. It contains 33,000 multiple-choice questions (MCQs) across text and image modalities, categorized into history-based, rule-based, and scenario-based types.

**Data Type**: question-answering pairs

**Domains**:
- Natural Language Processing

**Languages**:
- English
- Chinese
- Hindi
- Bengali
- Arabic
- Amharic
- Thai
- Indonesian
- Urdu
- French
- German
- Italian

**Similar Benchmarks**:
- SportQA
- SportU

**Resources**:
- [GitHub Repository](https://github.com/M-Groot7/CultSportQA)

## üéØ Purpose and Intended Users

**Goal**: The primary objective of the CultSportQA benchmark is to assess AI models' understanding of culturally significant traditional sports.

**Target Audience**:
- ML Researchers
- Industry Practitioners
- Model Developers
- Domain Experts

**Tasks**:
- Question Answering

**Limitations**: While this study represents one of the most comprehensive evaluations of language models in the context of traditional sports, it acknowledges limitations such as the limited geographic scope and representation of traditional sports.

## üíæ Data

**Source**: Data sources include Wikipedia, National Heritage and Sports Boards, Local Sports Blogs, Cultural Journals, News Outlets, and Academic Publications.

**Size**: 33,000 questions

**Format**: JSON

**Annotation**: Questions were manually created and verified by native language experts from the targeted countries.

## üî¨ Methodology

**Methods**:
- Automated metrics
- Human evaluation

**Metrics**:
- Accuracy

**Calculation**: Metrics were calculated based on model performance on the dataset across different modalities and question types.

**Interpretation**: High performance indicates a language model's strong understanding of traditional sports and cultural contexts.

**Validation**: The benchmark was validated through a rigorous multi-step process involving domain experts and cultural knowledge.

## ‚ö†Ô∏è Targeted Risks

**Risk Categories**:
- Bias
- Cultural Sensitivity
- Inclusivity

**Atlas Risks**:
No specific atlas risks defined

**Demographic Analysis**: The dataset includes a wide range of languages and cultural backgrounds to ensure diversity.

**Potential Harm**: N/A

## üîí Ethical and Legal Considerations

**Privacy And Anonymity**: Not Applicable

**Data Licensing**: Not Applicable

**Consent Procedures**: Not Applicable

**Compliance With Regulations**: Not Applicable
