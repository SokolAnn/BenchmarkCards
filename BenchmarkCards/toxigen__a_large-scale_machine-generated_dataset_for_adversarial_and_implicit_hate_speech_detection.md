# TOXIGEN: A Large-Scale Machine-Generated Dataset for Adversarial and Implicit Hate Speech Detection

## üìä Benchmark Details

**Name**: TOXIGEN: A Large-Scale Machine-Generated Dataset for Adversarial and Implicit Hate Speech Detection

**Overview**: We create TOXIGEN, a new large-scale and machine-generated dataset of 274k toxic and benign statements about 13 minority groups. We develop a demonstration-based prompting framework and an adversarial classifier-in-the-loop decoding method (ALICE) to generate subtly toxic and benign text with a massive pretrained language model (GPT-3). TOXIGEN is almost entirely implicit (98.2% implicit) and balanced between toxic and benign statements for each group. We release our code and data at https://github.com/microsoft/ToxiGen.

**Data Type**: text (single-sentence statements mentioning minority groups)

**Domains**:
- Natural Language Processing

**Similar Benchmarks**:
- ImplicitHateCorpus
- SocialBiasFrames
- DynaHate
- TweetBLM
- Breitfeller et al. (2019)
- Founta et al. (2018)
- Davidson et al. (2017)

**Resources**:
- [GitHub Repository](https://github.com/microsoft/ToxiGen)
- [Resource](https://arxiv.org/abs/2203.09509)

## üéØ Purpose and Intended Users

**Goal**: Provide a large-scale, balanced, and largely-implicit dataset of statements mentioning 13 minority groups to improve evaluation and training of toxic language detectors, particularly for implicitly toxic language and adversarial robustness.

**Target Audience**:
- ML Researchers
- Model Developers
- Industry Practitioners
- Domain Experts in Toxicity/Hate Speech Detection

**Tasks**:
- Toxicity Detection
- Hate Speech Detection
- Implicit Hate / Microaggression Detection
- Robustness Evaluation of Toxicity Classifiers

**Limitations**: The dataset only captures implicit toxicity for 13 identified minority groups and can naturally be noisy due to large-scale machine generation. Annotations may not capture the full complexity of human experiences and toxicity is subjective.

## üíæ Data

**Source**: Generated by GPT-3 (Brown et al., 2020) using demonstration-based prompting and an adversarial classifier-in-the-loop constrained beam search decoding method (ALICE); includes samples generated with top-k decoding and with ALICE.

**Size**: 274,186 examples (statements); covers over 135k toxic and over 135k benign statements. ALICE subset: 14,174 examples; top-k subset: 260,012 examples.

**Format**: Dataframe (released with fields: prompt, generation, generation_method, prompt_label, group, roberta_prediction).

**Annotation**: Human validation via Amazon Mechanical Turk. A human-validated test set (TOXIGEN-HUMAN VAL) of 792 statements rated by 3 annotators each (from a prequalified pool of 156 workers). Additional human annotations: 8,960 randomly sampled training examples. Harmfulness rated on a 1-5 Likert scale; mapping to classes uses the max of HARMFUL_IF_AI and HARMFUL_IF_HUMAN: scores <3 => non-toxic, =3 => ambiguous, >3 => toxic.

## üî¨ Methodology

**Methods**:
- Demonstration-based prompting
- Top-k decoding (baseline generation)
- Adversarial classifier-in-the-loop constrained beam search decoding (ALICE)
- Human validation via Amazon Mechanical Turk
- Fine-tuning toxicity classifiers (HateBERT, ToxDectRoBERTa)

**Metrics**:
- Area Under ROC Curve (AUC)
- Fleiss' kappa
- Krippendorff's alpha
- Human annotation Likert-scale mean (1-5)
- Percentage mistaken for human-written (%)

**Calculation**: For human annotation mapping: take the maximum of the HARMFUL_IF_AI and HARMFUL_IF_HUMAN scores and map to classes (<3 non-toxic, =3 ambiguous, >3 toxic). AUC reported for classifier evaluations on external human-written datasets and TOXIGEN-HUMAN VAL. Inter-annotator agreement reported as Fleiss' kappa and Krippendorff's alpha.

**Interpretation**: Higher AUC indicates better classifier performance. Fine-tuning classifiers on TOXIGEN improves performance on human-written implicit toxic datasets (reported improvements of +7‚Äì19%). Human-evaluation statistics (e.g., percent mistaken for human) indicate how human-like generated examples are.

**Baseline Results**: HateBERT (AUC): zero-shot on SocialBiasFrames 0.60; fine-tuned on ALICE 0.66; top-k 0.65; ALICE+top-k 0.71. On ImplicitHateCorpus: 0.60 (none), 0.60 (ALICE), 0.61 (top-k), 0.67 (ALICE+top-k). On DynaHate: 0.47 (none), 0.54 (ALICE), 0.59 (top-k), 0.66 (ALICE+top-k). On TOXIGEN-VAL: 0.57 (none), 0.93 (ALICE), 0.88 (top-k), 0.96 (ALICE+top-k). RoBERTa (AUC): SocialBiasFrames 0.65 (none), 0.70 (ALICE), 0.67 (top-k), 0.70 (ALICE+top-k); ImplicitHateCorpus 0.57, 0.64, 0.63, 0.66; DynaHate 0.49, 0.51, 0.50, 0.54; TOXIGEN-VAL 0.57, 0.87, 0.85, 0.93.

**Validation**: Human validation: TOXIGEN-HUMAN VAL (792 statements) annotated by 3 annotators each; ensured no training statement had cosine similarity above 0.7 with any test statement. Inter-annotator agreement: Fleiss' kappa = 0.46; Krippendorff's alpha = 0.64. Additional large-scale human validation performed on ~8,960 training samples.

## ‚ö†Ô∏è Targeted Risks

**Risk Categories**:
- Fairness
- Robustness
- Misuse
- Accuracy
- Societal Impact

**Atlas Risks**:
- **Fairness**: Data bias
- **Robustness**: Evasion attack
- **Misuse**: Spreading toxicity
- **Accuracy**: Poor model accuracy
- **Societal Impact**: Impact on affected communities

**Demographic Analysis**: TOXIGEN includes mentions of 13 minority identity groups (per-group counts and statistics reported in Table 2). Human annotator demographics (from a survey of MTurk workers): 56.9% identify as White, 9.8% as Black, 3.9% as Hispanic, 3.9% as Asian, 5.9% as Other; 45.1% female, 37.3% male, 2% non-binary; majority aged 25-45 (58.8%).

**Potential Harm**: ['Detecting implicit toxic language / implicit hate speech (stereotyping, microaggressions)', 'Preventing over-censoring or marginalization of minority groups by improving classifier robustness and reducing spurious identity-toxicity correlations', 'Identifying adversarially-generated toxicity that can harm targeted communities']

## üîí Ethical and Legal Considerations

**Privacy And Anonymity**: Annotators provided signed consent before viewing any text (annotation interface includes a strong warning and required signed consent). No additional anonymization procedures for data instances are specified in the paper.

**Data Licensing**: Not Applicable

**Consent Procedures**: For human validation, annotators were required to provide signed consent before any text was shown. Workers were prequalified; a demographic survey was optional and collected.

**Compliance With Regulations**: Not Applicable
