# PSEUDO EVAL

## 📊 Benchmark Details

**Name**: PSEUDO EVAL

**Overview**: PSEUDO EVAL is a multilingual code generation benchmark that provides solutions written in pseudocode as input to isolate and identify the bottleneck of language coding and problem-solving capabilities in large language models.

**Data Type**: problem-solution pairs and pseudocode

**Domains**:
- Natural Language Processing

**Languages**:
- Python
- C++
- Rust

**Similar Benchmarks**:
- HumanEval
- LiveCodeBench

**Resources**:
- [Resource](https://anonymous.4open.science/r/PseudocodeACL25-7B74/)

## 🎯 Purpose and Intended Users

**Goal**: To understand the bottlenecks in end-to-end code generation for large language models by isolating language-coding from problem-solving capabilities.

**Target Audience**:
- ML Researchers
- Model Developers

**Tasks**:
- Code Generation

**Limitations**: N/A

## 💾 Data

**Source**: User-submitted solutions on LeetCode and the problems indexed by LiveCodeBench.

**Size**: 1,060 subjects

**Format**: JSON

**Annotation**: Pseudocode generated by a reasoning model (DeepSeek-R1) given solution codes.

## 🔬 Methodology

**Methods**:
- Automated metrics

**Metrics**:
- Pass@k

**Calculation**: Pass@k rates are calculated based on tests published by LiveCodeBench.

**Interpretation**: Higher Pass@k rates indicate better performance of the models on the benchmark tasks.

**Baseline Results**: N/A

**Validation**: N/A

## ⚠️ Targeted Risks

**Risk Categories**:
- Bias
- Safety
- Accuracy

**Atlas Risks**:
- **Accuracy**: Data contamination, Unrepresentative data

**Demographic Analysis**: N/A

**Potential Harm**: N/A

## 🔒 Ethical and Legal Considerations

**Privacy And Anonymity**: Not Applicable

**Data Licensing**: Not Applicable

**Consent Procedures**: Not Applicable

**Compliance With Regulations**: Not Applicable
