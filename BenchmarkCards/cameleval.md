# CamelEval

## üìä Benchmark Details

**Name**: CamelEval

**Overview**: CamelEval is a new benchmark for Arabic LLMs, specifically designed to assess their conversational abilities and instruction-following proficiency within Arabic contexts.

**Data Type**: question-answering pairs

**Domains**:
- Natural Language Processing

**Languages**:
- Arabic
- English

**Similar Benchmarks**:
- Open Arabic LLM Leaderboard (OALL)
- AlpacaEval

**Resources**:
- [Resource](https://huggingface.co/spaces/OALL/Open-Arabic-LLM-Leaderboard)

## üéØ Purpose and Intended Users

**Goal**: To provide a benchmark for evaluating the performance of Arabic LLMs in instruction-following and generating culturally relevant responses.

**Target Audience**:
- ML Researchers
- Industry Practitioners
- Model Developers
- Domain Experts

**Tasks**:
- Text Generation
- Conversation Generation
- Instruction Following

**Limitations**: N/A

## üíæ Data

**Source**: Curated set of prompts based on a corpus of human-curated, textbook-quality content spanning various fields.

**Size**: 805 prompts

**Format**: N/A

**Annotation**: Prompts generated by native-speaking annotators with manual reviews.

## üî¨ Methodology

**Methods**:
- LLM-as-a-judge evaluation

**Metrics**:
- Win Rate

**Calculation**: Win Rate is calculated based on the performance of LLMs against each other on the test prompts.

**Interpretation**: A higher win rate indicates better performance in generating helpful and accurate responses.

**Baseline Results**: N/A

**Validation**: Tested against existing models in OALL for comparative performance.

## ‚ö†Ô∏è Targeted Risks

**Risk Categories**:
- Bias
- Accuracy
- Robustness

**Atlas Risks**:
- **Fairness**: Data bias
- **Accuracy**: Poor model accuracy
- **Robustness**: Prompt injection attack

**Demographic Analysis**: N/A

**Potential Harm**: N/A

## üîí Ethical and Legal Considerations

**Privacy And Anonymity**: Not Applicable

**Data Licensing**: MIT

**Consent Procedures**: Not Applicable

**Compliance With Regulations**: Not Applicable
