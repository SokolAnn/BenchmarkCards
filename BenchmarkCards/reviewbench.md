# ReviewBench

## 📊 Benchmark Details

**Name**: ReviewBench

**Overview**: ReviewBench is a benchmark specifically designed to evaluate the quality of automatically generated review comments by large language models (LLMs), assessing them across multiple dimensions.

**Data Type**: review comments

**Domains**:
- Natural Language Processing

**Languages**:
- English

**Similar Benchmarks**:
- Review-CoT

**Resources**:
- [Resource](https://openreview.net/)
- [Resource](https://papers.nips.cc/)

## 🎯 Purpose and Intended Users

**Goal**: The primary objective of ReviewBench is to provide standardized quantitative evaluation criteria for review comments generated by LLMs.

**Target Audience**:
- ML Researchers
- Model Developers

**Tasks**:
- Review Comment Generation

**Limitations**: N/A

## 💾 Data

**Source**: Reviews from publicly available academic review platforms such as ICLR and NeurIPS collected for generating metrics.

**Size**: 100 papers

**Format**: JSON

**Annotation**: N/A

## 🔬 Methodology

**Methods**:
- Human evaluation
- Automated metrics

**Metrics**:
- Language Diversity
- Semantic Consistency
- Sentiment Consistency

**Calculation**: Metrics are computed based on the structure and content of review comments generated by models compared to human-written reviews.

**Interpretation**: Scores reflect the alignment of model-generated reviews with human reviewers in terms of content quality and alignment.

**Baseline Results**: N/A

**Validation**: Reviewed comments were compared against human-generated references to ensure validity.

## ⚠️ Targeted Risks

**Risk Categories**:
- Bias
- Fairness
- Accuracy

**Atlas Risks**:
- **Fairness**: Output bias
- **Accuracy**: Poor model accuracy

**Demographic Analysis**: N/A

## 🔒 Ethical and Legal Considerations

**Privacy And Anonymity**: Not Applicable

**Data Licensing**: Not Applicable

**Consent Procedures**: Not Applicable

**Compliance With Regulations**: Not Applicable
