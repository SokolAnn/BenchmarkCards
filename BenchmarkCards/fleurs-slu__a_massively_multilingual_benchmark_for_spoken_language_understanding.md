# Fleurs-SLU: A Massively Multilingual Benchmark for Spoken Language Understanding

## üìä Benchmark Details

**Name**: Fleurs-SLU: A Massively Multilingual Benchmark for Spoken Language Understanding

**Overview**: Fleurs-SLU is a multilingual SLU benchmark that encompasses 692 hours of speech for topical utterance classification in 102 languages and multiple-choice question answering via listening comprehension spanning 944 hours of speech across 92 languages.

**Data Type**: speech utterance classification and multiple-choice question answering

**Domains**:
- Natural Language Processing

**Languages**:
- English
- Afrikaans
- Amharic
- Modern Standard Arabic
- Assamese
- North Azerbaijani
- Bengali
- Bulgarian
- Catalan
- Cebuano
- Czech
- Central Kurdish
- Danish
- German
- Greek
- French
- Nigerian Fulfulde
- West Central Oromo
- Gujarati
- Hausa
- Hebrew
- Hindi
- Croatian
- Hungarian
- Armenian
- Igbo
- Indonesian
- Icelandic
- Italian
- Javanese
- Japanese
- Kannada
- Georgian
- Kazakh
- Kabuverdianu
- Halh
- Khmer
- Kyrgyz
- Korean
- Lao
- Lingala
- Lithuanian
- Ganda
- Luo
- Latvian
- Malayalam
- Marathi
- Macedonian
- Maltese
- Maori
- Burmese
- Dutch
- Norwegian Bokm√•l
- Nepali
- Northern Sotho
- Nyanja
- Occitan
- Odia
- Eastern Panjabi
- Southern Pashto
- Western Persian
- Polish
- Portuguese
- Romanian
- Russian
- Slovak
- Slovenian
- Shona
- Sindhi
- Somali
- Spanish
- Serbian
- Swedish
- Swahili
- Tamil
- Telugu
- Tajik
- Tagalog
- Thai
- Turkish
- Ukrainian
- Urdu
- Northern Uzbek
- Vietnamese
- Wolof
- Xhosa
- Yoruba
- Chinese (Simplified)
- Chinese (Traditional)
- Standard Malay
- Zulu

**Resources**:
- [Resource](https://huggingface.co/datasets/SIB-Fleurs)
- [Resource](https://huggingface.co/datasets/Belebele-Fleurs)

## üéØ Purpose and Intended Users

**Goal**: To evaluate massively multilingual speech models in end-to-end speech classification and multiple-choice question answering.

**Target Audience**:
- ML Researchers
- Industry Practitioners
- Model Developers

**Tasks**:
- Utterance Classification
- Multiple-Choice Question Answering

**Limitations**: N/A

## üíæ Data

**Source**: Merged datasets derived from Flores, SIB-200, and Belebele.

**Size**: 692 hours for utterance classification, 944 hours for multiple-choice question answering

**Format**: N/A

**Annotation**: N/A

## üî¨ Methodology

**Methods**:
- End-to-End Evaluation
- Cascaded Systems

**Metrics**:
- Accuracy
- F1 Score

**Calculation**: Metrics calculated based on the performance of models across various languages and tasks.

**Interpretation**: Higher scores indicate better model performance in classification and understanding tasks.

**Baseline Results**: N/A

**Validation**: Models evaluated on separate validation and test splits to ensure robustness.

## ‚ö†Ô∏è Targeted Risks

**Risk Categories**:
- Accuracy
- Fairness

**Atlas Risks**:
No specific atlas risks defined

**Demographic Analysis**: N/A

**Potential Harm**: N/A

## üîí Ethical and Legal Considerations

**Privacy And Anonymity**: Not Applicable

**Data Licensing**: Not Applicable

**Consent Procedures**: Not Applicable

**Compliance With Regulations**: Not Applicable
