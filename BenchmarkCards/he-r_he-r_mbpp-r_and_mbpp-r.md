# HE-R, HE-R+, MBPP-R, and MBPP-R+

## üìä Benchmark Details

**Name**: HE-R, HE-R+, MBPP-R, and MBPP-R+

**Overview**: This paper presents a systematic approach to transform existing coding benchmarks with predefined test cases into scoring and ranking benchmarks for evaluating synthetic verifiers, which includes four new benchmarks: HE-R, HE-R+, MBPP-R, and MBPP-R+.

**Data Type**: code solutions and test case pairs

**Domains**:
- Natural Language Processing

**Languages**:
- English

**Similar Benchmarks**:
- HumanEval
- Mostly Basic Programming Problems
- TESTEVAL
- TestGenEval

**Resources**:
- [Resource](https://huggingface.co/datasets/nvidia/Scoring-Verifiers)
- [GitHub Repository](https://github.com/aleksficek/Scoring-Verifiers)

## üéØ Purpose and Intended Users

**Goal**: To evaluate synthetic verification methods for code solutions and understand how well synthetic verifiers approximate solution correctness and rank solutions for coding problems.

**Target Audience**:
- Machine Learning Researchers
- Model Developers

**Tasks**:
- Code Verification
- Test Case Generation

**Limitations**: The quality and effectiveness of the benchmarks are directly tied to the reliability of the original benchmarks they are based on.

## üíæ Data

**Source**: Created from existing benchmarks HumanEval and Mostly Basic Programming Problems.

**Size**: 164 problems for HE-R and HE-R+, 974 problems for MBPP-R, 378 problems for MBPP-R+

**Format**: JSON

**Annotation**: Solutions were generated by LLMs using diverse prompting techniques.

## üî¨ Methodology

**Methods**:
- Automated metrics
- Human evaluation

**Metrics**:
- Top-1 Accuracy
- Spearman's œÅ Coefficient
- Bottom-1 Accuracy
- Mean Absolute Error (MAE)

**Calculation**: Metrics are calculated based on the performance of solutions in passing predefined test cases and correlation with expected rankings.

**Interpretation**: A higher Top-1 Accuracy indicates that the synthetic verifier can correctly rank the best solution first, while Spearman's œÅ assesses the accuracy of the ranking.

**Baseline Results**: Results from multiple LLMs evaluated using the new benchmarks, showing various accuracies across standard and reasoning models.

**Validation**: Benchmarks validated against various model outputs and performance metrics derived from synthetic verifiers.

## ‚ö†Ô∏è Targeted Risks

**Risk Categories**:
- Accuracy
- Fairness
- Privacy

**Atlas Risks**:
- **Accuracy**: Unrepresentative data
- **Fairness**: Data bias
- **Privacy**

**Demographic Analysis**: N/A

## üîí Ethical and Legal Considerations

**Privacy And Anonymity**: Not Applicable

**Data Licensing**: Not Applicable

**Consent Procedures**: Not Applicable

**Compliance With Regulations**: Not Applicable
