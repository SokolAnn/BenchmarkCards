# ChestX-ray8

## üìä Benchmark Details

**Name**: ChestX-ray8

**Overview**: ChestX-ray8: a hospital-scale chest X-ray database comprising 108,948 frontal-view X-ray images of 32,717 unique patients with text-mined eight disease image labels (multi-label per image) mined from associated radiological reports via NLP. The dataset is used to validate a unified weakly-supervised multi-label image classification and disease localization framework for common thoracic diseases.

**Data Type**: image (frontal-view chest X-rays; multi-label image classification labels and bounding boxes)

**Domains**:
- Medical Diagnosis
- Computer Vision
- Natural Language Processing

**Similar Benchmarks**:
- OpenI
- ChestX-ray14
- ImageNet
- MS COCO
- PASCAL VOC

**Resources**:
- [Resource](https://nihcc.app.box.com/v/ChestXray-NIHCC)
- [Resource](https://openi.nlm.nih.gov)
- [Resource](https://www.cc.nih.gov/drd/summers.html)
- [Resource](https://arxiv.org/abs/1705.02315)

## üéØ Purpose and Intended Users

**Goal**: To construct a hospital-scale chest X-ray image database (ChestX-ray8) and provide quantitative performance benchmarking on eight common thoracic pathology multi-label classification and weakly-supervised localization to facilitate development of large-scale deep learning based CAD systems.

**Target Audience**:
- ML Researchers
- Medical Imaging Researchers
- Clinical Researchers
- Model Developers

**Tasks**:
- Multi-label Image Classification
- Object Detection/Localization
- Named Entity Recognition (clinical radiology reports)

**Limitations**: Limited number of hand-annotated bounding boxes (1,600 boxes on 983 images) relative to the full dataset; many pathologies occupy small spatial extents making detection/localization difficult; class imbalance (some diseases, e.g., Pneumonia, have <1% instances) affects performance; text-mined labels may be noisy (mitigated by syntactic negation/uncertainty rules).

## üíæ Data

**Source**: Images and associated radiology reports extracted from the institute's Picture Archiving and Communication System (PACS); disease labels text-mined from associated radiological reports using NLP tools (DNorm and MetaMap) with syntactic negation and uncertainty rules; a subset of images annotated with bounding boxes by a board-certified radiologist.

**Size**: 108,948 frontal-view X-ray images; 32,717 unique patients (collected 1992‚Äì2015); 24,636 images contain one or more pathologies; 84,312 images labeled as Normal; 983 images with 1,600 hand-labeled bounding boxes.

**Format**: Images extracted from DICOM and resized to 1024x1024 bitmap images; radiology reports as plain text (processed for NLP).

**Annotation**: Image-level disease labels automatically generated by text-mining radiology reports using DNorm and MetaMap with hand-crafted syntactic rules for negation and uncertainty; 1,600 bounding boxes manually annotated by a board-certified radiologist for 983 images (used for localization evaluation).

## üî¨ Methodology

**Methods**:
- Model-based evaluation (deep convolutional neural networks fine-tuned from ImageNet pre-trained models: AlexNet, GoogLeNet, VGGNet-16, ResNet-50)
- Automated metrics (ROC/AUC, Precision, Recall, F1-Score)
- Localization evaluation using Intersection over Union (IoU) and Intersection over detected bounding-box area ratio (IoBB)

**Metrics**:
- Area Under the Receiver Operating Characteristic Curve (AUC)
- Accuracy
- Precision
- Recall
- F1 Score
- Intersection over Union (IoU)
- Intersection over detected bounding-box area ratio (IoBB)
- Average False Positive (AFP)

**Calculation**: AUC values computed from ROC curves on the held-out test split. Labeling evaluation computed using Precision, Recall, and F1-score against OpenI and annotated reports. Localization correctness defined as IoU > T(IoU) or IoBB > T(IoBB) with reported thresholds (IoBB thresholds: 0.1, 0.25, 0.5, 0.75, 0.9; IoU thresholds: reported from 0.1 to 0.7).

**Interpretation**: Higher AUC indicates better pathology recognition. The paper reports that classes with larger/clearer manifestations (e.g., Cardiomegaly AUC=0.8141, Pneumothorax AUC=0.7891) are better detected, while small or highly variable findings (e.g., Mass AUC=0.5609, Nodule lower) yield lower performance. Localization performance is limited by low-resolution heatmaps and limited bounding-box annotations.

**Baseline Results**: ResNet-50 (ChestX-ray8) AUCs: Atelectasis 0.7069; Cardiomegaly 0.8141; Effusion 0.7362; Infiltration 0.6128; Mass 0.5609; Nodule 0.7164; Pneumonia 0.6333; Pneumothorax 0.7891. Image labeling (text-mining) on OpenI: total Precision=0.90, Recall=0.91, F1=0.90 (our method) vs. MetaMap total P=0.84, R=0.88, F1=0.86. Localization results (example) reported as localization Accuracy and AFP across multiple IoBB/IoU thresholds (see paper Tables 4 and 7).

**Validation**: Dataset randomly split at patient level into training (70%), validation (10%), and testing (20%) for CNN fine-tuning. OpenI and a manually annotated report subset used as gold standards to evaluate text-mined labels. The 983 images with 1,600 B-Boxes are reserved for localization testing (not used for training).

## ‚ö†Ô∏è Targeted Risks

**Risk Categories**:
- Accuracy

**Atlas Risks**:
- **Accuracy**: Data contamination, Unrepresentative data, Poor model accuracy

## üîí Ethical and Legal Considerations

**Privacy And Anonymity**: Not Applicable

**Data Licensing**: Not Applicable

**Consent Procedures**: Not Applicable

**Compliance With Regulations**: Not Applicable
