# NiM-Benchmark

## ğŸ“Š Benchmark Details

**Name**: NiM-Benchmark

**Overview**: NiM-Benchmark is designed to evaluate Multi-modal Large Language Models' (MLLMs) ability to locate and reason about fine-grained details within complex documents, including newspapers, menus, and academic materials.

**Data Type**: image-question pairs

**Domains**:
- Natural Language Processing

**Languages**:
- English

**Resources**:
- [Resource](https://huggingface.co/datasets/NiM-Benchmark)

## ğŸ¯ Purpose and Intended Users

**Goal**: To assess MLLMs' effectiveness in extracting precise information from visually rich document layouts.

**Target Audience**:
- ML Researchers
- Industry Practitioners

**Tasks**:
- Document Visual Question Answering

**Limitations**: Although our method performs well on existing DocVQA datasets, it struggles with long length documents.

## ğŸ’¾ Data

**Source**: Curated documents from multiple domains including academic papers, newspapers, magazines, restaurant menus, and lecture screenshots.

**Size**: 2,970 images and 1,180 question-answer pairs

**Format**: images

**Annotation**: Hybrid approach: initially generated by MLLMs and then verified by human annotators.

## ğŸ”¬ Methodology

**Methods**:
- Human evaluation
- Automated metrics

**Metrics**:
- Exact Match
- F1 Score
- ANLS Score

**Calculation**: Metrics are computed based on model predictions against human-validated ground truths.

**Interpretation**: Higher scores indicate better model performance in accurately answering questions about fine-grained details.

**Baseline Results**: GPT-4o baseline performance scores on NiM-Benchmark.

**Validation**: Rigorously validated through both automated methods and human assessments.

## âš ï¸ Targeted Risks

**Risk Categories**:
- Accuracy
- Fairness

**Atlas Risks**:
- **Accuracy**: Poor model accuracy
- **Fairness**: Data bias

**Demographic Analysis**: N/A

## ğŸ”’ Ethical and Legal Considerations

**Privacy And Anonymity**: Not Applicable

**Data Licensing**: Not Applicable

**Consent Procedures**: Not Applicable

**Compliance With Regulations**: Not Applicable
