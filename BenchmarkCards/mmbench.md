# MMBench

## üìä Benchmark Details

**Name**: MMBench

**Overview**: MMBench is a bilingual benchmark for assessing the multi-modal capabilities of vision-language models (VLMs). It is a systematically designed objective evaluation benchmark containing over 3,000 multiple-choice questions across 20 fine-grained ability dimensions, introduces a CircularEval strategy, and uses large language models to convert free-form predictions into pre-defined choices to yield accurate evaluation results for models with limited instruction-following capability.

**Data Type**: multimodal (image + text) multiple-choice question-answer pairs

**Domains**:
- Natural Language Processing
- Computer Vision
- Multimodal

**Languages**:
- English
- Chinese

**Similar Benchmarks**:
- VQAv2
- COCO Caption
- GQA
- OK-VQA
- TextVQA
- ScienceQA
- Youcook2
- OwlEval
- LVLM-eHub
- MME
- SEEDBench

**Resources**:
- [Resource](https://arxiv.org/abs/2307.06281)
- [Resource](https://arxiv.org/pdf/2307.06281v3)

## üéØ Purpose and Intended Users

**Goal**: To robustly and holistically evaluate the multi-modal perception and reasoning abilities of vision-language models across 20 fine-grained ability dimensions using an objective, scalable benchmark.

**Target Audience**:
- Research community
- ML Researchers
- Model Developers

**Tasks**:
- Question Answering
- Visual Question Answering
- Object Localization
- Optical Character Recognition
- Image Scene Recognition
- Image Quality Assessment
- Image-Text Understanding

**Limitations**: N/A

## üíæ Data

**Source**: Manually collected from multiple sources: ~80% from the Internet and ~20% from validation sets of public datasets and other sources. Listed image/problem sources include ARAS, CLEVR, COCO, KonIQ-10k, LLaV A, PISC, Places, ScienceQA, ShapeWorld, TextVQA, VSR, W3C School, and Internet. Questions and choices are manually constructed or selected; MMBench-CN translations were generated by GPT-4 and verified by humans.

**Size**: 3,217 examples

**Format**: N/A

**Annotation**: Manually collected and constructed by volunteers (undergraduate/graduate students). Quality control includes LLM-based text-only filtering (GPT-4, Gemini-Pro, Qwen-Max majority voting), automatic filtering using multiple VLMs, and manual verification. Translations to Chinese done with GPT-4 and human verification.

## üî¨ Methodology

**Methods**:
- Automated metrics (exact match / Top-1 accuracy)
- Heuristic matching to extract choice labels from model outputs
- LLM-based choice extraction (GPT-4 used to map free-form outputs to choices)
- CircularEval (multi-pass circular-shift consistency evaluation)

**Metrics**:
- Top-1 Accuracy
- Matching Success Rate (heuristic matching)
- Alignment Rate with Human (LLM choice extraction alignment)

**Calculation**: CircularEval: for an N-choice question, the question is fed to the VLM N times with circularly shifted choices; a model is considered correct only if it predicts the correct choice in all passes. Choice extraction: first attempt heuristic matching for explicit labels; if that fails, use GPT-4 to map free-form prediction to one of the candidate choices (or pseudo-choice 'Z').

**Interpretation**: Higher Top-1 Accuracy under CircularEval indicates better and more consistent multimodal understanding. CircularEval is stricter than single-pass (VanillaEval) and reveals robustness and instruction-following limitations; alignment rate between LLM extractor and humans indicates reliability of LLM-based matching.

**Baseline Results**: Selected CircularEval overall Top-1 accuracies on MMBench test (as reported): InternLM-XComposer2: 78.1% overall; GPT-4v: 74.3% overall; Qwen-VL-Max: 75.4% overall; InternLM-XComposer2 reported as the top-performing evaluated model.

**Validation**: Validated LLM choice extraction by sampling ~420 hard examples (10% of unparsable predictions) and comparing GPT-4 extraction with human annotations; GPT-4 alignment with humans reported as 91.5%. CircularEval vs VanillaEval comparisons demonstrate significant drops for many VLMs, supporting CircularEval's stricter evaluation behavior.

## ‚ö†Ô∏è Targeted Risks

**Risk Categories**:
- Fairness
- Accuracy

**Atlas Risks**:
- **Fairness**: Decision bias
- **Accuracy**: Poor model accuracy

## üîí Ethical and Legal Considerations

**Privacy And Anonymity**: Not Applicable

**Data Licensing**: Not Applicable

**Consent Procedures**: Not Applicable

**Compliance With Regulations**: Not Applicable
