# LegalEval-Q: A New Benchmark for The Quality Evaluation of LLM-Generated Legal Text

## üìä Benchmark Details

**Name**: LegalEval-Q: A New Benchmark for The Quality Evaluation of LLM-Generated Legal Text

**Overview**: LegalEval-Q introduces a multidimensional assessment model that measures critical quality attributes of legal texts, including clarity, coherence, and terminological precision. This benchmark is designed to systematically evaluate the quality of responses generated by language models in various legal contexts.

**Data Type**: legal question-answering pairs

**Domains**:
- Natural Language Processing
- Legal

**Languages**:
- Chinese

**Similar Benchmarks**:
- MMLU (Massive Multitask Language Understanding)
- BIG-Bench

**Resources**:
- [GitHub Repository](https://github.com/lyxx3rd/LegalEval-Q)

## üéØ Purpose and Intended Users

**Goal**: To establish standardized evaluation protocols for assessing the quality of LLM-generated legal texts and identify the optimal models for specific legal applications.

**Target Audience**:
- Legal Practitioners
- ML Researchers
- AI Developers

**Tasks**:
- Text Quality Evaluation
- Legal Question Answering

**Limitations**: The current benchmark is specialized in the legal domain, which may bias its application to other fields.

## üíæ Data

**Source**: The data includes legal questions sourced from the DISC-Law-SFT-Pair dataset, Criminal-Law-Dataset, and a proprietary dataset focused on the Civil Code of China.

**Size**: 10,000 queries

**Format**: JSON

**Annotation**: Data generated by models during testing without predefined answers.

## üî¨ Methodology

**Methods**:
- Regression Model Assessment
- Qualitative Evaluation

**Metrics**:
- Score (0-100 scale)

**Calculation**: Scores are derived from a combination of content quality, comments, and conclusions assessing the generated responses.

**Interpretation**: Higher scores indicate better quality of textual output, reflecting clarity, coherence, and appropriate terminology use.

**Validation**: Evaluation conducted on a validation set with strictly non-overlapping content from the training set.

## ‚ö†Ô∏è Targeted Risks

**Risk Categories**:
- Fairness
- Accuracy
- Privacy

**Atlas Risks**:
No specific atlas risks defined

**Demographic Analysis**: Demographic factors are not specifically analyzed but could influence text quality assessment.

**Potential Harm**: ['Bias in legal text generation affecting fairness and outcome.']

## üîí Ethical and Legal Considerations

**Privacy And Anonymity**: The study does not specifically discuss privacy measures.

**Data Licensing**: Not Applicable

**Consent Procedures**: Not Applicable

**Compliance With Regulations**: Not Applicable
