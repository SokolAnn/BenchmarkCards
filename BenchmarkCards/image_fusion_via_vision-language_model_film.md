# Image Fusion via Vision-Language Model (FILM)

## 📊 Benchmark Details

**Name**: Image Fusion via Vision-Language Model (FILM)

**Overview**: This paper presents the FILM paradigm for image fusion, which utilizes explicit textual information from source images to guide the fusion process, showing promising results in multiple image fusion tasks.

**Data Type**: paragraph descriptions for image pairs

**Domains**:
- Computer Vision

**Languages**:
- English

**Resources**:
- [GitHub Repository](https://github.com/Zhaozixiang1228/IF-FILM)

## 🎯 Purpose and Intended Users

**Goal**: To advance image fusion techniques by integrating vision-language models to enhance semantic understanding in fusion processes.

**Target Audience**:
- ML Researchers
- Computer Vision Practitioners

**Tasks**:
- Image Fusion

**Limitations**: N/A

## 💾 Data

**Source**: Vision-Language Fusion (VLF) dataset containing annotated prompts and descriptions for various image fusion datasets.

**Size**: 7040 paragraph descriptions

**Format**: Text

**Annotation**: ChatGPT-generated paragraphs based on image pairs from established datasets.

## 🔬 Methodology

**Methods**:
- Quantitative evaluation
- Validation against state-of-the-art methods

**Metrics**:
- Entropy (EN)
- Standard Deviation (SD)
- Spatial Frequency (SF)
- Average Gradient (AG)
- Visual Information Fidelity (VIF)
- QAB/F

**Calculation**: Metrics calculated based on the comparison of fused and source images.

**Interpretation**: Higher values in metrics indicate better quality of the fused image.

**Validation**: Tested against various state-of-the-art image fusion methods.

## ⚠️ Targeted Risks

**Risk Categories**:
- Bias
- Safety

**Atlas Risks**:
No specific atlas risks defined

**Demographic Analysis**: N/A

## 🔒 Ethical and Legal Considerations

**Privacy And Anonymity**: Not Applicable

**Data Licensing**: Not Applicable

**Consent Procedures**: Not Applicable

**Compliance With Regulations**: Not Applicable
