# PandaLM (Reproducible and Automated Language Model Assessment)

## üìä Benchmark Details

**Name**: PandaLM (Reproducible and Automated Language Model Assessment)

**Overview**: We introduce a judge large language model, named PandaLM, which is trained to distinguish the superior model given several LLMs. PandaLM focuses beyond just the objective correctness of responses and addresses subjective factors such as relative conciseness, clarity, adherence to instructions, comprehensiveness, and formality. It is designed as an automatic, reproducible, and privacy-protected evaluation method to support hyperparameter optimization for instruction-tuning LLMs.

**Data Type**: text (instruction-input-response tuples with paired model responses and preference labels; includes evaluation reasons and reference responses)

**Domains**:
- Natural Language Processing
- Legal
- Biomedical

**Languages**:
- N/A

**Similar Benchmarks**:
- lm-eval
- MMLU
- TruthfulQA
- HellaSwag
- BERTScore
- MAUVE

**Resources**:
- [Resource](N/A)

## üéØ Purpose and Intended Users

**Goal**: Construct an automated, reliable, and robust evaluation method that can be integrated into open-sourced LLMs and used as the judging basis for hyperparameter optimization of instruction-tuned models.

**Target Audience**:
- ML Researchers
- Model Developers
- Instruction tuning practitioners

**Tasks**:
- Model Evaluation (response ranking: win/tie/lose)
- Hyperparameter Optimization for Instruction Tuning

**Limitations**: The selected hyperparameter search range may not encompass absolute optimal hyperparameters; core training data is distilled from GPT-3.5 and may not fully resonate with human preferences; current emphasis is on outcome-based evaluation which is resource-intensive; only supervised training is studied (semi-supervised, noisy, and imbalanced training are future directions).

## üíæ Data

**Source**: Training data: instructions and inputs sampled from the Alpaca 52K dataset and paired responses produced by instruction-tuned models LLaMA-7B, Bloom-7B, Cerebras-GPT-6.7B, OPT-7B, and Pythia-6.9B; training labels (evaluation result, evaluation reason, reference response) were distilled from GPT-3.5 with heuristic filtering. Test data: human-annotated test dataset sampled from the human evaluation dataset of self-instruct, with paired responses from the same set of models and labels provided by three human annotators.

**Size**: Training: 300,000 examples (filtered training dataset). Test: 1,000 examples (filtered; original unfiltered 2,500 examples).

**Format**: N/A

**Annotation**: Training labels generated by GPT-3.5 (self-instruct) with heuristic filtering (filtering invalid evaluations and inconsistent samples when swapping response order). Test dataset annotated by three human experts (hired from an annotation company) with inter-annotator agreement measured by Cohen's Kappa (average scores 0.85, 0.86, 0.88); samples with significant divergences excluded.

## üî¨ Methodology

**Methods**:
- Model-based evaluation (PandaLM)
- Human evaluation
- Automated comparative evaluation using GPT-3.5 and GPT-4

**Metrics**:
- Accuracy
- Precision
- Recall
- F1 Score
- Win/Lose/Tie counts

**Calculation**: Metrics are computed by comparing judge model predictions (win/tie/lose) against human labels (or GPT-4 labels in some domain tests). Accuracy/Precision/Recall/F1 are calculated from these comparisons. During inference, response order is swapped and conflicting evaluations are set to 'Tie'.

**Interpretation**: Exceeding the human inter-annotator agreement (IAA ~ 0.85) is considered strong performance; for the three-class task (win/tie/lose) random guess baseline is ~33%. Higher accuracy/precision/recall/F1 indicates closer alignment with human preferences or chosen gold standard (GPT-4 in some domain tests).

**Baseline Results**: Human annotation inter-annotator agreement (Cohen's Kappa averages 0.85/0.86/0.88). Table 2 results vs. human: GPT-3.5 Accuracy 0.6296, Precision 0.6195, Recall 0.6359, F1 0.5820; GPT-4 Accuracy 0.6647, Precision 0.6620, Recall 0.6815, F1 0.6180; PandaLM-7B Accuracy 0.5926, Precision 0.5728, Recall 0.5923, F1 0.5456; PandaLM-70B Accuracy 0.6687, Precision 0.7402, Recall 0.6687, F1 0.6923; PandaLM-70B-LoRA Accuracy 0.6186, Precision 0.7757, Recall 0.6186, F1 0.6654.

**Validation**: Reliability validated with a human-annotated test dataset (1K filtered examples) with three independent annotators and Cohen's Kappa IAA (averages 0.85/0.86/0.88). Robustness tested across distribution shifts using LSAT, PubMedQA, and BioASQ (GPT-4 used as gold in these domain tests). Position-bias addressed by swapping response order and setting conflicting results to 'Tie'.

## ‚ö†Ô∏è Targeted Risks

**Risk Categories**:
- Privacy
- Accuracy
- Governance

**Atlas Risks**:
- **Privacy**: Exposing personal information
- **Accuracy**: Data contamination
- **Governance**: Lack of system transparency

**Demographic Analysis**: N/A

**Potential Harm**: ['Data leakage']

## üîí Ethical and Legal Considerations

**Privacy And Anonymity**: PandaLM is designed to avoid API-based evaluations to prevent potential data leakage; the human-generated test dataset is reported to contain no personally identifiable information or offensive content; annotators were paid and treated as described (paid $50 per hour).

**Data Licensing**: Not Applicable

**Consent Procedures**: Not Applicable

**Compliance With Regulations**: Not Applicable
