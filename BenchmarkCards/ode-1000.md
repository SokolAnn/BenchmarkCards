# ODE-1000

## üìä Benchmark Details

**Name**: ODE-1000

**Overview**: The ODE-1000 benchmark comprises 1,000 diverse ODE problems designed to evaluate the ability of language models to generate executable code that accurately solves ordinary differential equations.

**Data Type**: question-answering pairs

**Domains**:
- Natural Language Processing

**Languages**:
- English

**Resources**:
- [GitHub Repository](https://github.com/SqueezeAILab/sciml-agent)

## üéØ Purpose and Intended Users

**Goal**: To evaluate scientific code generation capability of LLMs for solving ordinary differential equations.

**Target Audience**:
- ML Researchers
- Domain Experts

**Tasks**:
- Code Generation
- Numerical Problem Solving

**Limitations**: N/A

## üíæ Data

**Source**: Synthesized problems using GPT-4.1 derived from natural language descriptions.

**Size**: 1,000 examples

**Format**: JSON

**Annotation**: Automatically generated

## üî¨ Methodology

**Methods**:
- Human evaluation
- Automated metrics

**Metrics**:
- Code execution rate
- Accuracy
- Mean relative L2 error

**Calculation**: Accuracy is defined as the fraction of instances where the relative error L2 is less than 0.01.

**Interpretation**: High accuracy indicates a model's ability to correctly generate numerically valid code for solving ODEs.

**Baseline Results**: Models show significant improvements in accuracy and code execution rates post fine-tuning.

**Validation**: Results validated through comparisons with reference solutions.

## ‚ö†Ô∏è Targeted Risks

**Risk Categories**:
- Accuracy
- Robustness

**Atlas Risks**:
- **Accuracy**: Poor model accuracy
- **Robustness**: Evasion attack

**Demographic Analysis**: N/A

## üîí Ethical and Legal Considerations

**Privacy And Anonymity**: Not Applicable

**Data Licensing**: Not Applicable

**Consent Procedures**: Not Applicable

**Compliance With Regulations**: Not Applicable
