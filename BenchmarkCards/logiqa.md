# LogiQA

## 📊 Benchmark Details

**Name**: LogiQA

**Overview**: LogiQA is a comprehensive dataset designed for testing logical reasoning in machine reading comprehension, consisting of 8,678 question-answer instances.

**Data Type**: question-answering pairs

**Domains**:
- Natural Language Processing

**Languages**:
- English
- Chinese

**Similar Benchmarks**:
- SQuAD
- TriviaQA
- DROP

**Resources**:
- [GitHub Repository](https://github.com/lgw863/LogiQA-dataset)

## 🎯 Purpose and Intended Users

**Goal**: To facilitate research in logical reasoning and machine reading comprehension.

**Target Audience**:
- ML Researchers
- Model Developers

**Tasks**:
- Machine Reading Comprehension

**Limitations**: N/A

## 💾 Data

**Source**: Publicly available logical examination papers for reading comprehension from civil service exams.

**Size**: 8,678 question-answer pairs

**Format**: N/A

**Annotation**: Expert-written questions

## 🔬 Methodology

**Methods**:
- Rule-based evaluation
- Deep learning methods
- Pre-trained methods

**Metrics**:
- Accuracy

**Calculation**: Accuracy is calculated based on correct answer selection from multiple-choice questions.

**Interpretation**: Higher accuracy indicates better logical reasoning capabilities of models tested.

**Validation**: Results are validated against human performance and compared with baseline models.

## ⚠️ Targeted Risks

**Risk Categories**:
- Fairness
- Accuracy

**Atlas Risks**:
- **Accuracy**: Poor model accuracy
- **Fairness**

**Demographic Analysis**: N/A

## 🔒 Ethical and Legal Considerations

**Privacy And Anonymity**: Not Applicable

**Data Licensing**: Not Applicable

**Consent Procedures**: Not Applicable

**Compliance With Regulations**: Not Applicable
