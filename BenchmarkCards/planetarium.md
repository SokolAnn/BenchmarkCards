# Planetarium

## 📊 Benchmark Details

**Name**: Planetarium

**Overview**: Planetarium is a benchmark designed to evaluate language models’ ability to generate PDDL code from natural language descriptions of planning tasks. It includes a dataset of 145,918 text-to-PDDL pairs and a novel PDDL equivalence algorithm for evaluating generated PDDL correctness.

**Data Type**: text-to-PDDL pairs

**Domains**:
- Artificial Intelligence

**Languages**:
- English

**Resources**:
- [GitHub Repository](https://github.com/BatsResearch/planetarium)

## 🎯 Purpose and Intended Users

**Goal**: To evaluate language models' performance in generating PDDL code from natural language descriptions of planning problems.

**Target Audience**:
- ML Researchers
- Model Developers

**Tasks**:
- Code Generation

**Limitations**: Currently only supports the Blocks World, Gripper, and Floor Tile domains.

## 💾 Data

**Source**: Dataset based on the International Planning Competition (IPC), consisting of text descriptions paired with ground truth PDDL problems.

**Size**: 145,918 pairs

**Format**: CSV

**Annotation**: The dataset was procedurally generated by handcrafting templates for each task configuration.

## 🔬 Methodology

**Methods**:
- Automated metrics

**Metrics**:
- Parseable Problems
- Solvable Problems
- Correct Problems

**Calculation**: Each output from language models is evaluated for syntactic parseability, solvability, and semantic correctness against the ground truth.

**Interpretation**: The performance of models is assessed based on the percentage of generated PDDL that is valid, solvable, and semantically correct.

**Validation**: Evaluated using a set of heldout configurations from the dataset.

## ⚠️ Targeted Risks

**Risk Categories**:
- Accuracy

**Atlas Risks**:
No specific atlas risks defined

## 🔒 Ethical and Legal Considerations

**Privacy And Anonymity**: Not Applicable

**Data Licensing**: CC-BY-4.0

**Consent Procedures**: Not Applicable

**Compliance With Regulations**: Not Applicable
