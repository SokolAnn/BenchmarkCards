# EQ-Bench

## ğŸ“Š Benchmark Details

**Name**: EQ-Bench

**Overview**: EQ-Bench is a novel benchmark designed to evaluate aspects of emotional intelligence in Large Language Models (LLMs), assessing their ability to understand complex emotions and social interactions by rating the intensity of emotional states of characters in dialogues.

**Data Type**: text

**Domains**:
- Natural Language Processing

**Languages**:
- English

**Similar Benchmarks**:
- MMLU (Massive Multitask Language Understanding)
- SECEU (Emotional Intelligence of Large Language Models)

**Resources**:
- [GitHub Repository](https://github.com/EQ-bench/EQ-Bench)
- [Resource](https://eqbench.com)

## ğŸ¯ Purpose and Intended Users

**Goal**: To assess the emotional understanding capabilities of Large Language Models.

**Target Audience**:
- ML Researchers
- Industry Practitioners
- Model Developers

**Tasks**:
- Emotional Understanding

**Limitations**: N/A

## ğŸ’¾ Data

**Source**: A set of 60 questions designed by the authors based on dialogues generated by GPT-4.

**Size**: 60 questions

**Format**: N/A

**Annotation**: Answers were determined based on author consensus rather than crowd-sourced input.

## ğŸ”¬ Methodology

**Methods**:
- Automated metrics

**Metrics**:
- Correlation with multi-domain benchmarks

**Calculation**: Scores are calculated based on the differences between the model's ratings and reference answers, normalized to sum to 10.

**Interpretation**: Scores are interpreted as reflecting the model's understanding of emotional nuances within dialogues.

**Baseline Results**: EQ-Bench scores correlate strongly with other benchmarks like MMLU (r=0.97).

**Validation**: Test results are validated by repeating the benchmark with multiple runs.

## âš ï¸ Targeted Risks

**Risk Categories**:
- Bias
- Safety
- Robustness

**Atlas Risks**:
- **Fairness**: Data bias
- **Accuracy**: Unrepresentative data

**Demographic Analysis**: N/A

## ğŸ”’ Ethical and Legal Considerations

**Privacy And Anonymity**: Not Applicable

**Data Licensing**: MIT license for the benchmark code and questions.

**Consent Procedures**: Not Applicable

**Compliance With Regulations**: Not Applicable
