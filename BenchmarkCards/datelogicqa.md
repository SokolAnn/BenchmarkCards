# DateLogicQA

## 📊 Benchmark Details

**Name**: DateLogicQA

**Overview**: DateLogicQA is a human-curated benchmark of 190 questions designed to understand temporal bias in Large Language Models (LLMs), covering seven date formats across past, present, and future contexts, examining commonsense, factual, conceptual, and numerical reasoning types.

**Data Type**: question-answering pairs

**Domains**:
- Natural Language Processing

**Languages**:
- English

**Resources**:
- [Resource](https://arxiv.org/abs/2412.13377v2)

## 🎯 Purpose and Intended Users

**Goal**: The primary objective of DateLogicQA is to assess LLMs’ tokenization and understanding of dates, aiming to evaluate context-rich date interpretation.

**Target Audience**:
- ML Researchers
- Model Developers

**Tasks**:
- Temporal Reasoning

**Limitations**: N/A

## 💾 Data

**Source**: Human curated benchmark consisting of 190 questions.

**Size**: 190 questions

**Format**: N/A

**Annotation**: Curated by human evaluators.

## 🔬 Methodology

**Methods**:
- Human evaluation

**Metrics**:
- Accuracy

**Calculation**: Evaluated using human-led assessments of model responses.

**Interpretation**: Correct answers indicate successful tokenization and logical reasoning.

**Validation**: High inter-annotator agreement with a Cohen’s kappa (K) score of 0.80.

## ⚠️ Targeted Risks

**Risk Categories**:
- Bias
- Accuracy

**Atlas Risks**:
No specific atlas risks defined

**Demographic Analysis**: N/A

## 🔒 Ethical and Legal Considerations

**Privacy And Anonymity**: Not Applicable

**Data Licensing**: Not Applicable

**Consent Procedures**: Not Applicable

**Compliance With Regulations**: Not Applicable
