# MultiLoKo

## üìä Benchmark Details

**Name**: MultiLoKo

**Overview**: MultiLoKo is a new benchmark for evaluating multilinguality in LLMs covering 31 languages. It consists of locally sourced questions, human and machine translations, aiming to better assess the multilingual capabilities of language models.

**Data Type**: question-answering pairs

**Domains**:
- Natural Language Processing

**Languages**:
- English
- Arabic
- Bengali
- Cantonese
- Czech
- Dutch
- Farsi
- French
- German
- Hebrew
- Hindi
- Indonesian
- Italian
- Japanese
- Khmer
- Korean
- Malay
- Mandarin (simplified)
- Mandarin (traditional)
- Marathi
- Polish
- Portuguese
- Romanian
- Russian
- Spanish
- Swedish
- Tagalog
- Thai
- Turkish
- Urdu
- Vietnamese

**Similar Benchmarks**:
- PAWS-X
- XNLI
- XCOPA

**Resources**:
- [GitHub Repository](https://github.com/facebookresearch/multiloko/)

## üéØ Purpose and Intended Users

**Goal**: To evaluate the multilinguality of large language models and to study the effects of various design choices in multilingual evaluation.

**Target Audience**:
- ML Researchers
- Model Developers
- Industry Practitioners

**Tasks**:
- Question Answering

**Limitations**: N/A

## üíæ Data

**Source**: Sourced from the 6K most visited Wikipedia pages for each language, with questions generated by native speakers.

**Size**: 15,500 parallel questions

**Format**: JSON

**Annotation**: Questions were generated and reviewed by native speakers, ensuring local relevance and quality.

## üî¨ Methodology

**Methods**:
- Human evaluation
- Automated metrics

**Metrics**:
- Exact Match accuracy (EM)
- Gap (between best and worst performing language)
- Mother Tongue Effect (MTE)
- Locality Effect (LE)

**Calculation**: EM is calculated based on the percentage of correct answers that match the reference. Gap is the difference between the best and worst performing languages. MTE and LE are also derived from the scores of different language conditions.

**Interpretation**: Higher EM indicates better performance, while a smaller Gap indicates better parity across languages. Positive MTE indicates information is easier to access in the local language.

**Validation**: Data was split into a devset and a blind test split to assess generalization.

## ‚ö†Ô∏è Targeted Risks

**Risk Categories**:
- Bias
- Accuracy
- Fairness

**Atlas Risks**:
- **Fairness**: Data bias
- **Accuracy**: Unrepresentative data

**Demographic Analysis**: N/A

## üîí Ethical and Legal Considerations

**Privacy And Anonymity**: Not Applicable

**Data Licensing**: Not Applicable

**Consent Procedures**: Not Applicable

**Compliance With Regulations**: Not Applicable
