# Sensitive Questions and Acceptable Responses (SQUARE)

## üìä Benchmark Details

**Name**: Sensitive Questions and Acceptable Responses (SQUARE)

**Overview**: SQUARE is a large-scale Korean dataset of sensitive questions and acceptable/non-acceptable responses. The dataset contains ~49k sensitive questions with ~42k acceptable and ~46k non-acceptable responses, constructed via human-machine collaboration using HyperCLOVA and human-in-the-loop annotation based on real news headlines. It is intended to support identification and generation of acceptable responses to sensitive questions to improve safety of language models.

**Data Type**: text (question-response pairs)

**Domains**:
- Natural Language Processing

**Languages**:
- Korean
- English

**Similar Benchmarks**:
- WaNLI
- ToxiGen
- SOLID
- KOLD
- KorQuAd
- RealToxicityPrompts

**Resources**:
- [GitHub Repository](https://github.com/naver-ai/korean-safety-benchmarks)
- [Resource](https://arxiv.org/abs/2305.17696)
- [Resource](https://huggingface.co/beomi/KcELECTRA-base-v2022)
- [Resource](https://news.naver.com/main/ranking/popularDay.naver)
- [Resource](https://www1.president.go.kr/petitions)
- [Resource](https://www.bigkinds.or.kr)

## üéØ Purpose and Intended Users

**Goal**: Provide a dataset to enable safer conversations on sensitive issues by (1) identifying acceptable responses to sensitive questions (binary classification) and (2) generating acceptable responses to sensitive questions (generation), thereby improving safety of LLM outputs in such scenarios.

**Target Audience**:
- ML Researchers
- Model Developers
- Industry Practitioners

**Tasks**:
- Acceptable Response Classification
- Acceptable Response Generation

**Limitations**: SQUARE focuses on three categories of sensitive questions (contentious, ethical, predictive) and six types of acceptable responses; it does not cover all sensitive issues. Some sensitive issues may be Korean-specific and cultural differences may limit generalization to other languages/societies.

## üíæ Data

**Source**: Sensitive topic titles crawled from Ranking news (Naver News), The Blue House National Petition, and Daily Top 10 Issues at BigKinds; questions and responses generated by HyperCLOVA and then filtered and annotated by human crowd workers in an iterative human-in-the-loop process.

**Size**: 51,197 questions total (49,313 labeled sensitive; 1,884 non-sensitive); 88,657 responses total (42,629 acceptable; 46,028 non-acceptable).

**Format**: N/A

**Annotation**: Manual annotation by crowd workers (258 workers). Each labeled item annotated by three annotators; majority vote taken. Hierarchical annotation design (quality check, subjectivity, acceptability, and reasons); multiple-choice reasons allowed; human-in-the-loop iterations used to select ambiguous/challenging examples.

## üî¨ Methodology

**Methods**:
- Human evaluation
- Automated metrics
- Model-based evaluation (fine-tuning classifiers, e.g., KcELECTRA)
- Filter-based moderation experiments (generate multiple candidate responses, classify, and select best)
- Human-in-the-loop data selection and annotation

**Metrics**:
- Accuracy
- Macro-F1
- Krippendorff's alpha (inter-annotator agreement)
- Ratio of acceptable responses (human evaluation)

**Calculation**: Automated metrics (Accuracy and Macro-F1) computed on held-out test and test_ood splits for classifiers fine-tuned on the dataset. Human evaluation: each question-response pair evaluated by 3 annotators; proportion (ratio) of responses labeled 'acceptable' reported. Inter-annotator agreement measured with Krippendorff's alpha. Statistical significance of moderation effects tested with one-proportion z-tests.

**Interpretation**: An acceptable response classification accuracy below ~80% indicates the dataset is challenging. Improvements in the ratio of acceptable responses (human-evaluated) indicate effective moderation/generation methods; e.g., filter-based moderation increased acceptable response rates substantially for tested models.

**Baseline Results**: Acceptable response classifier (fine-tuned KcELECTRA) achieved 74.6% accuracy (macro-F1 74.4%) on test and 77.7% accuracy (macro-F1 76.9%) on test_ood. Filter-based moderation increased the percentage of acceptable responses measured by human evaluation: HyperCLOVA improved by about 25% and GPT-3 improved by about 16% (as reported in the paper).

**Validation**: Human evaluations conducted (reported human-eval statistics), test and test_ood splits used to evaluate generalization to unseen topics, and statistical significance tested using one-proportion z-tests (reported z and p-values in the paper).

## ‚ö†Ô∏è Targeted Risks

**Risk Categories**:
- Safety
- Fairness
- Bias
- Misinformation

**Atlas Risks**:
- **Fairness**: Output bias, Data bias
- **Value Alignment**: Toxic output, Harmful output
- **Accuracy**: Poor model accuracy
- **Robustness**: Hallucination
- **Societal Impact**: Human exploitation, Impact on cultural diversity

**Demographic Analysis**: Annotator demographics are provided for 258 crowd workers including gender, age, country of origin, domestic area, education, sexual orientation, and disability (see Table 8 in the paper).

**Potential Harm**: ['Generating offensive content', 'Reinforcing stereotypes and prejudices', 'Motivating unethical responses or behaviors', 'Disseminating misinformation (from predictive responses)', 'Potential mental harms to annotators during data construction']

## üîí Ethical and Legal Considerations

**Privacy And Anonymity**: Not Applicable

**Data Licensing**: Not Applicable

**Consent Procedures**: Not Applicable

**Compliance With Regulations**: This study was approved by the public institutional review board (IRB) affiliated with the Ministry of Health and Welfare of South Korea (P01-202211-01-016). The paper notes precautions to mitigate potential harms to annotators and reports annotator compensation details.
