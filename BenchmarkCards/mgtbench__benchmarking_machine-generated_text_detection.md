# MGTBench: Benchmarking Machine-Generated Text Detection

## üìä Benchmark Details

**Name**: MGTBench: Benchmarking Machine-Generated Text Detection

**Overview**: The paper proposes the first benchmark framework for machine-generated text (MGT) detection against powerful large language models (LLMs), named MGTBench. MGTBench is a modular framework (input, detection, evaluation) that integrates multiple metric-based and model-based detection methods, generates LLM-produced texts on existing human-written datasets using representative LLMs, and provides standardized evaluation (including robustness/adversarial testing) to compare detection and attribution methods.

**Data Type**: text (human-written texts and machine-generated texts)

**Domains**:
- Natural Language Processing

**Similar Benchmarks**:
- TURINGBENCH
- Ghostbuster

**Resources**:
- [GitHub Repository](https://github.com/xinleihe/MGTBench)
- [Resource](https://arxiv.org/abs/2303.14822)

## üéØ Purpose and Intended Users

**Goal**: Provide a benchmarking framework (MGTBench) to evaluate machine-generated text detection and text attribution methods against powerful LLMs, and to assess robustness under adversarial attacks.

**Target Audience**:
- Researchers
- Users

**Tasks**:
- Text Classification
- Text Attribution

**Limitations**: Choice of LLM/Methods/Datasets. The study concentrates on 6 representative LLMs and 13 detection methods on 3 benchmarking datasets.

## üíæ Data

**Source**: Datasets: Essay, WP, and Reuters provided by Verma et al. [36]. For each dataset entry, human-written text (HWT) and machine-generated texts (MGTs) generated by six LLMs (ChatGLM, Dolly, ChatGPT-turbo, GPT4All, StableLM, and Claude) are included.

**Size**: Essay: 1,000 examples; WP: 1,000 examples; Reuters: 1,000 examples

**Format**: N/A

**Annotation**: Texts are labeled as HWT (human-written text) and MGT (machine-generated text); MGT labels are assigned via generation from the six LLMs.

## üî¨ Methodology

**Methods**:
- Automated metrics (metric-based detection)
- Model-based evaluation (classification models)
- Ablation studies
- Adversarial robustness testing

**Metrics**:
- Accuracy
- Precision
- Recall
- F1-score
- Area Under ROC Curve (AUC)

**Calculation**: Standard classification metrics (Accuracy, Precision, Recall, F1-score, AUC) computed at sample level; F1-score is used as the main evaluation metric unless otherwise noted.

**Interpretation**: Higher F1-score indicates better detection/attribution. The paper reports that LM Detector outperforms other methods; larger text length (around 200 words) generally yields better detection performance; metric-based methods show better transferability across LLMs while model-based methods adapt better across datasets; robustness is measured by F1-score degradation under adversarial attacks.

**Baseline Results**: Example results reported: LM Detector achieves 0.993 F1-score while Log-Likelihood achieves 0.968 F1-score to differentiate human vs ChatGPT-turbo on Essay. Log-Likelihood reaches 0.970, 0.980, and 0.972 F1-score on Essay, WP, and Reuters respectively for ChatGLM-generated texts vs HWTs (as reported in Table 2).

**Validation**: Entries are randomly split with 80% for training and 20% for testing. Evaluations are performed across three datasets and six LLMs; ablation studies (text length, training sample size), transfer experiments (train/test dataset and train/test LLM shifts), and adversarial attack evaluations (paraphrasing, random spacing, adversarial perturbation) are conducted for validation.

## ‚ö†Ô∏è Targeted Risks

**Risk Categories**:
- Fairness
- Accuracy
- Robustness
- Misuse
- Legal Compliance
- Explainability
- Societal Impact

**Atlas Risks**:
- **Fairness**: Data bias
- **Accuracy**: Poor model accuracy
- **Robustness**: Evasion attack
- **Misuse**: Spreading disinformation, Dangerous use
- **Legal Compliance**: Legal accountability
- **Explainability**: Untraceable attribution
- **Transparency**: Uncertain data provenance
- **Societal Impact**: Impact on education: plagiarism

**Potential Harm**: ['Spreading misinformation / disinformation', 'Academic misuse in education (making fair judgment difficult / plagiarism)', 'Phishing and scam facilitation']

## üîí Ethical and Legal Considerations

**Privacy And Anonymity**: Not Applicable

**Data Licensing**: Not Applicable

**Consent Procedures**: Not Applicable

**Compliance With Regulations**: Not Applicable
