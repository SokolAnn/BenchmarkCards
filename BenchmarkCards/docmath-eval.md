# DOCMATH-EVAL

## üìä Benchmark Details

**Name**: DOCMATH-EVAL

**Overview**: DOCMATH-EVAL is a comprehensive benchmark specifically designed to evaluate the numerical reasoning capabilities of LLMs in the context of understanding and analyzing specialized documents containing both text and tables.

**Data Type**: question-answering pairs

**Domains**:
- Natural Language Processing
- Finance

**Languages**:
- English

**Similar Benchmarks**:
- TAT-QA
- FinQA
- TAT-HQA
- MultiHiertt

**Resources**:
- [GitHub Repository](https://github.com/yale-nlp/DocMath-Eval)
- [Resource](https://docmath-eval.github.io/)

## üéØ Purpose and Intended Users

**Goal**: The primary objective of the benchmark is to systematically evaluate LLMs‚Äô numerical reasoning ability to understand and interpret long and specialized documents.

**Target Audience**:
- ML Researchers
- Industry Practitioners
- Model Developers
- Domain Experts

**Tasks**:
- Numerical Reasoning

**Limitations**: N/A

## üíæ Data

**Source**: Constructed from scratch and adapted from existing finance QA benchmarks.

**Size**: 4,000 questions

**Format**: Python programs

**Annotation**: Annotated by human experts with a focus on clarity and correctness.

## üî¨ Methodology

**Methods**:
- Human evaluation
- Automated metrics

**Metrics**:
- Accuracy

**Calculation**: Accuracy is calculated based on the performance of evaluated LLMs on benchmark sets.

**Interpretation**: Higher accuracy indicates better model performance in solving numerical reasoning tasks over complex documents.

**Validation**: Quality validation protocols were implemented to ensure annotation precision and correctness.

## ‚ö†Ô∏è Targeted Risks

**Risk Categories**:
- Fairness
- Accuracy
- Robustness

**Atlas Risks**:
No specific atlas risks defined

**Demographic Analysis**: N/A

## üîí Ethical and Legal Considerations

**Privacy And Anonymity**: Not Applicable

**Data Licensing**: Not Applicable

**Consent Procedures**: Not Applicable

**Compliance With Regulations**: Not Applicable
