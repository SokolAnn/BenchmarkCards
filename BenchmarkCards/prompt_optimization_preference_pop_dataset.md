# Prompt Optimization Preference (POP) dataset

## üìä Benchmark Details

**Name**: Prompt Optimization Preference (POP) dataset

**Overview**: A large-scale Prompt Optimization Preference (POP) dataset of 30,000 prompt optimization preference examples, constructed by querying GPT-3.5-turbo and GPT-4 on naive prompts sampled from the Alpaca dataset and cross-validated by an external alignment model, GPT-4 self-check, and human experts. The dataset is intended to enable training of local LLM-based optimizers (FIPO) for model-agnostic prompt optimization.

**Data Type**: text (naive prompts, optional naive responses, optional ground truth responses, and contrastive optimized prompt pairs)

**Domains**:
- Natural Language Processing

**Similar Benchmarks**:
- Alpaca dataset

**Resources**:
- [GitHub Repository](https://github.com/LuJunru/FIPO_Project)
- [Resource](https://arxiv.org/abs/2402.11811)

## üéØ Purpose and Intended Users

**Goal**: To collect a large-scale prompt optimization preference dataset (POP) and develop Free-form Instruction-oriented Prompt Optimization (FIPO), a locally trained, model-agnostic optimizer that produces refined prompts to improve downstream model answer quality.

**Tasks**:
- Text Generation
- Question Answering
- Multiple-choice Question Answering
- Mathematical Reasoning
- Commonsense Reasoning

**Limitations**: 1) Overwhelmed Cheating Notes: FIPO sometimes provides overly detailed instructions ('cheating notes'), especially on mathematical questions. 2) Evaluation Metrics: Current evaluation primarily focuses on accuracy; interpretability, fairness, and ethical implications are not addressed. 3) Optimization of In-context Exemplars: FIPO does not optimize in-context examples and focuses only on task instructions.

**Out of Scope Uses**:
- Optimization of in-context exemplars (FIPO focuses on optimizing task instructions only)
- Consideration of interpretability, fairness, and broader ethical implications (these are noted as future work)

## üíæ Data

**Source**: Naive prompts sampled from the Alpaca dataset (52K diverse instructions and corresponding responses from Text-davinci-003). Contrastive POP data collected by sending naive prompt, naive response, and ground truth response to GPT-3.5-turbo and GPT-4 to obtain chosen and rejected optimized prompts (xo+, xo‚àí). GPT-4 responses are treated as ground truth responses when official human ground truth is unavailable.

**Size**: 30,000 examples

**Format**: N/A

**Annotation**: Optimized prompts generated by GPT-3.5-turbo and GPT-4; dataset quality cross-validated using an external alignment model UltraRM, GPT-4 self-judgement, and manual checking by human experts (reported win rates).

## üî¨ Methodology

**Methods**:
- Supervised Fine-tuning (SFT)
- Direct Preference Optimization (DPO)
- Identity Preference Optimization (IPO)
- Iterative Preference Learning (IPL)
- Cross-validation with UltraRM (alignment model), GPT-4 self-check, and human expert checking
- Automated evaluation on downstream benchmarks

**Metrics**:
- Accuracy
- Win Rate (percentage)

**Calculation**: Accuracy is reported for downstream benchmarks in few-shot format using strict answering templates (correct answers / total). Dataset quality is measured by win-rate percentages indicating proportions where GPT-4-generated response or GPT-4-optimized prompt is better than alternatives (reported from UltraRM, GPT-4 self-check, and human experts).

**Interpretation**: Higher accuracy on downstream benchmarks indicates more effective prompt optimization. Dataset quality is interpreted via win rates (the paper reports average win rates exceeding ~85% for response and prompt categories).

**Baseline Results**: The paper compares FIPO to APE and PromptAgent and reports superior results on most downstream tests. Example results from Table 2: Llama2-7B weighted average improved from 41.73 (Naive Prompt) to 48.10 (FIPO Optimizer); Tulu2-13B from 52.53 to 54.79; Baichuan2-13B from 52.36 to 54.35. Detailed per-benchmark tables are provided in the paper.

**Validation**: POP dataset validated via three cross-validation methods: critiques from external alignment model UltraRM, GPT-4 self-judgement, and manual checking by human experts (Table 1 reports win rates: UltraRM, GPT-4 self-check, Human Expert; average win rates exceed 85%).

## ‚ö†Ô∏è Targeted Risks

**Risk Categories**:
- Privacy
- Accuracy
- Misuse
- Governance

**Atlas Risks**:
- **Privacy**: Exposing personal information
- **Accuracy**: Poor model accuracy
- **Misuse**: Improper usage
- **Governance**: Incomplete usage definition

**Potential Harm**: ['Privacy risk from exposing sensitive information to third-party LLM services (noted as a drawback of online ad-hoc APO that FIPO aims to mitigate)', "Provision of overly detailed 'cheating notes' in optimized prompts (noted as a limitation that may misalign with intended use)"]

## üîí Ethical and Legal Considerations

**Privacy And Anonymity**: The paper notes privacy risks of online ad-hoc APO relying on external LLM services (exposing sensitive information to third-party systems) and states FIPO eliminates dependence on in-box model generators to reduce this privacy risk.

**Data Licensing**: Not Applicable

**Consent Procedures**: Not Applicable

**Compliance With Regulations**: Not Applicable
