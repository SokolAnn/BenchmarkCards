# AUTOPENBENCH

## 📊 Benchmark Details

**Name**: AUTOPENBENCH

**Overview**: AUTOPENBENCH is an open benchmark for evaluating generative agents in automated penetration testing, comprising 33 tasks that simulate vulnerable systems for the agents to attack.

**Data Type**: text

**Domains**:
- Natural Language Processing
- Cybersecurity

**Languages**:
- English

**Resources**:
- [GitHub Repository](https://github.com/lucagioacchini/auto-pen-bench)

## 🎯 Purpose and Intended Users

**Goal**: To provide a comprehensive and standard framework for evaluating and comparing generative agents in penetration testing.

**Target Audience**:
- Cybersecurity Researchers
- AI Researchers
- Penetration Testing Professionals

**Tasks**:
- Penetration Testing

**Limitations**: N/A

## 💾 Data

**Source**: 33 vulnerable Docker containers representing various cyber-attack scenarios.

**Size**: N/A

**Format**: Docker containers

**Annotation**: N/A

## 🔬 Methodology

**Methods**:
- Performance evaluation
- Task execution tracking

**Metrics**:
- Success Rate (SR)
- Progress Rate (PR)

**Calculation**: Metrics are calculated based on the agent's ability to complete defined tasks in a penetration testing scenario.

**Interpretation**: Higher metrics indicate better performance of the generative agents in completing the penetration testing tasks.

**Validation**: Evaluation is performed on two types of agents: fully autonomous and semi-autonomous.

## ⚠️ Targeted Risks

**Risk Categories**:
- Robustness
- Safety

**Atlas Risks**:
No specific atlas risks defined

## 🔒 Ethical and Legal Considerations

**Privacy And Anonymity**: Not Applicable

**Data Licensing**: Not Applicable

**Consent Procedures**: Not Applicable

**Compliance With Regulations**: Not Applicable
