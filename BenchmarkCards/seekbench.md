# SeekBench

## üìä Benchmark Details

**Name**: SeekBench

**Overview**: SeekBench is the first benchmark for evaluating the epistemic competence of LLM search agents through step-level analysis of their response traces, comprising 190 expert-annotated traces with over 1,800 response steps.

**Data Type**: question-answering pairs

**Domains**:
- Natural Language Processing

**Languages**:
- English

**Resources**:
- [Resource](https://arxiv.org/abs/2509.22391)

## üéØ Purpose and Intended Users

**Goal**: To assess search agents' epistemic behaviors reflecting sound evidence practices, including reasoning grounded in evidence, adaptive search, and calibrated decision-making under uncertainty.

**Target Audience**:
- AI Researchers
- Model Developers

**Tasks**:
- Question Answering

**Limitations**: N/A

## üíæ Data

**Source**: Expert-annotated traces generated by LLM search agents.

**Size**: 190 traces with over 1,800 response steps

**Format**: N/A

**Annotation**: Manual annotation by experts

## üî¨ Methodology

**Methods**:
- Human evaluation
- Automated metrics

**Metrics**:
- Reasoning Quality Index (RQI)
- Evidence Recovery Function (ERF)
- Calibration Error (CE)

**Calculation**: Metrics are calculated based on the annotated features of agent response traces and their epistemic behaviors.

**Interpretation**: Higher RQI indicates better reasoning quality, while lower ERF represents effective recovery from insufficient evidence, and lower CE shows better calibration.

**Validation**: Human annotators achieved high inter-annotator reliability (Cohen's Kappa Œ∫ > 0.8) and strong alignment with LLM judges.

## ‚ö†Ô∏è Targeted Risks

**Risk Categories**:
- Fairness
- Accuracy

**Atlas Risks**:
- **Accuracy**: Poor model accuracy

**Demographic Analysis**: N/A

## üîí Ethical and Legal Considerations

**Privacy And Anonymity**: Not Applicable

**Data Licensing**: Not Applicable

**Consent Procedures**: Not Applicable

**Compliance With Regulations**: Not Applicable
