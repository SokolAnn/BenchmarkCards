# EconEvals

## 📊 Benchmark Details

**Name**: EconEvals

**Overview**: We develop benchmarks for LLM agents that act in, learn from, and strategize in unknown environments, derived from key economic decision-making tasks.

**Data Type**: decision-making tasks

**Domains**:
- Economics

**Languages**:
- English

**Similar Benchmarks**:
- MMLU
- GPQA

**Resources**:
- [GitHub Repository](https://github.com/sara-fish/econ-evals-paper)

## 🎯 Purpose and Intended Users

**Goal**: To assess the capabilities and tendencies of LLM agents in economic decision-making tasks.

**Target Audience**:
- ML Researchers
- Economists
- Industry Practitioners

**Tasks**:
- Optimization
- Decision Making

**Limitations**: N/A

## 💾 Data

**Source**: Synthetic environments created based on economic models.

**Size**: 12 instances for each difficulty level (Basic, Medium, Hard)

**Format**: N/A

**Annotation**: N/A

## 🔬 Methodology

**Methods**:
- Multi-turn interactions
- Automated evaluations

**Metrics**:
- Total profit
- Worker pay
- Utility scores

**Calculation**: Scores are calculated based on the performance of LLM agents on various tasks over multiple periods.

**Interpretation**: Scores indicate the capability and tendencies of LLMs in economic settings.

**Validation**: Tested across various LLMs with established measures.

## ⚠️ Targeted Risks

**Risk Categories**:
- Fairness
- Accuracy
- Robustness

**Atlas Risks**:
- **Fairness**: Decision bias
- **Accuracy**: Unrepresentative data
- **Robustness**: Evasion attack

## 🔒 Ethical and Legal Considerations

**Privacy And Anonymity**: Not Applicable

**Data Licensing**: Not Applicable

**Consent Procedures**: Not Applicable

**Compliance With Regulations**: Not Applicable
