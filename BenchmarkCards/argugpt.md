# ArguGPT

## üìä Benchmark Details

**Name**: ArguGPT

**Overview**: ArguGPT is a carefully balanced corpus of argumentative essays consisting of 4,038 machine-generated essays (from seven GPT-family models) paired with roughly equal number of human-written essays drawn from in-class/homework exercises (WECCL), TOEFL writing tasks (TOEFL11), and GRE issue tasks. The corpus includes an out-of-distribution test set (machine essays from non-GPT models and additional human essays) and is intended to enable human evaluation, linguistic analysis, and benchmarking of AIGC detectors.

**Data Type**: text (argumentative essays)

**Domains**:
- Natural Language Processing
- Education

**Languages**:
- English

**Similar Benchmarks**:
- TuringBench
- TOEFL11
- WECCL

**Resources**:
- [GitHub Repository](https://github.com/huhailinguist/ArguGPT)
- [Resource](https://huggingface.co/spaces/SJTU-CL/argugpt-detector)
- [Resource](https://arxiv.org/abs/2304.07666)

## üéØ Purpose and Intended Users

**Goal**: Provide a balanced corpus of human and machine-authored argumentative essays to (1) establish a baseline of ESOL instructors' ability to detect AI-generated essays, (2) analyze lexical and syntactic characteristics of machine-generated essays, and (3) build and benchmark automated AIGC detectors.

**Target Audience**:
- ESOL instructors
- Corpus linguists
- Computational linguists
- Natural Language Processing researchers
- AIGC detector developers

**Tasks**:
- Text Classification
- Authorship Attribution
- Linguistic Analysis

**Limitations**: None

## üíæ Data

**Source**: Human essays: WECCL (Written English Corpus of Chinese Learners), TOEFL11, and GRE sample essays collected from GRE-prep materials. Machine essays: generated in response to the same prompts using seven GPT-family models (gpt2-xl, text-babbage-001, text-curie-001, text-davinci-001, text-davinci-002, text-davinci-003, gpt-3.5-turbo). Out-of-distribution (OOD) machine essays generated by other models (gpt-3.5-turbo (additional), gpt-4, claude-instant, bloomz-7b, flan-t5-11b).

**Size**: 4,038 machine-generated essays; 4,115 human-written essays; total 8,153 essays (in-distribution, excluding OOD). Total tokens (w/o OOD): 2,449,790 tokens. OOD test set: 500 machine essays and 500 human essays.

**Format**: N/A

**Annotation**: Essays scored and labeled into three levels (low / medium / high) using automated scoring (YouDao). TOEFL11 human essays retain their original three-level labels from dataset owners. GRE human essays scored by YouDao. Each machine-generated essay is also assigned an automated score by YouDao.

## üî¨ Methodology

**Methods**:
- Human evaluation (two-round Turing-style test with 43 ESL instructors)
- Automated classifier evaluation (fine-tuned RoBERTa-large, SVMs with linguistic features)
- Off-the-shelf detector evaluation (GPTZero, RoBERTa from Guo et al. 2023)
- Zero/few-shot evaluation with gpt-3.5-turbo
- Linguistic analyses using Lu (2010, 2012) syntactic and lexical complexity measures
- N-gram analysis with log-likelihood

**Metrics**:
- Accuracy
- Pearson correlation
- Log-likelihood

**Calculation**: Accuracy is measured as the proportion of correct classifications on held-out test sets. For GPTZero, a returned probability > 0.65 is considered AI-written (per GPTZero documentation). Pearson correlation coefficients reported for agreement between automated scoring systems (YouDao vs Pigai) (e.g., Pearson r = 0.7570 overall in pilot). Log-likelihood is used to rank n-grams overused by machines vs humans.

**Interpretation**: Higher Accuracy indicates better detection performance. Human instructors achieved 61.6% accuracy in round 1 and 67.7% after minimal self-training. High document-level accuracy (e.g., RoBERTa-large reported ~99% on in-distribution test) indicates strong in-domain detection; drops in OOD accuracy indicate limited generalization.

**Baseline Results**: Human instructors: 61.6% accuracy (round 1), 67.7% (round 2). RoBERTa-large fine-tuned on ArguGPT: ~99% document-level accuracy and ~93% sentence-level accuracy (in-distribution). SVM with function-word features: 95.14% document-level accuracy (reported best SVM). GPTZero: reported 90+% accuracy on in-distribution document and sentence levels but poor generalization on some OOD models.

**Validation**: Dataset split into train/dev/test (dev and test sets each contain 700 essays). Out-of-distribution test set of 500 machine and 500 human essays used to evaluate generalization. Additional evaluations at paragraph- and sentence-level splits.

## ‚ö†Ô∏è Targeted Risks

**Risk Categories**:
- Robustness
- Governance
- Intellectual Property
- Societal Impact
- Accuracy

**Atlas Risks**:
- **Governance**: Lack of testing diversity, Unrepresentative risk testing
- **Intellectual Property**: Data usage rights restrictions
- **Societal Impact**: Impact on education: plagiarism

**Demographic Analysis**: N/A

**Potential Harm**: ['Academic integrity violations (students submitting AI-generated essays as their own work; plagiarism/cheating)']

## üîí Ethical and Legal Considerations

**Privacy And Anonymity**: Not Applicable

**Data Licensing**: Human-authored essays are not publicly released due to copyright; only indices of human essays will be released. Machine-authored essays and models are released via GitHub (see resources).

**Consent Procedures**: Not Applicable

**Compliance With Regulations**: Not Applicable
