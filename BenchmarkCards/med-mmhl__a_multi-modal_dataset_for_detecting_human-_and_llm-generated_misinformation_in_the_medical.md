# Med-MMHL: A Multi-Modal Dataset for Detecting Human- and LLM-Generated Misinformation in the Medical Domain

## üìä Benchmark Details

**Name**: Med-MMHL: A Multi-Modal Dataset for Detecting Human- and LLM-Generated Misinformation in the Medical Domain

**Overview**: A novel multi-modal misinformation detection dataset in a general medical domain encompassing multiple diseases. Med-MMHL incorporates both human-generated misinformation and misinformation generated by LLMs (e.g., ChatGPT). The dataset is intended to facilitate research and development of methods for detecting misinformation across diverse diseases and scenarios at the sentence, document, and multi-modal levels.

**Data Type**: multimodal (text and images): news articles, claims (short articles), tweets, and associated images

**Domains**:
- Healthcare
- Natural Language Processing

**Similar Benchmarks**:
- MedHelp
- COAID
- MM-COVID
- CHECKED
- TruthSeeker
- ANTi-Vax
- COVID-Rumor
- ReCOVery
- Monant

**Resources**:
- [GitHub Repository](https://github.com/styxsys0927/Med-MMHL)
- [Resource](https://arxiv.org/abs/2306.08871)

## üéØ Purpose and Intended Users

**Goal**: Facilitate comprehensive research and development of methodologies for detecting misinformation across diverse diseases and various scenarios, including human- and LLM-generated misinformation detection at the sentence, document, and multi-modal levels.

**Tasks**:
- Text Classification (Fake News Detection - document/article-level)
- Text Classification (Fake Claim Detection - claim/short-article-level)
- Text Classification (LLM-generated Fake Sentence Detection - sentence-level)
- Text Classification (Fake Tweet Detection - social media tweets)
- Multimodal Classification (Multimodal Fake News Detection using text and images)

**Limitations**: Due to the Twitter Developer Agreement, the authors release only tweet IDs (users must rehydrate to obtain full tweet content). LLM-generated fake tweets are not generated/released due to Twitter policy. The authors did not provide explicit per-article disease labels in the released data (they conducted disease-level analysis but did not provide specific disease labels for articles/tweets).

## üíæ Data

**Source**: News articles from authoritative medical websites (ClevelandClinic, NIH, WebMD, Mayo, Healthline, ScienceDaily); fake news archived and claims extracted from fact-checking websites (AFPFactCheck, CheckYourFact, FactCheck, HealthFeedback, LeadStories, PolitiFact); tweets crawled by querying news titles (tweet IDs released per Twitter Developer Agreement); LLM-generated fake news created by prompting ChatGPT 3.5 to produce adversarial/modified versions of real news.

**Size**: 12,968 total news items; 27,633 total tweets. Breakdown (explicit in paper): Real news 4,554; Human-generated fake news 469; LLM-generated fake news 2,095; Real claims 2,283; Fake claims 3,567; Real sentences 41,365; LLM fake sentences 17,608; Real tweets 7,738; Fake tweets 6,927.

**Format**: N/A

**Annotation**: Labels derived from fact-checking websites: summaries of fake news opinions labeled as 'fake claims' and fact-check corrections labeled as 'real claims'. Human-generated fake news taken from archived fake articles; LLM-generated fake news and fake sentences produced by prompting ChatGPT 3.5 and labeled as 'LLM fake'. Tweets labeled based on association with real or fake news titles. Images collected alongside articles when available.

## üî¨ Methodology

**Methods**:
- Model-based evaluation using baseline models (text-only and multimodal baselines)
- Automated metrics (Accuracy, Precision, Recall, F1, Macro F1)
- Train/validation/test split (7:1:2) for experiments

**Metrics**:
- Accuracy
- Precision
- Recall
- F1 Score
- Macro F1 Score

**Calculation**: The paper states it adopts commonly used metrics (Accuracy, Precision, Recall, F1 and Macro F1) but does not provide further bespoke calculation details.

**Interpretation**: Pretrained transformer-based methods outperform simpler methods; dataset imbalance leads to higher false positives and lower recall; LLM-generated fake sentences are more difficult to detect than LLM-generated fake news (document-level), indicating sentence-level detection is a challenging area.

**Baseline Results**: Selected baseline results from the paper: Fake news detection (human + LLM fake): FN-BERT Acc 95.784%, BERT Acc 95.657%, BioBERT Acc 94.941%. LLM-generated fake sentence detection: SentenceBERT Acc 96.040%, DistilBERT Acc 95.149%. Multimodal fake news detection (only human-generated fake): CLIP Acc 96.324%, VisualBERT Acc 96.103%. Fake tweet detection (only human-generated fake): FN-BERT Acc 98.602%, BERT Acc 98.056%. (Full tables provided in paper Tables 3 and 4.)

**Validation**: Dataset split into training/validation/test with ratio 7:1:2. Extensive baseline experiments and analyses (text-level and embedding-level) were conducted to evaluate methods on the proposed tasks.

## ‚ö†Ô∏è Targeted Risks

**Risk Categories**:
- Misinformation
- Robustness
- Data Laws
- Misuse

**Atlas Risks**:
- **Data Laws**: Data usage restrictions
- **Misuse**: Spreading disinformation

**Potential Harm**: Detecting and preventing medical misinformation that can mislead individual treatments and influence national policies.

## üîí Ethical and Legal Considerations

**Privacy And Anonymity**: Due to the Twitter Developer Agreement, the authors only release tweet IDs and provide code to rehydrate tweets; full tweet content is not directly released by the authors.

**Data Licensing**: Not Applicable

**Consent Procedures**: Not Applicable

**Compliance With Regulations**: The dataset collection and release comply with the Twitter Developer Agreement constraints (time range and release of tweet IDs) as described in the paper.
