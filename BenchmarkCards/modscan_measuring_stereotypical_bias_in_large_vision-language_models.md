# ModSCAN (Measuring Stereotypical Bias in Large Vision-Language Models)

## üìä Benchmark Details

**Name**: ModSCAN (Measuring Stereotypical Bias in Large Vision-Language Models)

**Overview**: ModSCAN is a measurement framework developed to scan and evaluate the stereotypical bias within Large Vision-Language Models (LVLMs) regarding gender and race across different scenarios, including occupations, descriptors, and persona traits. It identifies the presence of biases and suggests mitigation strategies.

**Data Type**: image-caption pairs

**Domains**:
- Natural Language Processing
- Computer Vision

**Languages**:
- English

**Resources**:
- [Resource](https://arxiv.org/abs/2410.06967)

## üéØ Purpose and Intended Users

**Goal**: To measure and understand stereotypical bias in LVLMs and provide insights into mitigating such biases.

**Target Audience**:
- ML Researchers
- Ethics Researchers
- AI Practitioners

**Tasks**:
- Bias Measurement
- Evaluation of Vision-Language Models

**Limitations**: Focus on only gender and racial stereotypes; future work will explore other aspects.

## üíæ Data

**Source**: UTKFace dataset and images generated by Stable Diffusion (SD) version 2.1

**Size**: 2,800 images for persona traits evaluations

**Format**: JPEG

**Annotation**: The images are labeled based on gender, race, and age, facilitating analysis of stereotypical bias.

## üî¨ Methodology

**Methods**:
- Automated metrics
- Statistical analysis on model outputs

**Metrics**:
- Bias Score

**Calculation**: Bias Score is computed based on the frequency of stereotypical associations made by the LVLMs in their outputs for various attributes.

**Interpretation**: Higher bias scores indicate stronger stereotypical associations, reflecting potential biases in model outputs.

**Baseline Results**: LLaVA-v1.5 and CogVLM exhibited the highest bias scores, indicating significant stereotypical biases.

**Validation**: The framework was validated through comparative evaluation across multiple LVLMs.

## ‚ö†Ô∏è Targeted Risks

**Risk Categories**:
- Bias
- Fairness

**Atlas Risks**:
- **Fairness**: Data bias, Output bias
- **Accuracy**: Unrepresentative data

**Demographic Analysis**: Focus on gender and race; future analyses planned for additional demographics.

**Potential Harm**: Risk of perpetuating societal stereotypes and biases in decision-making processes.

## üîí Ethical and Legal Considerations

**Privacy And Anonymity**: Not Applicable

**Data Licensing**: Publicly available datasets used or generated data.

**Consent Procedures**: Not Applicable

**Compliance With Regulations**: Not Applicable
