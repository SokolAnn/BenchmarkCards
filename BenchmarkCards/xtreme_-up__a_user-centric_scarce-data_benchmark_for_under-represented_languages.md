# XTREME -UP: A User-Centric Scarce-Data Benchmark for Under-Represented Languages

## üìä Benchmark Details

**Name**: XTREME -UP: A User-Centric Scarce-Data Benchmark for Under-Represented Languages

**Overview**: XTREME -UP is a benchmark defined by: its focus on the scarce-data scenario rather than zero-shot; its focus on user-centric tasks‚Äîtasks with broad adoption by speakers of high-resource languages; and its focus on under-represented languages. XTREME -UP evaluates the capabilities of language models across 88 under-represented languages over 9 key user-centric technologies including ASR, OCR, MT, and information access tasks. The benchmark creates new datasets for OCR, autocomplete, semantic parsing, and transliteration, and builds on and refines existing datasets for other tasks. It provides methodology for evaluating text-only, multi-modal (vision, audio, and text), supervised parameter tuning, and in-context learning.

**Data Type**: text, audio, image (multimodal)

**Domains**:
- Natural Language Processing
- Computer Vision
- Speech Recognition

**Similar Benchmarks**:
- XTREME
- XGLUE
- XTREME-R

**Resources**:
- [GitHub Repository](https://github.com/google-research/xtreme-updatasets)
- [GitHub Repository](https://github.com/google-research/xtreme-up)
- [Resource](https://cloud.google.com/vision/docs/ocr)
- [GitHub Repository](https://github.com/facebookresearch/flores/tree/main/flores200)

## üéØ Purpose and Intended Users

**Goal**: To evaluate multilingual models on user-centric tasks in a few-shot (scarce-data) setting focused on under-represented languages, standardizing in-language fine-tuning and in-context learning and providing datasets and baselines for input/output and information access tasks.

**Target Audience**:
- ML Researchers
- Model Developers
- Industry Practitioners
- Domain Experts

**Tasks**:
- Automatic Speech Recognition
- Optical Character Recognition
- Autocomplete (Predictive Text)
- Transliteration
- Machine Translation
- Question Answering (in-language and cross-language)
- Retrieval for Question Answering (in-language and cross-language)
- Named Entity Recognition
- Semantic Parsing

**Limitations**: XTREME -UP focuses on a scarce-data scenario where each under-represented language has a maximum of 8 hours of annotation effort for training splits. The benchmark notes that few-shot in-context learning is less effective than fine-tuning on 100s of examples for under-represented languages and that the scalar averaged score is not a substitute for per-task or per-language analysis. The authors also note dataset-specific limitations such as the need to obtain publishers' permission to expand OCR data.

**Out of Scope Uses**:
- Using the test split for iterative evaluation or hill-climbing (the paper states: 'The test split should not be used for iterative evaluation of your models or other sorts of hill-climbing').
- Using off-the-shelf machine translation systems for data augmentation without reporting and consideration of resulting data leakage (the paper warns that use of pre-existing MT systems can leak supervised data into the experimental setup).

## üíæ Data

**Source**: XTREME -UP is composed of newly created datasets (OCR, autocomplete, semantic parsing, transliteration) and adapted/refined existing datasets including FLEURS (ASR), FLORES-101 (machine translation), TyDi QA and XOR-TyDi QA (question answering), Dakshina (transliteration), MasakhaNER / MasakhaNER 2.0 (NER), MTOP (semantic parsing), and Universal Dependencies (autocomplete).

**Size**: Per-task training totals are provided in Table 1. Examples include: ASR: 274,514 training examples; OCR: 60,947 training examples; Autocomplete: 44,554 training examples; Transliteration: 7,360 training examples; Machine Translation: 19,877 training examples; In-language QA: 59,559 training examples; Cross-language QA: 22,544 training examples; In-language Retrieval: 29,683 training examples; Cross-language Retrieval: 13,270 training examples; NER: 28,023 training examples; Semantic Parsing: 6,373 training examples. The benchmark covers 88 under-represented languages for evaluation.

**Format**: Varied by task and explicitly stated formats include JSONL and TSV for ASR transcript/ground truth pairs; image files with page transcriptions for OCR; parallel source/target text for MT and QA; CoNLL tokenized and raw text with byte-level spans for NER; JSON-like input/target records for autocomplete and transliteration.

**Annotation**: Annotation procedures vary by task and include: professional human translation and localization for semantic parsing and cross-lingual QA; human romanization for transliteration (Dakshina additions); human-annotated MasakhaNER NER data; OCR page transcriptions curated from public-domain books; ASR transcripts generated by Maestro-U and paired with ground-truth transcripts; automatic mapping of token-level NER annotations to raw text byte spans. Where applicable, annotators were professional translators or trained annotators as described in the paper.

## üî¨ Methodology

**Methods**:
- Multilingual fine-tuning (single model fine-tuned on combined training data across languages per task)
- In-context learning (5-shot in-language exemplars)
- Automated metric evaluation
- Baseline model evaluation using subword and byte-based models (mT5, ByT5) and instruction-tuned large models (Flan-PaLM)

**Metrics**:
- Character Error Rate (CER)
- chrF (character n-gram F-score)
- Accuracy@3
- Span F1
- Mean Reciprocal Rank (MRR)
- F1 Score
- Exact Match (EM)

**Calculation**: For each task, task-specific scores are computed and averaged across under-represented languages. For metrics where lower is better (e.g., CER), scores are inverted before averaging. MRR is renormalized from the 0.0‚Äì1.0 range to 0‚Äì100 before averaging. A final scalar score is obtained by averaging the task scores (average over ULs).

**Interpretation**: The averaged scalar provides a quick overall impression of a system's quality across tasks but is not a substitute for analyzing per-task, per-language, or example-type performance. Higher metric values generally indicate better performance except for CER (lower is better; inverted for averaging).

**Baseline Results**: Baseline results are provided for mT5-Base and ByT5-Base (multilingual fine-tuning) and Flan-PaLM-62B (5-shot in-context learning). ByT5-Base generally outperforms mT5-Base across most tasks. In-context learning with Flan-PaLM-62B underperforms fine-tuning in the scarce-data UL setting. Detailed baseline numbers are presented in Table 4 of the paper and per-language results are available at the project repository.

**Validation**: XTREME -UP provides train, validation, and test splits per task with deduplication and overlap avoidance across QA tasks and other datasets. For retrieval tasks, validation/test indexes were created (271k in-language passages and 447k English passages for cross-language) to enable efficient experimentation with distractors. The paper documents split creation and recommends use of train/dev/test in the customary scientific manner to avoid overfitting to the test split.

## ‚ö†Ô∏è Targeted Risks

**Risk Categories**:
- Fairness
- Accuracy
- Governance

**Atlas Risks**:
- **Accuracy**: Data contamination
- **Fairness**: Data bias
- **Governance**: Lack of data transparency

**Demographic Analysis**: Analyses show lower performance on many African languages (examples cited include Amharic, Yoruba, Ghom√°l√°'), and challenges for several South Asian and Southeast Asian languages (e.g., Lao, Khmer, Burmese). The paper provides per-language analyses and highlights the lowest-performing languages per task.

**Potential Harm**: The benchmark aims to detect and help address inequitable access to language technology for speakers of under-represented languages (the paper states that speakers of ULs will not be able to reap the benefits of language technologies unless these challenges are addressed).

## üîí Ethical and Legal Considerations

**Privacy And Anonymity**: Not Applicable

**Data Licensing**: Not Applicable

**Consent Procedures**: Not Applicable

**Compliance With Regulations**: Not Applicable
