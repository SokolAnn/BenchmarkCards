# VLM2-Bench

## 📊 Benchmark Details

**Name**: VLM2-Bench

**Overview**: VLM2-Bench is a benchmark designed to assess whether vision-language models (VLMs) can visually link matching cues across multiple images and videos, featuring 9 subtasks and over 3,000 test cases.

**Data Type**: question-answer pairs

**Domains**:
- Natural Language Processing
- Computer Vision

**Languages**:
- English

**Resources**:
- [Resource](https://vlm2-bench.github.io/)

## 🎯 Purpose and Intended Users

**Goal**: To assess the capability of vision-language models in visually linking matching cues, a skill crucial for coherent multimodal reasoning.

**Target Audience**:
- ML Researchers
- Model Developers

**Tasks**:
- Visual Cue Linking
- Question Answering

**Limitations**: N/A

## 💾 Data

**Source**: Curated from multiple sources ensuring diversity in visual cue representation.

**Size**: 3,060 question-answer pairs

**Format**: Various including True/False, multiple-choice, numerical, and open-ended

**Annotation**: Semi-automated with human verification for quality control.

## 🔬 Methodology

**Methods**:
- Automated metrics
- Human evaluation

**Metrics**:
- Accuracy

**Calculation**: Accuracy computed based on paired evaluation of question-answer correctness.

**Interpretation**: Measured accuracy signifies how well models link and identify visual cues.

**Baseline Results**: Chance-level and human-level baselines were established for comparison.

**Validation**: Verification through inter-annotator agreements resulting in a high Fleiss’ Kappa score.

## ⚠️ Targeted Risks

**Risk Categories**:
- Bias
- Safety
- Privacy
- Robustness

**Atlas Risks**:
No specific atlas risks defined

**Demographic Analysis**: N/A

## 🔒 Ethical and Legal Considerations

**Privacy And Anonymity**: Data maintained under CC-BY 4.0 license for academic use with attribution.

**Data Licensing**: CC-BY 4.0 for research purposes.

**Consent Procedures**: Not Applicable

**Compliance With Regulations**: Not Applicable
