# VGC-Bench

## 📊 Benchmark Details

**Name**: VGC-Bench

**Overview**: VGC-Bench is a benchmark designed to evaluate AI generalization in Pokémon VGC, including infrastructure for multi-agent learning, a suite of competitive baselines, and robust evaluation tools for performance, generalization, exploitability, and human interaction.

**Data Type**: human-play datasets

**Domains**:
- Artificial Intelligence
- Computer Science

**Languages**:
- English

**Resources**:
- [GitHub Repository](https://github.com/cameronangliss/VGC-Bench)

## 🎯 Purpose and Intended Users

**Goal**: To evaluate the generalization capabilities of AI agents in the challenging and combinatorially complex environment of Pokémon VGC.

**Target Audience**:
- AI Researchers
- Game Developers

**Tasks**:
- Multi-Agent Learning
- Reinforcement Learning

**Limitations**: N/A

## 💾 Data

**Source**: Human battle replays from Pokémon Showdown.

**Size**: 330,000 games

**Format**: N/A

**Annotation**: Extracted from human-play data logs.

## 🔬 Methodology

**Methods**:
- Behavior Cloning
- Reinforcement Learning
- Self-Play
- Fictitious Play
- Double Oracle

**Metrics**:
- ELO Rating
- Win Rate

**Calculation**: Win rates and ELO ratings calculated based on cross-evaluations among agents.

**Interpretation**: Higher ELO ratings and win rates indicate better performance against competing agents.

**Validation**: Extensive agent cross-evaluation against human players and other AI agents.

## ⚠️ Targeted Risks

**Risk Categories**:
- Generalization
- Performance

**Atlas Risks**:
No specific atlas risks defined

**Demographic Analysis**: N/A

## 🔒 Ethical and Legal Considerations

**Privacy And Anonymity**: Not Applicable

**Data Licensing**: Not Applicable

**Consent Procedures**: Not Applicable

**Compliance With Regulations**: Not Applicable
