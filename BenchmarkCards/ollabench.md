# OllaBench

## üìä Benchmark Details

**Name**: OllaBench

**Overview**: OllaBench is a novel evaluation framework that assesses LLMs‚Äô accuracy, wastefulness, and consistency in answering scenario-based information security compliance and non-compliance questions, built on 24 cognitive behavioral theories and empirical evidence from 38 peer-reviewed papers.

**Data Type**: scenario-based questions

**Domains**:
- Natural Language Processing
- Cybersecurity

**Languages**:
- English

**Similar Benchmarks**:
- HELM

**Resources**:
- [GitHub Repository](https://github.com/Cybonto/OllaBench)

## üéØ Purpose and Intended Users

**Goal**: To provide a tool for evaluating the performance of large language models in the context of cybersecurity compliance and non-compliance.

**Target Audience**:
- Researchers
- Application Developers

**Tasks**:
- Scenario-based Evaluation
- Compliance Assessment

**Limitations**: N/A

## üíæ Data

**Source**: Generated dataset based on cognitive behavioral theories and empirical evidence.

**Size**: 10,000 unique scenarios

**Format**: N/A

**Annotation**: N/A

## üî¨ Methodology

**Methods**:
- Scenario-based evaluation
- Automated metrics

**Metrics**:
- Categorical Accuracy

**Calculation**: Categorical accuracy is calculated by the number of correct responses divided by the total number of questions.

**Interpretation**: Higher scores indicate better performance in answering compliance/non-compliance questions.

**Baseline Results**: N/A

**Validation**: N/A

## ‚ö†Ô∏è Targeted Risks

**Risk Categories**:
- Accuracy
- Fairness

**Atlas Risks**:
- **Accuracy**: Poor model accuracy

**Demographic Analysis**: N/A

## üîí Ethical and Legal Considerations

**Privacy And Anonymity**: Not Applicable

**Data Licensing**: Not Applicable

**Consent Procedures**: Not Applicable

**Compliance With Regulations**: Not Applicable
