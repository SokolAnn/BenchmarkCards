# ContractEval

## üìä Benchmark Details

**Name**: ContractEval

**Overview**: ContractEval is the first benchmark to systematically evaluate both open-source and proprietary large language models (LLMs) on clause-level contract review tasks in legal risk identification.

**Data Type**: question-answering pairs

**Domains**:
- Natural Language Processing
- Legal

**Languages**:
- English

**Similar Benchmarks**:
- CUAD

**Resources**:
- [GitHub Repository](https://github.com/olivialiu121/ContractEval)

## üéØ Purpose and Intended Users

**Goal**: To assess the performance of large language models in identifying clause-level legal risks in commercial contracts.

**Target Audience**:
- Legal Professionals
- ML Researchers
- Model Developers

**Tasks**:
- Clause-Level Risk Identification

**Limitations**: N/A

## üíæ Data

**Source**: Contract Understanding Atticus Dataset (CUAD)

**Size**: 4,128 examples

**Format**: N/A

**Annotation**: Annotated by law students with specialized training under supervision of experienced lawyers.

## üî¨ Methodology

**Methods**:
- Automated metrics

**Metrics**:
- F1 Score
- F2 Score
- Jaccard similarity

**Calculation**: F1 Score is calculated as 2 * (Precision * Recall) / (Precision + Recall).

**Interpretation**: A higher F1 Score indicates better performance in identifying relevant clauses.

**Baseline Results**: GPT 4.1 and GPT 4.1 mini achieved the highest F1 scores of 0.641 and 0.644 respectively.

**Validation**: Evaluated on the Contract Understanding Atticus Dataset test set.

## ‚ö†Ô∏è Targeted Risks

**Risk Categories**:
- Accuracy
- Fairness

**Atlas Risks**:
No specific atlas risks defined

**Demographic Analysis**: N/A

## üîí Ethical and Legal Considerations

**Privacy And Anonymity**: Not Applicable

**Data Licensing**: Not Applicable

**Consent Procedures**: Not Applicable

**Compliance With Regulations**: Not Applicable
