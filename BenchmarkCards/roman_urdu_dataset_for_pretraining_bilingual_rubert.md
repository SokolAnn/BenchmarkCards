# Roman Urdu Dataset for Pretraining Bilingual RUBERT

## 📊 Benchmark Details

**Name**: Roman Urdu Dataset for Pretraining Bilingual RUBERT

**Overview**: This paper introduces a novel dataset of scraped tweets containing 54M tokens and 3M sentences for the resource-starved language Roman Urdu, enabling further research in NLP for this language.

**Data Type**: text

**Domains**:
- Natural Language Processing

**Languages**:
- Roman Urdu
- English

**Resources**:
- [Resource](https://www.ijunoon.com/transliteration/urdu-to-roman/)

## 🎯 Purpose and Intended Users

**Goal**: To provide a dataset and model for the Roman Urdu language, facilitating NLP tasks and research in low-resource languages.

**Target Audience**:
- ML Researchers
- Domain Experts

**Tasks**:
- Text Classification
- Sentiment Analysis

**Limitations**: N/A

## 💾 Data

**Source**: Scraped tweets from Twitter

**Size**: 3,040,153 sentences and 54,622,490 tokens

**Format**: N/A

**Annotation**: N/A

## 🔬 Methodology

**Methods**:
- Pretraining
- Evaluation on Masked Language Modeling (MLM) and Next Sentence Prediction (NSP)

**Metrics**:
- Accuracy

**Calculation**: Model performance calculated based on MLM and NSP tasks metrics from pretraining.

**Interpretation**: Higher accuracy indicates better performance in understanding and generating Roman Urdu from English.

**Baseline Results**: Performance comparison with English BERT shows that RUBERT performs with notable improvement.

**Validation**: N/A

## ⚠️ Targeted Risks

**Risk Categories**:
- Bias
- Safety

**Atlas Risks**:
No specific atlas risks defined

**Demographic Analysis**: N/A

## 🔒 Ethical and Legal Considerations

**Privacy And Anonymity**: Not Applicable

**Data Licensing**: Not Applicable

**Consent Procedures**: Not Applicable

**Compliance With Regulations**: Not Applicable
