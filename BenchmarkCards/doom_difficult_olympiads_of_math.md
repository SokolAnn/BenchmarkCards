# DOoM (Difficult Olympiads of Math)

## üìä Benchmark Details

**Name**: DOoM (Difficult Olympiads of Math)

**Overview**: This paper introduces DOoM, a new open-source benchmark designed to assess the capabilities of language models in solving mathematics and physics problems in Russian.

**Data Type**: question-answering pairs

**Domains**:
- Natural Language Processing

**Languages**:
- Russian

**Similar Benchmarks**:
- MMLU

**Resources**:
- [Resource](https://arxiv.org/abs/2509.23529)

## üéØ Purpose and Intended Users

**Goal**: To provide a structured dataset of problems ranging from school-level to Olympiad difficulty for evaluating reasoning skills in the Russian language.

**Target Audience**:
- ML Researchers
- Industry Practitioners
- Model Developers
- Domain Experts

**Tasks**:
- Mathematics Problem Solving
- Physics Problem Solving

**Limitations**: N/A

## üíæ Data

**Source**: Problems collected from Russian school textbooks and archives of school and university-level olympiads.

**Size**: N/A

**Format**: N/A

**Annotation**: N/A

## üî¨ Methodology

**Methods**:
- Automated metrics

**Metrics**:
- Accuracy

**Calculation**: The model-generated answer is compared against a known reference solution, with binary scoring assigned.

**Interpretation**: Higher scores indicate better performance in solving math and physics tasks.

**Baseline Results**: N/A

**Validation**: Initial testing of several models has shown correlations between performance and the volume of tokens used.

## ‚ö†Ô∏è Targeted Risks

**Risk Categories**:
- Accuracy
- Fairness

**Atlas Risks**:
No specific atlas risks defined

**Demographic Analysis**: N/A

**Potential Harm**: N/A

## üîí Ethical and Legal Considerations

**Privacy And Anonymity**: Not Applicable

**Data Licensing**: Not Applicable

**Consent Procedures**: Not Applicable

**Compliance With Regulations**: Not Applicable
