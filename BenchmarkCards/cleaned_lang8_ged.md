# Cleaned Lang8 GED

## 📊 Benchmark Details

**Name**: Cleaned Lang8 GED

**Overview**: This study focuses on enhancing Grammatical Error Detection (GED) using improved transformer-based models fine-tuned on a rigorously cleaned Lang8 dataset.

**Data Type**: sentence pairs

**Domains**:
- Natural Language Processing

**Languages**:
- English

**Similar Benchmarks**:
- Lang8

**Resources**:
- [GitHub Repository](https://github.com/atmabodha/OpenNLP/tree/main/Cleaned%20Lang8GED)

## 🎯 Purpose and Intended Users

**Goal**: To enhance the performance of grammatical error detection systems for second language learners.

**Target Audience**:
- ML Researchers
- Educators
- NLP Practitioners

**Tasks**:
- Grammatical Error Detection

**Limitations**: N/A

## 💾 Data

**Source**: Lang8 dataset, cleaned through rigorous data processing techniques.

**Size**: 200,000 sentence pairs

**Format**: CSV

**Annotation**: Automatically generated by cleaning through rigorous steps.

## 🔬 Methodology

**Methods**:
- Fine-tuning
- Evaluation against existing models

**Metrics**:
- F1 Score
- Accuracy
- Precision
- Recall

**Calculation**: Metrics calculated using the fine-tuned models on test datasets.

**Interpretation**: An F1 score of 0.91 indicates very good performance in GED tasks.

**Baseline Results**: BERT-base-uncased achieved an F1 score of 0.91 when trained on 180,000 sentences.

**Validation**: Models were validated per epoch; training and validation accuracy monitored.

## ⚠️ Targeted Risks

**Risk Categories**:
- Accuracy
- Fairness

**Atlas Risks**:
- **Accuracy**: Unrepresentative data
- **Fairness**: Output bias

**Demographic Analysis**: N/A

## 🔒 Ethical and Legal Considerations

**Privacy And Anonymity**: Not Applicable

**Data Licensing**: Not Applicable

**Consent Procedures**: Not Applicable

**Compliance With Regulations**: Not Applicable
