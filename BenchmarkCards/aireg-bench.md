# AIReg-Bench

## 📊 Benchmark Details

**Name**: AIReg-Bench

**Overview**: AIReg-Bench is the first benchmark dataset designed to test how well Large Language Models (LLMs) can assess compliance with the EU AI Act (AIA), consisting of 120 technical documentation excerpts annotated by legal experts.

**Data Type**: text

**Domains**:
- Legal

**Languages**:
- English

**Similar Benchmarks**:
- LegalAgentBench

**Resources**:
- [GitHub Repository](https://github.com/camlsys/aireg-bench)

## 🎯 Purpose and Intended Users

**Goal**: To provide a standardized method for quantitatively evaluating LLMs in assessing compliance with the EU AI Act.

**Target Audience**:
- ML Researchers
- Legal Compliance Experts
- AI Practitioners

**Tasks**:
- Compliance Assessment

**Limitations**: N/A

## 💾 Data

**Source**: Generated by an LLM and reviewed by legal experts.

**Size**: 120 excerpts

**Format**: text

**Annotation**: Annotated by legal experts to indicate compliance with specific articles of the AIA.

## 🔬 Methodology

**Methods**:
- Human evaluation
- Automated metrics

**Metrics**:
- Cohen's Kappa
- Spearman's Rank Correlation

**Calculation**: Metrics are calculated based on the agreement between LLM outputs and human expert annotations.

**Interpretation**: Higher values indicate better alignment between LLM assessments and human expert judgments.

**Baseline Results**: Gemini 2.5 Pro achieved the highest Kappa score of 0.863.

**Validation**: Validated through evaluation of 10 frontier LLMs using the dataset.

## ⚠️ Targeted Risks

**Risk Categories**:
- Bias
- Accuracy

**Atlas Risks**:
- **Accuracy**: Poor model accuracy

**Demographic Analysis**: N/A

## 🔒 Ethical and Legal Considerations

**Privacy And Anonymity**: Not Applicable

**Data Licensing**: Not Applicable

**Consent Procedures**: Not Applicable

**Compliance With Regulations**: Not Applicable
