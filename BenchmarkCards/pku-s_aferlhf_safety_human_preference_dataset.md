# PKU-S AFERLHF (Safety Human Preference Dataset)

## üìä Benchmark Details

**Name**: PKU-S AFERLHF (Safety Human Preference Dataset)

**Overview**: The PKU-S AFERLHF dataset is designed to promote research on safety alignment in large language models by providing annotated question-answering pairs with safety meta-labels for multiple harm categories.

**Data Type**: question-answering pairs

**Domains**:
- Natural Language Processing

**Languages**:
- English

**Similar Benchmarks**:
- SafeRLHF
- BEAVER-TAILS

**Resources**:
- [Resource](https://huggingface.co/datasets/PKU-Alignment/PKU-SafeRLHF)

## üéØ Purpose and Intended Users

**Goal**: To advance LLM safety alignment efforts through a comprehensive dataset that categorizes harmful content and aligns models with human safety preferences.

**Target Audience**:
- AI Researchers
- ML Practitioners

**Tasks**:
- Safety Alignment
- Preference Learning

**Limitations**: The dataset is relatively small compared to large-scale datasets constructed by commercial organizations, and it may not cover all possible harm categories comprehensively.

## üíæ Data

**Source**: The dataset includes prompts generated by the Llama family models and responses with annotations by a team of human annotators.

**Size**: 265,000 question-answer pairs and 166,800 preference data

**Format**: JSON

**Annotation**: Dual- and single-preference annotations with safety meta-labels provided by human annotators and AI collaboration.

## üî¨ Methodology

**Methods**:
- Human evaluation
- Automated metrics

**Metrics**:
- Accuracy
- Precision
- Recall
- F1 Score

**Calculation**: Metrics are calculated based on the performance of large language models on the annotated question-answering pairs.

**Interpretation**: Higher scores in metrics indicate better model performance in safety alignment and preference adherence.

**Validation**: Data undergoes quality checks and human validation to ensure reliability and accuracy.

## ‚ö†Ô∏è Targeted Risks

**Risk Categories**:
- Safety
- Bias

**Atlas Risks**:
- **Fairness**: Data bias
- **Robustness**: Evasion attack

**Demographic Analysis**: The dataset includes diverse demographic considerations, but regional biases may still exist.

**Potential Harm**: ['Harmful content detection', 'Enhancing safety in model applications']

## üîí Ethical and Legal Considerations

**Privacy And Anonymity**: The dataset design considers privacy principles and IRB approval has been sought for data usage.

**Data Licensing**: CC BY-NC 4.0

**Consent Procedures**: Annotation procedures were conducted with informed consent from annotators.

**Compliance With Regulations**: The dataset complies with applicable ethical standards in AI research.
