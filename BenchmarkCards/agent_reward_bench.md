# AGENT REWARD BENCH

## üìä Benchmark Details

**Name**: AGENT REWARD BENCH

**Overview**: AGENT REWARD BENCH is the first benchmark to assess the effectiveness of LLM judges for evaluating web agents. It contains 1302 trajectories across 5 benchmarks and 4 LLMs, aimed at determining the capability of LLMs in automatic evaluation of web agents.

**Data Type**: trajectories

**Domains**:
- Natural Language Processing

**Languages**:
- English

**Resources**:
- [Resource](https://agent-reward-bench.github.io)

## üéØ Purpose and Intended Users

**Goal**: To determine the effectiveness of LLM judges for evaluating web agent trajectories.

**Target Audience**:
- Researchers
- Practitioners

**Tasks**:
- Evaluation of Automatic Evaluations

**Limitations**: N/A

## üíæ Data

**Source**: Expert-annotated trajectories from multiple web agent benchmarks.

**Size**: 1302 trajectories

**Format**: N/A

**Annotation**: Annotated by expert reviewers following a structured evaluation framework.

## üî¨ Methodology

**Methods**:
- Comparison against expert annotations
- Performance measurement of LLM judges

**Metrics**:
- Precision
- Recall
- F1 Score

**Calculation**: Precision is calculated as the ratio of true positives over all predicted positives.

**Interpretation**: Higher precision indicates better performance in classifying successful trajectories.

**Baseline Results**: N/A

**Validation**: Inter-annotator agreement of 89.3% on success.

## ‚ö†Ô∏è Targeted Risks

**Risk Categories**:
- Accuracy
- Fairness
- Robustness

**Atlas Risks**:
No specific atlas risks defined

**Demographic Analysis**: N/A

**Potential Harm**: N/A

## üîí Ethical and Legal Considerations

**Privacy And Anonymity**: Not Applicable

**Data Licensing**: Not Applicable

**Consent Procedures**: Not Applicable

**Compliance With Regulations**: Not Applicable
