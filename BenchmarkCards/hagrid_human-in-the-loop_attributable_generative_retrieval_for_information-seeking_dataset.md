# HAGRID (Human-in-the-loop Attributable Generative Retrieval for Information-seeking Dataset)

## üìä Benchmark Details

**Name**: HAGRID (Human-in-the-loop Attributable Generative Retrieval for Information-seeking Dataset)

**Overview**: A new dataset for building end-to-end generative information-seeking models that are capable of retrieving candidate quotes and generating attributed explanations. HAGRID is constructed based on human and LLM collaboration: answers are generated by an LLM (GPT-3.5) conditioned on relevant passages from the English subset of MIRACL, and human annotators evaluate the generated explanations for informativeness and attributability.

**Data Type**: question-answering pairs with supporting passages (quotes) and attributed explanatory answers (text)

**Domains**:
- Natural Language Processing
- Information Retrieval

**Languages**:
- English

**Similar Benchmarks**:
- MIRACL
- ALCE
- SQuAD
- Natural Questions
- ELI5
- TYDIQA
- ERASER

**Resources**:
- [GitHub Repository](https://github.com/project-miracl/hagrid)
- [Resource](https://arxiv.org/abs/2307.16883)

## üéØ Purpose and Intended Users

**Goal**: To establish a dataset for building open-source end-to-end search models capable of retrieving candidate quotes and generating attributable answers, enabling training and evaluation of information-seeking models with attribution capabilities.

**Target Audience**:
- Researchers
- Model Developers
- Open-source community

**Tasks**:
- Question Answering
- Information Retrieval
- Text Generation (generative information-seeking / retrieval-augmented generation)

**Limitations**: The scope is on information-seeking scenarios that mainly inquire about factual statements and does not cover more challenging questions requiring multi-hop reasoning or discrete reasoning. HAGRID covers only English.

## üíæ Data

**Source**: Built on the English subset of MIRACL (Zhang et al., 2022). Quotes are derived from English Wikipedia passages in MIRACL. Answers were generated using GPT-3.5 (gpt-3.5-turbo-0301). Human annotators evaluated informativeness and attributability.

**Size**: HAGRID training set: 1,922 questions; development set: 716 questions. Generated answers (prior to full annotation): 3,214 answers (train) and 1,318 answers (dev). Citations generated within answers: 6,577 (train) and 3,305 (dev).

**Format**: N/A

**Annotation**: Human annotation by 4 specialist annotators (1+ year experience). Annotators were interviewed and trained via onboarding sessions. Annotators were remunerated at $15.2 USD per hour. The project required approximately 1,400 annotation hours. Annotation labels: informativeness and attributability (detailed sentence-level preprocessing and grouping described).

## üî¨ Methodology

**Methods**:
- Model-generated answers using GPT-3.5 (gpt-3.5-turbo-0301) conditioned on provided quotes
- Human evaluation/annotation for informativeness and attributability
- Automated post-processing and format verification using regular expressions

**Metrics**:
- Informativeness (binary label)
- Attributability (binary label)
- BERTScore
- Coverage (percentage of words in answer also present in quotes)
- Density (average length of text fragments from quotes that subsume answer words)

**Calculation**: Informativeness: if at least one sentence within an answer is labelled informative, the entire answer is deemed informative. Attributability: an answer sentence is labelled attributable only if it is fully supported by a cited quote; if multiple citations appear in a sentence, all cited quotes must contain ample evidence; an answer is deemed attributable when all sentences are labelled attributable. Coverage measures the percentage of words in the answer that are also present in the quotes. Density quantifies the average length of text fragments from the quotes that subsume answer words. BERTScore is used to measure semantic similarity between answer sentences and corresponding cited sentences.

**Interpretation**: An answer is considered informative if at least one sentence is labelled informative; an answer is considered attributable only if all its sentences are supported by cited quotes. Higher coverage indicates greater lexical overlap (more extractive answers). BERTScore higher median indicates greater semantic similarity between answers and cited quotes. Reported distributions and percentages are used to analyze attributability and informativeness.

**Baseline Results**: Using GPT-3.5 generated answers (prior to human annotation): Train: 3,214 generated answers; Dev: 1,318 generated answers. Annotation results (final HAGRID): Informativeness - Train: 84% marked 'yes' (2,704/3,214), Dev: 90% marked 'yes' (1,179/1,318). Attributability - Train: 73% marked 'yes' (547/754 annotated for attributability), Dev: 71% marked 'yes' (826/1,157 annotated for attributability).

**Validation**: Human annotation with 4 specialist annotators who underwent interviews and task-specific training. Post-processing included parsing answers into sentences and grouping sentences by citation presence; format verification of model outputs via regular expressions. Not all generated answers were annotated for attributability due to budget constraints (partial annotation percentages provided).

## ‚ö†Ô∏è Targeted Risks

**Risk Categories**:
- Accuracy
- Fairness
- Robustness

**Atlas Risks**:
- **Robustness**: Hallucination
- **Fairness**: Output bias

**Potential Harm**: ['Misinformation (LLMs can generate text lacking sufficient grounding, posing risks of misinformation and hallucination)', 'Biased outputs / generation of stereotypes (models constructed on HAGRID may inadvertently produce biased outputs due to tendencies of LLMs)']

## üîí Ethical and Legal Considerations

**Privacy And Anonymity**: Not Applicable

**Data Licensing**: HAGRID is publicly released under the Apache 2.0 License.

**Consent Procedures**: Not Applicable

**Compliance With Regulations**: Not Applicable
