# Forensics-Bench

## üìä Benchmark Details

**Name**: Forensics-Bench

**Overview**: Forensics-Bench is a new forgery detection benchmark suite designed to assess Large Vision Language Models (LVLMs) across a wide range of forgery detection tasks, requiring comprehensive recognition, location, and reasoning capabilities on diverse forgeries.

**Data Type**: multi-choice visual questions

**Domains**:
- Computer Vision

**Languages**:
- English

**Similar Benchmarks**:
- FakeBench
- MMFakeBench
- MFC-Bench

**Resources**:
- [Resource](N/A)

## üéØ Purpose and Intended Users

**Goal**: To comprehensively evaluate the discerning capabilities of LVLMs on forgery media.

**Target Audience**:
- ML Researchers
- Model Developers

**Tasks**:
- Forgery Detection

**Limitations**: N/A

## üíæ Data

**Source**: Curated dataset of forgery images and associated multi-choice questions.

**Size**: 63,292 samples

**Format**: N/A

**Annotation**: Meticulously annotated with multi-choice questions.

## üî¨ Methodology

**Methods**:
- Human evaluation
- Automated metrics

**Metrics**:
- Accuracy

**Calculation**: Accuracy calculated based on the number of correct responses over total questions.

**Interpretation**: Higher accuracy indicates better performance of the LVLM in forgery detection tasks.

**Validation**: Evaluated using competitive performance metrics against state-of-the-art LVLMs.

## ‚ö†Ô∏è Targeted Risks

**Risk Categories**:
- Accuracy
- Bias

**Atlas Risks**:
- **Accuracy**: Unrepresentative data, Poor model accuracy
- **Fairness**: Data bias

**Demographic Analysis**: N/A

## üîí Ethical and Legal Considerations

**Privacy And Anonymity**: Not Applicable

**Data Licensing**: Not Applicable

**Consent Procedures**: Not Applicable

**Compliance With Regulations**: Not Applicable
