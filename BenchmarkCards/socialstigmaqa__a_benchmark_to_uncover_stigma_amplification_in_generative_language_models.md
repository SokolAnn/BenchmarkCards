# SocialStigmaQA: A Benchmark to Uncover Stigma Amplification in Generative Language Models

## üìä Benchmark Details

**Name**: SocialStigmaQA: A Benchmark to Uncover Stigma Amplification in Generative Language Models

**Overview**: SocialStigmaQA is a comprehensive benchmark designed to capture the amplification of social bias via stigmas in generative language models. It consists of roughly 10,360 prompts involving various prompt styles to systematically test for both social bias and model robustness.

**Data Type**: question-answering pairs

**Domains**:
- Natural Language Processing

**Languages**:
- English

**Similar Benchmarks**:
- BBQ
- UnQover

**Resources**:
- [Resource](https://arxiv.org/abs/2312.07492)

## üéØ Purpose and Intended Users

**Goal**: To measure biases against 93 social stigmas in a question-answering format and to uncover trends in stigma amplification in language models.

**Target Audience**:
- ML Researchers
- Model Auditors
- Ethics Researchers

**Tasks**:
- Question Answering

**Limitations**: N/A

## üíæ Data

**Source**: Curated from a documented list of 93 US-centric stigmas, constructed into question-answering formats.

**Size**: 10,360 prompts

**Format**: JSON

**Annotation**: Manually curated and crafted templates by the authors.

## üî¨ Methodology

**Methods**:
- Automated metrics
- Human evaluation

**Metrics**:
- Bias proportion

**Calculation**: Quantified by calculating the proportion of socially biased output from responses to the prompt based on different decoding strategies.

**Interpretation**: Higher proportions indicate greater amplification of bias, with varying results based on prompt styles.

**Baseline Results**: Proportion of socially biased outputs ranges from 45% to 59% depending on the model and prompt style used.

**Validation**: Results validated via manual inspection of generated outputs.

## ‚ö†Ô∏è Targeted Risks

**Risk Categories**:
- Bias
- Fairness
- Privacy

**Atlas Risks**:
- **Fairness**: Data bias
- **Privacy**: Personal information in prompt

**Demographic Analysis**: The benchmark includes a demographic breakdown concerning the stigmas assessed.

**Potential Harm**: The benchmark is designed to address the potential harmful effects of biased outputs generated by language models.

## üîí Ethical and Legal Considerations

**Privacy And Anonymity**: The dataset focuses on social stigmas, hence it deals primarily with public perceptions rather than private details.

**Data Licensing**: Not Applicable

**Consent Procedures**: Not Applicable

**Compliance With Regulations**: Not Applicable
