# Synthetic MediaLiteracy Test (SMeL Test)

## 📊 Benchmark Details

**Name**: Synthetic MediaLiteracy Test (SMeL Test)

**Overview**: The SMeL Test is a benchmark that tests the ability of language models to actively filter out untrustworthy information in context. It evaluates how consistently models prioritize reliable sources over unreliable ones.

**Data Type**: text

**Domains**:
- Natural Language Processing

**Languages**:
- English

**Resources**:
- [GitHub Repository](https://github.com/username/repository)

## 🎯 Purpose and Intended Users

**Goal**: To measure how language models judge the trustworthiness of information presented in context.

**Target Audience**:
- ML Researchers
- Model Developers
- AI Practitioners

**Tasks**:
- Ignoring dubious sources
- Resolving contradictions
- Active filtering

**Limitations**: N/A

## 💾 Data

**Source**: Synthetic documents generated in the style of various sources including academic, news, and fictional writings.

**Size**: N/A

**Format**: N/A

**Annotation**: Documents were generated using models to ensure unique and controlled content.

## 🔬 Methodology

**Methods**:
- Automated metrics
- Evaluation of model outputs

**Metrics**:
- Hallucination rate

**Calculation**: Hallucination rates are calculated based on how often models incorrectly include unreliable information in responses.

**Interpretation**: Lower hallucination rates indicate better performance in prioritizing trustworthy over untrustworthy sources.

**Validation**: Models were validated on both synthetic and real datasets to assess generalizability.

## ⚠️ Targeted Risks

**Risk Categories**:
- Accuracy
- Fairness

**Atlas Risks**:
No specific atlas risks defined

**Demographic Analysis**: N/A

**Potential Harm**: N/A

## 🔒 Ethical and Legal Considerations

**Privacy And Anonymity**: Not Applicable

**Data Licensing**: Not Applicable

**Consent Procedures**: Not Applicable

**Compliance With Regulations**: Not Applicable
