# VISA (Visual Scene-Aware Machine Translation)

## 📊 Benchmark Details

**Name**: VISA (Visual Scene-Aware Machine Translation)

**Overview**: VISA is a new dataset that consists of 40k Japanese–English parallel sentence pairs and corresponding video clips, focusing on ambiguous subtitles from movies and TV episodes to facilitate multimodal machine translation research.

**Data Type**: parallel sentence pairs and video clips

**Domains**:
- Natural Language Processing

**Languages**:
- Japanese
- English

**Resources**:
- [GitHub Repository](https://github.com/ku-nlp/VISA)

## 🎯 Purpose and Intended Users

**Goal**: To construct a large-scale ambiguous parallel subtitles dataset for visual scene-aware machine translation research.

**Target Audience**:
- Research Community

**Tasks**:
- Machine Translation

**Limitations**: N/A

## 💾 Data

**Source**: Japanese–English parallel subtitles from the OpenSubtitles dataset and video clips from movies or TV episodes.

**Size**: 39,880 parallel subtitle pairs

**Format**: N/A

**Annotation**: Selected ambiguous subtitles verified through crowdsourcing for accuracy.

## 🔬 Methodology

**Methods**:
- Crowdsourced annotation

**Metrics**:
- BLEU
- METEOR
- RIBES

**Calculation**: Metrics were calculated using standard formulas for evaluation.

**Interpretation**: Higher scores indicate better translation quality.

**Validation**: Evaluated using established metrics against baseline VMT models.

## ⚠️ Targeted Risks

**Risk Categories**:
- Accuracy
- Fairness

**Atlas Risks**:
No specific atlas risks defined

**Demographic Analysis**: N/A

**Potential Harm**: N/A

## 🔒 Ethical and Legal Considerations

**Privacy And Anonymity**: Not Applicable

**Data Licensing**: Not Applicable

**Consent Procedures**: Not Applicable

**Compliance With Regulations**: Not Applicable
