# FELM (Factual Evaluation of large Language Models)

## üìä Benchmark Details

**Name**: FELM (Factual Evaluation of large Language Models)

**Overview**: FELM is a benchmark for evaluating the factuality of text generated by large language models (LLMs) across diverse domains. It involves collecting LLM-generated responses and annotating them with fine-grained factuality labels.

**Data Type**: Text

**Domains**:
- World Knowledge
- Science and Technology
- Math
- Writing and Recommendation
- Reasoning

**Similar Benchmarks**:
- HaluEval
- FactCC
- FEVER

**Resources**:
- [GitHub Repository](https://github.com/hkust-nlp/felm)

## üéØ Purpose and Intended Users

**Goal**: To provide a benchmark that aids in the assessment and improvement of factuality evaluators for large language models.

**Target Audience**:
- Researchers
- Practitioners in AI and NLP
- Developers of LLMs

**Tasks**:
- Assess factuality of generated text
- Guide development of reliable LLMs
- Conduct meta-evaluation of LLMs' factuality

**Limitations**: The dataset is collected solely from ChatGPT, which might affect the generalizability of the findings.

## üíæ Data

**Source**: Responses generated by ChatGPT

**Size**: 817 samples and 3948 segments

**Format**: Annotated text segments with factual labels

**Annotation**: Annotated with factuality labels, error types, and reference links.

## üî¨ Methodology

**Methods**:
- Prompt collection from various sources
- Response generation using ChatGPT
- Segmentation of responses into text spans
- Human annotation of factuality and error types

**Metrics**:
- F1 score
- Balanced classification accuracy

**Calculation**: F1 score is calculated based on the detection of factual errors at both segment-level and response-level.

**Interpretation**: The F1 score and balanced accuracy provide insights into the effectiveness of factuality evaluators.

**Baseline Results**: Current LLMs show unsatisfactory performance in detecting factual errors.

**Validation**: Annotation reviewed by experts and high agreement rates reported.

## ‚ö†Ô∏è Targeted Risks

**Risk Categories**:
- Accuracy
- Transparency
- Fairness
- Robustness
- Privacy

**Atlas Risks**:
- **Transparency**: Lack of training data transparency, Uncertain data provenance
- **Accuracy**: Poor model accuracy
- **Fairness**: Data bias
- **Robustness**: Data poisoning
- **Privacy**: Personal information in data

## üîí Ethical and Legal Considerations

**Privacy And Anonymity**: All annotated responses are treated to ensure privacy and anonymity.

**Data Licensing**: Data is collected from publicly available LLM outputs.

**Consent Procedures**: Not Applicable

**Compliance With Regulations**: Not Applicable
