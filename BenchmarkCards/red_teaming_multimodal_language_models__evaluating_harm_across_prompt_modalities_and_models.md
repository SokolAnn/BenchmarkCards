# Red Teaming Multimodal Language Models: Evaluating Harm Across Prompt Modalities and Models

## üìä Benchmark Details

**Name**: Red Teaming Multimodal Language Models: Evaluating Harm Across Prompt Modalities and Models

**Overview**: This paper introduces a novel adversarial benchmarking dataset including 726 prompts (half text-only, half multimodal) authored by 26 expert red teamers. The study evaluates the responses of four commercial MLLMs using human ratings from 17 annotators across 2,904 outputs, yielding over 47,000 annotations.

**Data Type**: adversarial prompts

**Domains**:
- Natural Language Processing

**Languages**:
- English

**Similar Benchmarks**:
- RealToxicityPrompts
- MultiStAR

**Resources**:
- [Resource](https://doi.org/10.48550/arXiv.2410.07073)
- [Resource](https://www.anthropic.com/news/claude-3-5-sonnet)
- [Resource](https://doi.org/10.48550/arXiv.2308.12966)

## üéØ Purpose and Intended Users

**Goal**: To systematically evaluate the robustness of four leading multimodal large language models (MLLMs) to adversarial prompts across text-only and multimodal formats.

**Target Audience**:
- ML Researchers
- Safety Engineers
- AI Developers

**Tasks**:
- Adversarial Text Generation

**Limitations**: The adversarial prompt pool targeted only three harm categories (illegal activity, disinformation, and unethical behaviour). Language and cultural context may influence harm judgments.

## üíæ Data

**Source**: Generated by a team of 26 red teamers using established scenarios and guidelines.

**Size**: 726 prompts

**Format**: N/A

**Annotation**: Rated by 17 trained annotators on a 5-point scale of harmfulness.

## üî¨ Methodology

**Methods**:
- Human evaluation
- Statistical analysis

**Metrics**:
- Attack Success Rate (ASR)
- Harmfulness ratings

**Calculation**: Calculated using generalized linear mixed models (GLMMs) and ratings from annotators.

**Interpretation**: Higher ASR indicates a greater vulnerability to adversarial prompts.

**Baseline Results**: Pixtral 12B showed a 62% ASR, while Claude Sonnet 3.5 showed a 10% ASR.

**Validation**: Validated through statistical analysis of ASR and annotator reliability.

## ‚ö†Ô∏è Targeted Risks

**Risk Categories**:
- Safety
- Robustness

**Atlas Risks**:
- **Robustness**: Evasion attack
- **Fairness**: Output bias

**Demographic Analysis**: N/A

**Potential Harm**: ['Harmfulness from unsafe model outputs']

## üîí Ethical and Legal Considerations

**Privacy And Anonymity**: All outputs were contained within secure research environments and not released publicly.

**Data Licensing**: Not Applicable

**Consent Procedures**: Not Applicable

**Compliance With Regulations**: Not Applicable
