# SuperCLUE: A Comprehensive Chinese Large Language Model Benchmark

## üìä Benchmark Details

**Name**: SuperCLUE: A Comprehensive Chinese Large Language Model Benchmark

**Overview**: SuperCLUE encompasses three sub-tasks: actual users‚Äô queries and ratings derived from an LLM battle platform (CArena), open-ended questions with single and multiple-turn dialogues (OPEN), and closed-ended questions with the same stems as open-ended single-turn ones (CLOSE). It is constructed to predict LLMs' performances on a diverse set of abilities in real Chinese scenarios and to study the relationship between closed-ended accuracy and human preferences.

**Data Type**: question-answering pairs (open-ended single- and multi-turn dialogues), closed-ended multiple-choice questions, and user interaction/rating data

**Domains**:
- Natural Language Processing

**Languages**:
- Chinese

**Similar Benchmarks**:
- CLUE
- MMLU
- Big-Bench
- HELM
- MMCU
- AGIEval
- C-Eval
- MT-bench
- Chatbot Arena

**Resources**:
- [Resource](https://www.CLUEbenchmarks.com)

## üéØ Purpose and Intended Users

**Goal**: Construct a benchmark that predicts LLMs‚Äô performances on a diverse set of abilities in real Chinese scenarios and to analyze the relationship between closed-ended accuracy and human preferences.

**Target Audience**:
- Model Developers
- Community Users

**Tasks**:
- Question Answering
- Dialogue (Multi-turn)
- Instruction Following
- Multiple-choice Question Answering
- Human Preference Evaluation

**Limitations**: Human evaluation is time-consuming and cost-intensive and thus hard to scale. Closed-ended multi-choice questions alone are insufficient to reflect human preferences in open interactive scenarios. Limited access prevented inclusion of some Chinese LLMs (e.g., Wenxin Yiyan, 360 Brain, SparkDesk) on the platform.

## üíæ Data

**Source**: CArena (LangYa Leaderboard model battle platform) providing user-model interactions with user-reported ratings; OPEN dataset of open-ended questions (OPEN SINGLE and OPEN MULTIPLE); CLOSE dataset derived from OPEN SINGLE by GPT-3.5 generation and human proofreading.

**Size**: CArena: 9.9k queries with ratings; OPEN: 600 questions total (300 single-turn, 300 multi-turn); CLOSE: 300 closed-ended questions (one per OPEN single-turn question).

**Format**: N/A

**Annotation**: Initial manual annotation of 300 entries (30 per capability category); trained a BERT classifier on these annotations to label remaining data; final labels reviewed and corrected by four human evaluators; CLOSE items generated by GPT-3.5 and proofread/verified by humans in a three-stage review process (each stage with three human reviewers).

## üî¨ Methodology

**Methods**:
- Zero-shot evaluation
- Human evaluation (model battle platform CArena, user self-reported ratings)
- Automatic evaluation using GPT-4 as a judge (pairwise comparisons)
- Closed-ended accuracy evaluation

**Metrics**:
- Accuracy
- Average win rate (win and tie rate)
- Pearson correlation
- Spearman correlation

**Calculation**: Closed-ended multi-choice: classification accuracy. Open-ended and CArena: average win rate (average of win and tie rates) against other models. OPEN automatic evaluation: pairwise comparisons judged by GPT-4. CArena uses user self-reported ratings from LangYa Leaderboard (Elo-based pairing).

**Interpretation**: Higher classification accuracy or higher average win rate indicates better model performance. The paper finds closed-ended accuracy is concentrated and less discriminative for Chinese LLMs and may not reflect human preferences; combining CLOSE and OPEN (especially OPEN MULTIPLE) yields higher correlation with user preferences in CArena. Agreement between GPT-4 judgments and human raters (Pearson correlation) is reported as 0.80.

**Baseline Results**: GPT-4: CLOSE 70.67% accuracy; OPEN SINGLE win&tie 94.52%; OPEN MULTI 94.87%; OPEN ALL 94.64%. MiniMax: CLOSE 60.67%; OPEN SINGLE 65.32%; OPEN MULTI 47.34%; OPEN ALL 57.94%; CArena 86.69%. ChatGLM2-6B: CLOSE 57.67%; OPEN SINGLE 42.33%; OPEN MULTI 30.67%; OPEN ALL 36.50%; CArena 85.63%.

**Validation**: Validated against user self-reported ratings from CArena treated as gold standard. Agreement between GPT-4 automatic judgments and human reviewers measured with Pearson correlation = 0.80. CLOSE conversion underwent a three-stage human review process (each stage with three human reviewers).

## ‚ö†Ô∏è Targeted Risks

**Risk Categories**:
- Safety
- Accuracy

**Atlas Risks**:
No specific atlas risks defined

**Demographic Analysis**: N/A

## üîí Ethical and Legal Considerations

**Privacy And Anonymity**: Interactions on the LangYa Leaderboard are anonymous and user-reported ratings are collected from anonymized interactions.

**Data Licensing**: Not Applicable

**Consent Procedures**: Not Applicable

**Compliance With Regulations**: Not Applicable
