# SPARK (Multi-Vision Sensor Perception and Reasoning Benchmark)

## üìä Benchmark Details

**Name**: SPARK (Multi-Vision Sensor Perception and Reasoning Benchmark)

**Overview**: SPARK is designed to evaluate multi-vision input Large Vision-Language Models (LVLMs) on two fronts: multi-vision perception and multi-vision reasoning. It generates 6,248 vision-language test samples to investigate multi-vision sensory perception and reasoning related to physical sensor knowledge proficiency.

**Data Type**: vision-language test samples

**Domains**:
- Computer Vision

**Languages**:
- English

**Similar Benchmarks**:
- MME
- MMBench
- LVLM-eHub
- SEED-Bench

**Resources**:
- [GitHub Repository](https://github.com/top-yun/SPARK)

## üéØ Purpose and Intended Users

**Goal**: To evaluate the ability of Large Vision-Language Models (LVLMs) to understand and process multi-vision sensory inputs.

**Target Audience**:
- ML Researchers
- Industry Practitioners
- Model Developers

**Tasks**:
- Multi-Vision Perception
- Multi-Vision Reasoning

**Limitations**: N/A

## üíæ Data

**Source**: Generated from various multi-vision sensor datasets including MS-COCO, M3FD, and medical imaging datasets.

**Size**: 6,248 test samples

**Format**: N/A

**Annotation**: Manually crafted questions and answers related to multi-vision sensors.

## üî¨ Methodology

**Methods**:
- Accuracy assessment
- Human evaluation

**Metrics**:
- Accuracy

**Calculation**: Measurement of the proportion of correctly answered questions by the models.

**Interpretation**: The models' ability to understand the given inputs and the complexity of reasoning they can perform.

**Validation**: Efficacy of the benchmark tested against ten leading LVLMs.

## ‚ö†Ô∏è Targeted Risks

**Risk Categories**:
- Accuracy

**Atlas Risks**:
- **Accuracy**

## üîí Ethical and Legal Considerations

**Privacy And Anonymity**: Not Applicable

**Data Licensing**: Not Applicable

**Consent Procedures**: Not Applicable

**Compliance With Regulations**: Not Applicable
