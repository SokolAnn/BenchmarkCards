# LONG REASON ARENA

## üìä Benchmark Details

**Name**: LONG REASON ARENA

**Overview**: A benchmark specifically designed to assess the long reasoning capabilities of Large Language Models (LLMs) through algorithmic execution.

**Data Type**: algorithmic problems

**Domains**:
- Natural Language Processing

**Languages**:
- English

**Similar Benchmarks**:
- LongBench
- GSM-Infinite
- LongGenBench
- LR2Bench

**Resources**:
- [GitHub Repository](https://github.com/LongReasonArena/LongReasonArena)

## üéØ Purpose and Intended Users

**Goal**: To evaluate the long reasoning capabilities of LLMs through algorithm execution, reflecting retrieval and backtracking tasks.

**Target Audience**:
- ML Researchers
- Developers of LLMs
- AI practitioners

**Tasks**:
- Algorithm Execution
- Long Reasoning

**Limitations**: The evaluation primarily focuses on deductive reasoning and may not encompass all reasoning forms.

## üíæ Data

**Source**: Collected algorithmic problems from LeetCode.

**Size**: 1,000 samples

**Format**: N/A

**Annotation**: Problems are curated and categorized based on expected reasoning steps.

## üî¨ Methodology

**Methods**:
- Automated Evaluation Metrics

**Metrics**:
- Accuracy

**Calculation**: Model performance is measured based on the correctness of outputs against expected algorithmic results.

**Interpretation**: Higher accuracy indicates better long reasoning capabilities of the model.

**Baseline Results**: Deepseek-R1 achieves only 7.5% accuracy on Level 3 tasks.

**Validation**: Validation conducted through algorithmic execution and performance analysis of multiple models.

## ‚ö†Ô∏è Targeted Risks

**Risk Categories**:
- Fairness
- Accuracy
- Robustness

**Atlas Risks**:
No specific atlas risks defined

**Demographic Analysis**: N/A

**Potential Harm**: N/A

## üîí Ethical and Legal Considerations

**Privacy And Anonymity**: Not Applicable

**Data Licensing**: Not Applicable

**Consent Procedures**: Not Applicable

**Compliance With Regulations**: Not Applicable
