# MTEB-NL (Massive Text Embedding Benchmark for Dutch)

## üìä Benchmark Details

**Name**: MTEB-NL (Massive Text Embedding Benchmark for Dutch)

**Overview**: MTEB-NL is a comprehensive benchmark combining existing and newly created datasets for evaluating Dutch embeddings across various tasks, comprising a total of 40 datasets.

**Data Type**: text

**Domains**:
- Natural Language Processing

**Languages**:
- Dutch

**Similar Benchmarks**:
- MTEB (Massive Text Embedding Benchmark)
- DUMB (A Benchmark for Smart Evaluation of Dutch Models)

**Resources**:
- [Resource](https://huggingface.co/collections/clips/mteb-nl-6888d7136112c731605f93ed)
- [Resource](https://huggingface.co/collections/clips/e5-nl-68be9d3760240ce5c7d9f831)
- [GitHub Repository](https://github.com/nikolay-banar/mteb-nl-dev)
- [GitHub Repository](https://github.com/ELotfi/e5-nl)

## üéØ Purpose and Intended Users

**Goal**: To evaluate and improve Dutch embedding models by providing a diverse set of tasks and datasets.

**Target Audience**:
- ML Researchers
- Model Developers
- Industry Practitioners

**Tasks**:
- Text Classification
- Named Entity Recognition
- Question Answering
- Retrieval
- Clustering
- Semantic Textual Similarity

**Limitations**: The current benchmark relies partly on machine- and human-translated datasets, limiting its ability to fully capture linguistic nuances and cultural context.

## üíæ Data

**Source**: Compiles various Dutch retrieval datasets and newly created datasets augmented with synthetic data.

**Size**: 40 datasets

**Format**: N/A

**Annotation**: Combination of human-annotated and synthetic data generated by large language models.

## üî¨ Methodology

**Methods**:
- Zero-shot evaluation
- Training evaluation of embedding models

**Metrics**:
- F1 Score
- Accuracy
- MAP (Mean Average Precision)

**Calculation**: Metrics are calculated based on model performance across various tasks defined in the benchmark.

**Interpretation**: Higher scores indicate better performance of models in retrieving relevant information or producing correct classifications.

**Validation**: The benchmark is validated using diverse datasets and continuous evaluation against existing models.

## ‚ö†Ô∏è Targeted Risks

**Risk Categories**:
- Bias
- Quality
- Cultural Relevance

**Atlas Risks**:
No specific atlas risks defined

**Demographic Analysis**: N/A

## üîí Ethical and Legal Considerations

**Privacy And Anonymity**: Not Applicable

**Data Licensing**: Data is available under various licenses including CC BY, CC BY-SA, and others.

**Consent Procedures**: Not Applicable

**Compliance With Regulations**: Not Applicable
