# Causal Document-Grounded Dialogue (CausalDD)

## ðŸ“Š Benchmark Details

**Name**: Causal Document-Grounded Dialogue (CausalDD)

**Overview**: We present the first causally-complete dataset construction strategy for developing million-scale DocGD pre-training corpora. Additionally, we propose a causally-perturbed pre-training strategy to better capture causality by introducing perturbations on the variables and optimizing the overall causal effect.

**Data Type**: text (document-grounded dialogue pairs: dialogue context, supporting document, evidence, response)

**Domains**:
- Natural Language Processing

**Languages**:
- English
- Chinese

**Similar Benchmarks**:
- Doc2dial
- MultiDoc2dial
- Doc2bot
- CoQA
- QuAC
- DoQA
- Wizard

**Resources**:
- [GitHub Repository](https://github.com/Vamsi995/Paraphrase-Generator)
- [Resource](https://creativecommons.org/licenses/by-sa/3.0)
- [Resource](https://edition.cnn.com/2023/01/07/health/dog-and-cat-new-year-resolutions-wellness/index.html)

## ðŸŽ¯ Purpose and Intended Users

**Goal**: Create causally-complete pre-training datasets for document-grounded dialogue and design a causally-perturbed pre-training strategy to model causal relationships among document, evidence, dialogue context, and response, improving DocGD performance in fully-supervised, few-shot, low-resource, and zero-shot settings.

**Tasks**:
- Document-Grounded Dialogue
- Knowledge Grounding
- Response Generation

**Limitations**: Pre-training data are generated by models (dialogue inpainter and paraphrase model), so data quality is slightly inferior to manually annotated data; the pre-training data construction strategy may not be applicable to other tasks such as knowledge graph-grounded dialogue; effectiveness of task-specific pre-training decreases as the amount of labeled data increases.

## ðŸ’¾ Data

**Source**: Two causally-complete pre-training corpora constructed by the authors: (1) WikiDialog: transform Wikipedia documents into dialogues by generating pseudo-user utterances with a dialogue inpainter and paraphrasing evidence for agent responses; (2) Reddit: crawl Reddit submissions containing URLs, use linked web pages as documents and the conversations/replies as dialogues, inserting virtual evidence into documents.

**Size**: WikiDialog: 1.00M dialogues, 0.12M documents, 3.00M turns; Reddit: 1.00M dialogues, 1.00M documents, 1.39M turns; All: 2.00M dialogues, 1.12M documents, 4.39M turns

**Format**: N/A

**Annotation**: Automatically generated: pseudo-user utterances via a dialogue inpainter; paraphrases via a paraphrase model; Chinese pre-training data produced via translation model for English-to-Chinese conversion. Human annotation used only for downstream human evaluation.

## ðŸ”¬ Methodology

**Methods**:
- DocGD-specific pre-training (optimize LDocGD to sequentially generate evidence and response conditioned on dialogue context and document)
- Causally-perturbed pre-training (introduce perturbations to document and evidence variables and optimize NDE and TIE losses)
- Fine-tuning on downstream datasets (Doc2dial, MultiDoc2dial, Doc2bot)
- Human evaluation (pairwise comparisons on Relevance and Informativeness)

**Metrics**:
- Exact Match (EM)
- F1 Score (token-level F1)
- BLEU Score
- Distinct (Dist-n)
- Human evaluation: Relevance
- Human evaluation: Informativeness
- Statistical significance (p-value under t-test)

**Calculation**: Knowledge identification: Exact Match (EM) and token-level F1. Response generation: BLEU (Papineni et al., 2002) and Dist-n for diversity. NDE is measured via Kullback-Leibler divergence between p_theta(e;r|d;c) and p_theta(e;r|d';c) where d' perturbs non-evidence sentences (Eq.5). TIE is optimized via an unlikelihood loss LTIE = -sum log(1 - p_theta(e;r|b_d;c)) where b_d is the document with evidence removed (Eq.6). Total pretraining objective is L = LDocGD + LNDE + LTIE (Eq.7).

**Interpretation**: Higher EM and token-level F1 indicate better evidence identification; higher BLEU indicates better response generation quality; higher Dist-n indicates more diverse generation. Lower NDE indicates robustness to perturbations in irrelevant document sentences; higher TIE indicates increased reliance on grounding evidence. Improvements are reported as statistically significant with p-value < 0.05 under t-test where noted.

**Baseline Results**: Representative baselines and results reported in the paper: Doc2dial knowledge identification (EM / F1) - UniGDD: 65.6 / 76.4; CausalDD: 66.0 / 77.3; CausalDD large: 67.0 / 78.1 (Table 2). Doc2dial response generation (BLEU) - UniGDD/others: UniGDD baselines in Table 3; CausalDD: 43.0; CausalDD large: 42.5 (Table 3). MultiDoc2dial (F1 / EM / BLEU) - UniGDD: 61.5 / 45.8 / 31.8; CausalDD: 63.7 / 49.3 / 33.9; CausalDD large: 64.5 / 51.0 / 33.6 (Table 8). The paper reports consistent improvements over UniGDD and other baselines across settings.

**Validation**: Validated on three downstream DocGD benchmark datasets (Doc2dial, MultiDoc2dial, Doc2bot) across fully-supervised, few-shot (5/50/100 examples), low-resource (1%/5%/10% of training data), and zero-shot settings; additional validation via human evaluation (100 instances, 5 annotators, pairwise comparisons on Relevance and Informativeness) and ablation studies (effects of WikiDialog vs Reddit vs NDE vs TIE). Statistical significance testing reported (p-value < 0.05 under t-test).

## âš ï¸ Targeted Risks

**Risk Categories**:
- Privacy
- Intellectual Property

**Atlas Risks**:
- **Privacy**: Personal information in data
- **Intellectual Property**: Data usage rights restrictions, Copyright infringement

## ðŸ”’ Ethical and Legal Considerations

**Privacy And Anonymity**: The authors state they have taken measures to ensure that the Wikipedia texts do not contain private information and that the Reddit conversation data does not include any personal information; topics are described as public and harmless.

**Data Licensing**: Wikipedia corpus shared under the CC BY-SA 3.0 license (https://creativecommons.org/licenses/by-sa/3.0). The Reddit dump is referenced as shared for research purposes (Baumgartner et al., 2020).

**Consent Procedures**: Not Applicable

**Compliance With Regulations**: Not Applicable
