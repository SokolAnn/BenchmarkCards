# ANALO BENCH

## üìä Benchmark Details

**Name**: ANALO BENCH

**Overview**: ANALO BENCH is a benchmark to determine analogical reasoning ability in language models. It focuses on analogical reasoning over natural language stories that convey abstract concepts with varying levels of difficulty, emphasizing (i) recalling related experiences from a large pool of information and (ii) applying analogical reasoning to complex and lengthy scenarios. The benchmark contains human-written analogies and evaluates models on two tasks: selecting the most analogous story from a small story bank (T1) and retrieving the top-10 analogous stories from a large story bank (T2).

**Data Type**: text (human-written analogical sentence pairs and elaborated stories)

**Domains**:
- Natural Language Processing

**Languages**:
- English

**Similar Benchmarks**:
- STORY ANALOGY
- ParallelPARC
- ePiC
- GENTNER
- ARN

**Resources**:
- [GitHub Repository](https://github.com/JHU-CLSP/AnaloBench)
- [Resource](https://arxiv.org/abs/2402.12370)
- [Resource](https://www.arch.jhu.edu)

## üéØ Purpose and Intended Users

**Goal**: To benchmark analogical reasoning ability in language models by measuring their ability to identify abstract analogies in natural language stories across varying story lengths and candidate pool sizes.

**Target Audience**:
- ML Researchers
- Model Developers

**Tasks**:
- Multiple-choice Question Answering (Analogy Selection from a mini story bank, T1)
- Information Retrieval (Top-K Retrieval of analogous stories from a large story bank, T2)

**Limitations**: The dataset prioritizes quality over quantity and is relatively small. The authors note they cannot exclude the possibility that language models encountered labeled analogies during training or fine-tuning (potential data contamination). The benchmark does not evaluate parametric knowledge stored in model weights. Resource constraints limited the number of models evaluated and the extent of prompt engineering.

**Out of Scope Uses**:
- Evaluating analogical reasoning that relies on models' parametric knowledge (the benchmark controls accessible stories and does not evaluate retrieval from model parameters).

## üíæ Data

**Source**: Collected 340 analogies from 4 human annotators (the authors) after multiple rounds of editing. Seed sentences were sampled from Cambridge Dictionary examples of idioms and a metaphors dataset (Bizzoni and Lappin, 2018). Longer story elaborations (10- and 30-sentence versions) were generated using GPT-4; some evaluations also used Claude generations for comparison.

**Size**: 340 stories (340 analogies) grouped into 47 clusters; average cluster size 7.2 stories

**Format**: N/A

**Annotation**: Manual annotation by 4 human annotators (authors) with multiple rounds of editing and reviewer scrutiny to ensure clarity and analogical quality. Additional human evaluation for T1 was conducted by 3 separate annotators with adjudication.

## üî¨ Methodology

**Methods**:
- Human evaluation
- Automated metrics (classification and retrieval metrics)
- Model-based evaluation (evaluating multiple open-source and proprietary LMs)

**Metrics**:
- Accuracy
- Mean Average Precision (MAP)
- Precision@K
- Recall@K
- Mean Reciprocal Rank (MRR)

**Calculation**: All scores are reported as percentages. For T1 (mini story bank), Accuracy is used: a solver gets 1 if it chooses the most analogous story and 1/k if it reports no-answer or a k-way tie that includes the correct answer (k=4). For T2 (large story bank), retrieval metrics reported include MAP, Precision@K, Recall@K, and MRR.

**Interpretation**: Higher metric values indicate better analogy identification. Human performance is treated as an upper reference; model performance degrades with increasing story length. Scaling model size yields improvements on 1-sentence stories but provides minimal gains on longer stories. Performance near random baselines on T2 indicates limited retrieval ability for long-context analogies.

**Baseline Results**: Selected results reported: Random baseline (T1) = 25% accuracy. T1 results: GPT-4 = 89.1% (1-sentence), 66.5% (10-sentence), 60.7% (30-sentence); Human = 96.0% (1-sentence), 72.5% (10-sentence), 73.3% (30-sentence). T2 (GPT4-turbo) example metrics: P@3 = 42.9% (1-sentence), 6.5% (10-sentence), 3.9% (30-sentence); MAP = 55.4% (1-sentence), 14.2% (10-sentence), 10.8% (30-sentence).

**Validation**: Human evaluation with three annotators and adjudication was used to validate T1 examples (50 instances for 1- and 10-sentence; 30 instances for 30-sentence). Inter-annotator agreement is reported. Story elaborations were validated using entailment checks (Claude-3) showing high entailment rates between source sentences and expanded stories. Additional checks include evaluating GPT-4 on stories generated by Claude to measure self-evaluation bias.

## ‚ö†Ô∏è Targeted Risks

**Risk Categories**:
- Accuracy

**Atlas Risks**:
- **Accuracy**: Data contamination

**Potential Harm**: The authors note that analogical reasoning is inherently subjective and, if applied without care, may potentially lead to misleading or incorrect conclusions.

## üîí Ethical and Legal Considerations

**Privacy And Anonymity**: The authors state: 'The work presented here does not immediately raise any ethical concerns, to our knowledge.' Privacy and anonymization procedures are not discussed.

**Data Licensing**: Not Applicable

**Consent Procedures**: Not Applicable

**Compliance With Regulations**: Not Applicable
