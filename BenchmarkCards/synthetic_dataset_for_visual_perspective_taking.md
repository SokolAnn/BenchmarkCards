# Synthetic Dataset for Visual Perspective Taking

## 📊 Benchmark Details

**Name**: Synthetic Dataset for Visual Perspective Taking

**Overview**: This paper presents a synthetic dataset generated in NVIDIA Omniverse that supports supervised learning for spatial reasoning tasks in Visual Perspective Taking (VPT). Each instance includes an RGB image, a natural language description, and a ground-truth 4×4 transformation matrix representing object pose.

**Data Type**: RGB images and natural language descriptions with transformation matrices

**Domains**:
- Natural Language Processing
- Robotics

**Languages**:
- English

**Resources**:
- [Resource](https://huggingface.co/datasets/jwgcurrie/synthetic-distance)

## 🎯 Purpose and Intended Users

**Goal**: To provide a synthetic dataset for training Vision-Language Models to perform Visual Perspective Taking, which is crucial for Human-Robot Interaction.

**Target Audience**:
- Artificial Intelligence Researchers
- Robotics Developers
- Machine Learning Practitioners

**Tasks**:
- Spatial Reasoning
- Visual Perspective Taking

**Limitations**: N/A

## 💾 Data

**Source**: Synthetic scenes generated in NVIDIA Omniverse.

**Size**: N/A

**Format**: N/A

**Annotation**: Ground-truth transformation matrices generated along with RGB images and natural language prompts.

## 🔬 Methodology

**Methods**:
- Supervised learning

**Metrics**:
- N/A

**Calculation**: Performance will be evaluated based on the accuracy of spatial reasoning tasks.

**Interpretation**: N/A

**Validation**: N/A

## ⚠️ Targeted Risks

**Risk Categories**:
- Accuracy
- Fairness
- Privacy
- Safety

**Atlas Risks**:
No specific atlas risks defined

## 🔒 Ethical and Legal Considerations

**Privacy And Anonymity**: Not Applicable

**Data Licensing**: Not Applicable

**Consent Procedures**: Not Applicable

**Compliance With Regulations**: Not Applicable
