# DiLM (Distilling Dataset into Language Model)

## üìä Benchmark Details

**Name**: DiLM (Distilling Dataset into Language Model)

**Overview**: DiLM is the first text-level dataset distillation approach that trains a language model to generate informative synthetic samples as text data, enabling the training of models independent of word embedding weights and improving interpretability.

**Data Type**: text

**Domains**:
- Natural Language Processing

**Languages**:
- English

**Resources**:
- [GitHub Repository](https://github.com/arumaekawa/DiLM)

## üéØ Purpose and Intended Users

**Goal**: To distill a training dataset into a smaller number of informative synthetic samples that enable effective training of neural networks.

**Target Audience**:
- ML Researchers
- Model Developers

**Tasks**:
- Text Classification

**Limitations**: While successful as a text-level distillation method, there is still a performance gap from full datasets.

## üíæ Data

**Source**: GLUE benchmark datasets (SST-2, QQP, MNLI-m)

**Size**: Approximately 67,000 examples for SST-2, 364,000 examples for QQP, 393,000 examples for MNLI-m

**Format**: JSON

**Annotation**: Synthetic samples generated by training a language model.

## üî¨ Methodology

**Methods**:
- Gradient matching
- K-centers sampling
- Diverse mini-batch sampling

**Metrics**:
- Accuracy
- F1 Score

**Calculation**: Gradient matching loss calculated using cosine similarity between real and generated sample gradients.

**Interpretation**: Higher accuracy and F1 score indicate better model performance when trained on distilled datasets.

**Baseline Results**: Outperformed Random and K-centers methods on various benchmarks.

**Validation**: Performance validated across different models and settings using 100 runs for reliability.

## ‚ö†Ô∏è Targeted Risks

**Risk Categories**:
- Accuracy
- Fairness

**Atlas Risks**:
No specific atlas risks defined

**Demographic Analysis**: Further demographic analysis needed for fairness assessment.

## üîí Ethical and Legal Considerations

**Privacy And Anonymity**: Not Applicable

**Data Licensing**: Not Applicable

**Consent Procedures**: Not Applicable

**Compliance With Regulations**: Not Applicable
