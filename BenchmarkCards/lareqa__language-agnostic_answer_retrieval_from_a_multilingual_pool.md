# LAReQA: Language-agnostic answer retrieval from a multilingual pool

## üìä Benchmark Details

**Name**: LAReQA: Language-agnostic answer retrieval from a multilingual pool

**Overview**: LAReQA is a challenging new benchmark for language-agnostic answer retrieval from a multilingual candidate pool. Unlike previous cross-lingual tasks, LAReQA tests for "strong" cross-lingual alignment, requiring semantically related cross-language pairs to be closer in representation space than unrelated same-language pairs.

**Data Type**: question-answering pairs (text)

**Domains**:
- Natural Language Processing
- Information Retrieval

**Languages**:
- Arabic
- German
- Greek
- English
- Spanish
- Hindi
- Russian
- Thai
- Turkish
- Vietnamese
- Chinese

**Similar Benchmarks**:
- XQuAD
- MLQA
- XNLI
- XTREME
- BUCC
- Tatoeba
- BLI
- ReQA
- SQuAD
- USE-QA

**Resources**:
- [Resource](https://arxiv.org/abs/2004.05484)
- [Resource](https://en.wikipedia.org/wiki/Wikipedia:Size_of_Wikipedia)
- [Resource](https://pypi.org/project/thai-segmenter)
- [Resource](https://tfhub.dev/google/universal-sentence-encoder-multilingual-qa)

## üéØ Purpose and Intended Users

**Goal**: To evaluate language-agnostic answer retrieval from a multilingual candidate pool and to test for "strong" cross-lingual alignment and language bias in representations.

**Target Audience**:
- ML Researchers
- Model Developers
- Industry Practitioners

**Tasks**:
- Question Answering
- Information Retrieval
- Cross-lingual Retrieval

**Limitations**: Thai sentence breaking is noted as an outlier (around 70% the sentences per paragraph of other languages) due to lack of explicit sentence boundary markers, which affects candidate counts. The authors also note that Translate-Test baseline relies on an external machine translation system and thus sidesteps the alignment problem.

## üíæ Data

**Source**: Constructed by converting existing cross-lingual extractive QA tasks XQuAD and MLQA into retrieval tasks (XQuAD-R and MLQA-R) by breaking contextual paragraphs into sentences and including all sentences across the datasets as candidate answers; a sentence is correct if it contains the target answer span for that question or an equivalent question in another language (as identified by qasid).

**Size**: XQuAD-R: 11 languages with 1,190 questions per language (13,090 total questions); candidates per language as reported in Table 1 range from 852 to 1,276. MLQA-R: per-language question and candidate counts as reported in Table 1 (e.g., English: 1,148 questions, 6,264 candidates; see Table 1 in paper).

**Format**: N/A

**Annotation**: Annotated with sentence boundaries as generated by an internal sentence breaker; sentences labeled as correct answers if they contain the target answer span (mapping across languages via qasid).

## üî¨ Methodology

**Methods**:
- Automated metrics (mean average precision)
- Model-based evaluation using dual-encoder embedding retrieval baselines (mBERT variants)
- Zero-shot monolingual retrieval evaluation and ablation analyses (remove-one-target, single-answer retrieval)

**Metrics**:
- Mean Average Precision (mAP)
- Precision@k
- Mean Reciprocal Rank (MRR)

**Calculation**: mAP is defined as mAP = (1/|Q|) * sum_{qi in Q} (1/Ri) * sum_{j=1}^K P@j(qi) * rel(i,j), where Ri is the number of correct answers for question qi, P@j(qi) is Precision@j for qi, and rel(i,j) is 1 if the j-th ranked candidate for qi is correct, 0 otherwise. mAP ranges between 0 and 1.

**Interpretation**: mAP between 0 and 1; a perfect 1.0 occurs when a model ranks all correct answers in the top C positions. High mAP requires both strong QA retrieval quality and absence of language bias. Single-answer retrieval mAP is equivalent to mean reciprocal rank (MRR) in that setting.

**Baseline Results**: Main baseline mAPs on XQuAD-R and MLQA-R (Table 2): En-En: XQuAD-R 0.29, MLQA-R 0.36; X-X: XQuAD-R 0.23, MLQA-R 0.26; X-X-mono: XQuAD-R 0.52, MLQA-R 0.49; X-Y: XQuAD-R 0.66, MLQA-R 0.49; Translate-Test: XQuAD-R 0.72, MLQA-R 0.58. USE-QA on restricted set (Table 5): 0.51 on XQuAD-RUSE. Zero-shot monolingual retrieval results reported in Table 4.

**Validation**: Evaluations conducted on XQuAD-R (primary) and MLQA-R; MLQA dev set used rather than test set for evaluation speed. Multiple analyses (remove-one-target, single-answer retrieval, zero-shot monolingual pool) used to validate language bias characteristics. Training details (e.g., 32 TPU-v3 cores, batch sizes, 100,000 steps) reported and no overfitting observed.

## ‚ö†Ô∏è Targeted Risks

**Risk Categories**:
- Bias
- Accuracy

**Atlas Risks**:
- **Fairness**: Output bias
- **Accuracy**: Poor model accuracy

**Demographic Analysis**: N/A

**Potential Harm**: The paper states that language bias (e.g., same-language bias) is harmful because if the model prefers answers in a given language, it is prone to retrieve irrelevant results in that language over relevant results from another language.

## üîí Ethical and Legal Considerations

**Privacy And Anonymity**: Not Applicable

**Data Licensing**: Not Applicable

**Consent Procedures**: Not Applicable

**Compliance With Regulations**: Not Applicable
