# INCLUDE (Evaluating Multilingual Language Understanding with Regional Knowledge)

## üìä Benchmark Details

**Name**: INCLUDE (Evaluating Multilingual Language Understanding with Regional Knowledge)

**Overview**: INCLUDE is a comprehensive knowledge- and reasoning-centric benchmark across 44 written languages that evaluates multilingual LLMs for performance in the actual language environments where they would be deployed. It comprises 197,243 QA pairs collected from local exams and captures regional and cultural knowledge.

**Data Type**: question-answering pairs

**Domains**:
- Natural Language Processing
- Education

**Languages**:
- Albanian
- Arabic
- Armenian
- Azerbaijani
- Basque
- Belarusian
- Bengali
- Bulgarian
- Chinese
- Croatian
- Czech
- Danish
- Dutch
- Estonian
- Finnish
- French
- Georgian
- German
- Greek
- Hebrew
- Hindi
- Hungarian
- Indonesian
- Italian
- Japanese
- Kazakh
- Korean
- Lithuanian
- Malay
- Malayalam
- Nepali
- Macedonian
- Persian
- Polish
- Portuguese
- Russian
- Serbian
- Spanish
- Tagalog
- Tamil
- Telugu
- Turkish
- Ukrainian
- Urdu
- Uzbek
- Vietnamese

**Similar Benchmarks**:
- MMLU
- ArabicMMLU
- ChineseMMLU
- TurkishMMLU
- VNHSGE
- EXAMS

**Resources**:
- [Resource](https://huggingface.co/datasets/CohereForAI/include-base-44)

## üéØ Purpose and Intended Users

**Goal**: To assess the performance of large language models across a wide range of subjects and languages, focusing on cultural and regional knowledge.

**Target Audience**:
- ML Researchers
- AI Developers
- Educational Institutions

**Tasks**:
- Question Answering

**Limitations**: N/A

## üíæ Data

**Source**: Collected from local exams and assessments across various educational contexts and regions.

**Size**: 197,243 question-answering pairs

**Format**: N/A

**Annotation**: Manually verified by native speakers and annotators.

## üî¨ Methodology

**Methods**:
- Evaluation using state-of-the-art LLMs
- Cross-linguistic performance comparisons

**Metrics**:
- Accuracy

**Calculation**: Metrics are calculated based on the overall correct responses to the question-answering pairs.

**Interpretation**: High scores indicate better performance in understanding regional knowledge across multiple languages.

**Baseline Results**: Evaluated against the performance of notable LLMs such as GPT-4o and others.

**Validation**: N/A

## ‚ö†Ô∏è Targeted Risks

**Risk Categories**:
- Fairness
- Accuracy

**Atlas Risks**:
- **Fairness**: Data bias
- **Accuracy**: Poor model accuracy

**Demographic Analysis**: N/A

## üîí Ethical and Legal Considerations

**Privacy And Anonymity**: Not Applicable

**Data Licensing**: Not Applicable

**Consent Procedures**: Not Applicable

**Compliance With Regulations**: Not Applicable
