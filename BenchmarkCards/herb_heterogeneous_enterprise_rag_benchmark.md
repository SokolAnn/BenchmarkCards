# HERB (Heterogeneous Enterprise RAG Benchmark)

## üìä Benchmark Details

**Name**: HERB (Heterogeneous Enterprise RAG Benchmark)

**Overview**: HERB is a benchmark for evaluating Deep Search, focusing on complex retrieval-augmented generation tasks over diverse enterprise data, enabling fine-grained evaluation of long-context LLM and RAG systems.

**Data Type**: query-answer pairs, document retrieval

**Domains**:
- Natural Language Processing

**Languages**:
- English

**Similar Benchmarks**:
- HotpotQA
- MuSiQue
- MultiHopRAG

**Resources**:
- [GitHub Repository](https://github.com/SalesforceAIResearch/HERB)
- [Resource](https://huggingface.co/datasets/Salesforce/HERB)

## üéØ Purpose and Intended Users

**Goal**: To provide a comprehensive evaluation framework for RAG systems within realistic enterprise environments, highlighting the need for enhanced retrieval and reasoning capabilities.

**Target Audience**:
- ML Researchers
- Industry Practitioners
- Model Developers

**Tasks**:
- Question Answering
- Document Retrieval

**Limitations**: Constructing HERB requires significant human effort, making it less scalable compared to fully synthetic benchmarks.

## üíæ Data

**Source**: Synthetic data generated by simulating enterprise workflows across product planning, development, and support stages.

**Size**: 39,190 data artifacts, including 815 answerable and 699 unanswerable queries.

**Format**: JSON

**Annotation**: Manually curated by domain experts to ensure clarity and relevance to enterprise search tasks.

## üî¨ Methodology

**Methods**:
- Automated metrics
- Human evaluation

**Metrics**:
- Accuracy
- F1 Score

**Calculation**: Standard RAG evaluation practices using a combination of Likert scale for content-based queries and F1 scores for others.

**Interpretation**: Higher scores indicate better performance in answering queries accurately and retrieving relevant documents.

**Baseline Results**: Best-performing model achieves an average score of 32.96 on the HERB benchmark.

**Validation**: Cross-validation against existing benchmarks and human evaluations to assess effectiveness.

## ‚ö†Ô∏è Targeted Risks

**Risk Categories**:
- Accuracy
- Fairness
- Privacy
- Robustness

**Atlas Risks**:
- **Accuracy**: Unrepresentative data, Poor model accuracy
- **Fairness**: Data bias
- **Privacy**: Personal information in data
- **Robustness**: Data poisoning

**Demographic Analysis**: N/A

## üîí Ethical and Legal Considerations

**Privacy And Anonymity**: Not Applicable

**Data Licensing**: Not Applicable

**Consent Procedures**: Not Applicable

**Compliance With Regulations**: Not Applicable
