# CodeScope: An Execution-based Multilingual Multitask Multidimensional Benchmark for Evaluating LLMs on Code Understanding and Generation

## üìä Benchmark Details

**Name**: CodeScope: An Execution-based Multilingual Multitask Multidimensional Benchmark for Evaluating LLMs on Code Understanding and Generation

**Overview**: CodeScope is a comprehensive benchmark designed to evaluate the code understanding and generation capabilities of LLMs, addressing the limitations of existing benchmarks by considering a multilingual and multitask framework with execution-based evaluations.

**Data Type**: code samples

**Domains**:
- Computer Science

**Languages**:
- English

**Similar Benchmarks**:
- CodeXGLUE
- XLCoST

**Resources**:
- [GitHub Repository](https://github.com/WeixiangYAN/CodeScope)

## üéØ Purpose and Intended Users

**Goal**: To evaluate the coding capabilities of large language models (LLMs) through a comprehensive benchmark that incorporates multilingual and multitask settings along with execution-based evaluation.

**Target Audience**:
- ML Researchers
- Software Engineers
- Benchmark Developers

**Tasks**:
- Code Summarization
- Code Smell Detection
- Code Review
- Automated Testing
- Program Synthesis
- Code Translation
- Code Repair
- Code Optimization

**Limitations**: N/A

## üíæ Data

**Source**: The dataset is constructed from multiple independent sources, integrating distinct datasets for various programming languages and tasks.

**Size**: 43 programming languages; 8 coding tasks

**Format**: JSON

**Annotation**: Each coding task is annotated based on specific criteria relevant to the task, often employing manual reviews and validations.

## üî¨ Methodology

**Methods**:
- Automated metrics
- Execution-based evaluation
- Qualitative analysis

**Metrics**:
- Accuracy
- F1 Score
- BLEU Score
- ROUGE-L
- BERTScore

**Calculation**: Metrics are calculated based on execution results, comparing the output of the code generated by LLMs to expected outputs for various tasks.

**Interpretation**: Higher scores indicate better performance in understanding and generating code accurately as per the evaluations.

**Baseline Results**: Eight mainstream LLMs were evaluated against the benchmark to establish performance baselines.

**Validation**: Each task is subjected to a series of validation procedures to ensure reliability and accuracy of the results.

## ‚ö†Ô∏è Targeted Risks

**Risk Categories**:
- Bias
- Safety
- Robustness
- Fairness

**Atlas Risks**:
- **Fairness**: Data bias
- **Robustness**: Evasion attack
- **Accuracy**: Unrepresentative data

**Demographic Analysis**: N/A

**Potential Harm**: N/A

## üîí Ethical and Legal Considerations

**Privacy And Anonymity**: Not Applicable

**Data Licensing**: Not Applicable

**Consent Procedures**: Not Applicable

**Compliance With Regulations**: Not Applicable
