# Cross-Task Defense: Instruction-Tuning LLMs for Content Safety

## 📊 Benchmark Details

**Name**: Cross-Task Defense: Instruction-Tuning LLMs for Content Safety

**Overview**: The paper introduces a defense dataset designed for LLMs that focuses on safety-related examples for instruction tuning, aiming to minimize the risk of processing harmful content while retaining utility for benign tasks.

**Data Type**: text

**Domains**:
- Natural Language Processing

**Languages**:
- English

**Resources**:
- [GitHub Repository](https://github.com/FYYFU/safety-defense)

## 🎯 Purpose and Intended Users

**Goal**: To develop robust defenses for LLMs in processing malicious documents while maintaining their performance in benign NLP tasks.

**Target Audience**:
- ML Researchers
- Safety Engineers
- NLP Practitioners

**Tasks**:
- Text Classification
- Question Answering
- Summarization
- Translation
- Sentiment Analysis

**Limitations**: N/A

## 💾 Data

**Source**: Safety-related examples compiled from malicious documents generated by attacks and labeled by human annotators.

**Size**: 2,000 examples

**Format**: Raw text files

**Annotation**: Labeled by human annotators and generated using LLaMA-2-7B

## 🔬 Methodology

**Methods**:
- Instruction tuning
- Mixed-task training

**Metrics**:
- Task pass rate

**Calculation**: Calculated as the processing rate of LLMs on benign and malicious documents to evaluate defense performance.

**Interpretation**: A lower task processing rate on malicious documents indicates better defense capabilities.

**Validation**: Utilized various NLP tasks for evaluation with an emphasis on generalization performance.

## ⚠️ Targeted Risks

**Risk Categories**:
- Safety
- Accuracy

**Atlas Risks**:
- **Accuracy**: Poor model accuracy

**Demographic Analysis**: N/A

## 🔒 Ethical and Legal Considerations

**Privacy And Anonymity**: Not Applicable

**Data Licensing**: Not Applicable

**Consent Procedures**: Not Applicable

**Compliance With Regulations**: Not Applicable
