# VStyle (Voice Style Adaptation)

## 📊 Benchmark Details

**Name**: VStyle (Voice Style Adaptation)

**Overview**: VStyle is a bilingual benchmark for evaluating voice style adaptation (VSA) in spoken language models. It covers four categories of speech generation: acoustic attributes, natural language instructions, role-play, and implicit empathy, comprising 1,523 prompts designed around realistic interaction needs.

**Data Type**: spoken instructions

**Domains**:
- Natural Language Processing

**Languages**:
- Chinese
- English

**Resources**:
- [Resource](https://project_homepage_link)

## 🎯 Purpose and Intended Users

**Goal**: To provide a foundation for advancing human-centered spoken interaction.

**Target Audience**:
- ML Researchers
- Industrating Practitioners
- Speech Interaction Developers

**Tasks**:
- Voice Style Adaptation

**Limitations**: The instruction dataset reflects annotator preferences and model-driven patterns, which may diverge from real user demands.

## 💾 Data

**Source**: Constructed using a hybrid human–LLM approach with manual design and LLM-based expansion.

**Size**: 1,523 instructions

**Format**: N/A

**Annotation**: Hybrid human–LLM annotation with manual design and LLM-based expansion.

## 🔬 Methodology

**Methods**:
- Automated metrics
- Model-based evaluation

**Metrics**:
- Mean Opinion Score (MOS)

**Calculation**: Scores are obtained by averaging scores across categories with equal weights.

**Interpretation**: Higher MOS indicates better performance in voice style adaptation.

**Validation**: Evaluation involved assessing model outputs against human evaluations for consistency.

## ⚠️ Targeted Risks

**Risk Categories**:
- Fairness
- Accuracy
- Safety

**Atlas Risks**:
No specific atlas risks defined

**Demographic Analysis**: N/A

**Potential Harm**: N/A

## 🔒 Ethical and Legal Considerations

**Privacy And Anonymity**: Not Applicable

**Data Licensing**: Not Applicable

**Consent Procedures**: Not Applicable

**Compliance With Regulations**: Not Applicable
