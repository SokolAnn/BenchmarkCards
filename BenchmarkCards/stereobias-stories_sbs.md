# StereoBias-Stories (SBS)

## 📊 Benchmark Details

**Name**: StereoBias-Stories (SBS)

**Overview**: The StereoBias-Stories (SBS) dataset is designed to analyze gender bias in narratives generated by large language models (LLMs) using psychological stereotypes. It contains nearly 150,000 generated short stories that explore how gender representation shifts in response to different stereotypical attributes during story generation.

**Data Type**: narrative stories

**Domains**:
- Natural Language Processing

**Languages**:
- English

**Resources**:
- [Resource](https://arxiv.org/abs/2508.03292v1)

## 🎯 Purpose and Intended Users

**Goal**: To provide a detailed examination of how gender bias manifests within narratives generated by large language models and how it is influenced by psychological stereotypes.

**Target Audience**:
- ML Researchers
- Model Developers
- Fairness Researchers

**Tasks**:
- Story Generation
- Bias Analysis

**Limitations**: The analysis focuses on binary gender representations and may not encapsulate the complexity of gender identities. It also primarily uses English, limiting its generalizability across languages.

## 💾 Data

**Source**: Generated dataset from fine-tuning large language models across different prompting settings.

**Size**: 148,082 stories

**Format**: JSON

**Annotation**: Stories generated based on psychological stereotypes, evaluated using human judgments and automated metrics.

## 🔬 Methodology

**Methods**:
- Lexical evaluation
- User study
- Automated metrics

**Metrics**:
- Perplexity
- User ratings (1-5 scale)
- Attribute expression rating

**Calculation**: Metrics are calculated to evaluate the quality and bias of generated narratives.

**Interpretation**: A higher rating denotes better overall quality and stronger attribute expression in the narratives.

**Validation**: The dataset was evaluated using a user study with 58 participants and multiple automated metrics.

## ⚠️ Targeted Risks

**Risk Categories**:
- Bias
- Fairness

**Atlas Risks**:
- **Fairness**: Output bias
- **Accuracy**: Unrepresentative data

**Demographic Analysis**: No demographic analysis includes non-binary and diverse gender identities.

**Potential Harm**: ['Potential reinforcement of harmful gender stereotypes through generated narratives']

## 🔒 Ethical and Legal Considerations

**Privacy And Anonymity**: The content generation process included anonymization of participant data in the user study.

**Data Licensing**: Not Applicable

**Consent Procedures**: Participants in the user study were informed about the content nature and allowed to opt out.

**Compliance With Regulations**: Not Applicable
