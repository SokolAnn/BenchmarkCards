# Raccoon: Prompt Extraction Benchmark of LLM-Integrated Applications

## üìä Benchmark Details

**Name**: Raccoon: Prompt Extraction Benchmark of LLM-Integrated Applications

**Overview**: We introduce the Raccoon benchmark which comprehensively evaluates a model‚Äôs susceptibility to prompt extraction attacks. The benchmark encompasses 14 categories of prompt extraction attacks and assesses models under both defenseless and defended scenarios.

**Data Type**: prompt extraction attack data

**Domains**:
- Natural Language Processing

**Languages**:
- English

**Resources**:
- [GitHub Repository](https://github.com/M0gician/RaccoonBench)

## üéØ Purpose and Intended Users

**Goal**: To establish a systematic benchmark for assessing LLM robustness against prompt extraction attacks.

**Target Audience**:
- ML Researchers
- AI Practitioners

**Tasks**:
- Model Evaluation
- Security Assessment

**Limitations**: N/A

## üíæ Data

**Source**: Generated and sampled from various LLM-integrated applications.

**Size**: 197 distinct instruction prompts from GPT-integrated applications

**Format**: N/A

**Annotation**: Crowdsourced prompt attacks and defenses.

## üî¨ Methodology

**Methods**:
- Automated metrics
- Model-based evaluation

**Metrics**:
- Accuracy
- Attack Success Rate (ASR)

**Calculation**: ASR is calculated by the success rate of attacks over the instruction prompts based on Rouge-L scores.

**Interpretation**: A lower ASR indicates a model has better defenses against prompt extraction attacks.

**Validation**: The benchmark tested multiple models across various scenarios.

## ‚ö†Ô∏è Targeted Risks

**Risk Categories**:
- Robustness
- Privacy
- Security

**Atlas Risks**:
- **Robustness**: Prompt injection attack
- **Privacy**: Personal information in data

## üîí Ethical and Legal Considerations

**Privacy And Anonymity**: PII was removed from the data prior to publishing.

**Data Licensing**: Not Applicable

**Consent Procedures**: Not Applicable

**Compliance With Regulations**: Not Applicable
