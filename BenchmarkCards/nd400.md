# ND400

## 📊 Benchmark Details

**Name**: ND400

**Overview**: ND400 is a benchmark dataset created by sampling 200 image-text pairs from NoCaps and 200 image-text pairs from DOCCI, used to evaluate semantic drift in unified models by providing diverse and fine-grained visual details.

**Data Type**: image-text pairs

**Domains**:
- Natural Language Processing
- Computer Vision

**Languages**:
- English

**Similar Benchmarks**:
- NoCaps
- DOCCI

**Resources**:
- [GitHub Repository](https://github.com/mollahsabbir/Semantic-Drift-in-Unified-Models)

## 🎯 Purpose and Intended Users

**Goal**: To quantify how unified models preserve semantics over repeated modality shifts by evaluating semantic drift in generative tasks.

**Target Audience**:
- ML Researchers
- Model Developers

**Tasks**:
- Image Understanding
- Image Generation

**Limitations**: N/A

## 💾 Data

**Source**: Sampling from NoCaps and DOCCI datasets.

**Size**: 400 image-text pairs

**Format**: N/A

**Annotation**: N/A

## 🔬 Methodology

**Methods**:
- Cyclic evaluation protocol
- Quantitative assessment

**Metrics**:
- Mean Cumulative Drift (MCD)
- Semantic Drift Rate (SDR)
- Multi-Generation GenEval (MGG)

**Calculation**: Metrics are calculated by measuring semantic similarity at each generation step between the original input and subsequent outputs.

**Interpretation**: A higher Mean Cumulative Drift indicates better retention of semantic meaning across generations.

**Baseline Results**: N/A

**Validation**: Experiments conducted across multiple models to highlight variation in performance.

## ⚠️ Targeted Risks

**Risk Categories**:
- Accuracy
- Robustness

**Atlas Risks**:
- **Accuracy**: Unrepresentative data
- **Robustness**: Prompt injection attack

**Demographic Analysis**: N/A

## 🔒 Ethical and Legal Considerations

**Privacy And Anonymity**: Not Applicable

**Data Licensing**: Not Applicable

**Consent Procedures**: Not Applicable

**Compliance With Regulations**: Not Applicable
