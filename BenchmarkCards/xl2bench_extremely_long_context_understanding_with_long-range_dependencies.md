# XL2Bench (eXtremely Long context understanding with Long-range dependencies)

## ğŸ“Š Benchmark Details

**Name**: XL2Bench (eXtremely Long context understanding with Long-range dependencies)

**Overview**: XL2Bench is a benchmark for extremely long text understanding with long-range dependencies, including three scenariosâ€”Fiction Reading, Paper Reading, and Law Readingâ€”and four tasks: Memory Retrieval, Detailed Understanding, Overall Understanding, and Open-ended Generation, covering 27 subtasks in English and Chinese.

**Data Type**: text

**Domains**:
- Natural Language Processing

**Languages**:
- English
- Chinese

**Similar Benchmarks**:
- LongBench
- L-Eval
- InfiniteBench

**Resources**:
- [GitHub Repository](https://github.com/nuaa-nlp/XL2Bench)

## ğŸ¯ Purpose and Intended Users

**Goal**: The primary objective of XL2Bench is to evaluate large language models' ability to comprehend extremely long-text inputs with long-range dependencies.

**Target Audience**:
- ML Researchers
- Model Developers

**Tasks**:
- Memory Retrieval
- Detailed Understanding
- Overall Understanding
- Open-ended Generation

**Limitations**: N/A

## ğŸ’¾ Data

**Source**: Long texts from novels, academic papers, and legal legislations, processed with LLMs and human verification.

**Size**: Average length of 100K+ words (English) and 200K+ characters (Chinese) with over 632K questions.

**Format**: N/A

**Annotation**: Content Extraction, Data Integration, and Data Synthesis using LLMs with human verification.

## ğŸ”¬ Methodology

**Methods**:
- Human evaluation
- Automated metrics

**Metrics**:
- Accuracy
- F1 Score
- BLEU Score
- ROUGE-L

**Calculation**: Metrics are calculated based on the answers generated by LLMs compared to the ground truth.

**Interpretation**: Higher scores indicate better performance in memory retrieval and overall understanding tasks.

**Baseline Results**: Performance metrics of various state-of-the-art LLMs evaluated on XL2Bench.

**Validation**: The benchmark was validated through extensive empirical experiments across multiple leading LLMs.

## âš ï¸ Targeted Risks

**Risk Categories**:
- Bias
- Accuracy
- Robustness

**Atlas Risks**:
- **Accuracy**: Unrepresentative data, Poor model accuracy
- **Robustness**: Prompt injection attack
- **Fairness**: Data bias

**Demographic Analysis**: N/A

**Potential Harm**: N/A

## ğŸ”’ Ethical and Legal Considerations

**Privacy And Anonymity**: Not Applicable

**Data Licensing**: Not Applicable

**Consent Procedures**: Not Applicable

**Compliance With Regulations**: Not Applicable
