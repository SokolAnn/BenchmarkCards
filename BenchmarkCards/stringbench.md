# StringBench

## 📊 Benchmark Details

**Name**: StringBench

**Overview**: StringBench encompasses a wide range of string processing tasks, allowing systematic evaluation of LLMs’ performance in string processing capabilities.

**Data Type**: question-answering pairs

**Domains**:
- Natural Language Processing

**Languages**:
- English

**Resources**:
- [GitHub Repository](https://github.com/wxl-lxw/StringLLM)

## 🎯 Purpose and Intended Users

**Goal**: To systematically evaluate LLMs’ string processing capabilities and provide datasets that can help improve their performance.

**Target Audience**:
- ML Researchers
- Model Developers

**Tasks**:
- String Processing

**Limitations**: N/A

## 💾 Data

**Source**: Constructed using StringLLM, which combines atomic tasks into composite tasks and generates question-answer pairs.

**Size**: 1,511 tasks across three datasets

**Format**: N/A

**Annotation**: Ground truth answers generated by executing corresponding Python code for tasks.

## 🔬 Methodology

**Methods**:
- Raw Instructions
- Chain of Thought (CoT)
- Program of Thought (PoT)

**Metrics**:
- Accuracy

**Calculation**: Accuracy is measured by exact matches between the model's final output and the ground truth answer.

**Interpretation**: Higher accuracy indicates better performance in string processing tasks.

**Baseline Results**: N/A

**Validation**: Experiments conducted using a training-test split with 20% of data reserved for testing.

## ⚠️ Targeted Risks

**Risk Categories**:
- Accuracy

**Atlas Risks**:
- **Accuracy**: Poor model accuracy

**Demographic Analysis**: N/A

**Potential Harm**: N/A

## 🔒 Ethical and Legal Considerations

**Privacy And Anonymity**: Not Applicable

**Data Licensing**: Not Applicable

**Consent Procedures**: Not Applicable

**Compliance With Regulations**: Not Applicable
