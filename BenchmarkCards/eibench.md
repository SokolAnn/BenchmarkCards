# EIBench

## üìä Benchmark Details

**Name**: EIBench

**Overview**: EIBench is a curated benchmark for Emotion Interpretation (EI), featuring 1615 basic EI samples and 50 complex EI samples to evaluate the emotional triggers of affective states based on explicit and implicit factors.

**Data Type**: text

**Domains**:
- Natural Language Processing

**Languages**:
- English

**Resources**:
- [GitHub Repository](https://github.com/Lum1104/EIBench)

## üéØ Purpose and Intended Users

**Goal**: To provide a dataset that helps in understanding the causative factors behind emotional responses rather than just recognizing emotion labels.

**Target Audience**:
- ML Researchers
- Industry Practitioners
- Model Developers

**Tasks**:
- Emotion Interpretation

**Limitations**: N/A

## üíæ Data

**Source**: The dataset is derived from CAER-S and EmoSet.

**Size**: 1,665 samples

**Format**: N/A

**Annotation**: Coarse-to-Fine Self-Ask (CFSA) annotation method involving multiple rounds of questioning.

## üî¨ Methodology

**Methods**:
- Human evaluation
- Automated metrics

**Metrics**:
- Emotional Trigger Recall
- Long-Term Coherence

**Calculation**: Scores and evaluations are based on overlaps between predicted and actual emotional triggers.

**Interpretation**: Higher scores indicate better identification of emotional triggers.

**Validation**: The accuracy of annotations is validated by human evaluators achieving high confidence scores.

## ‚ö†Ô∏è Targeted Risks

**Risk Categories**:
- Bias
- Accuracy

**Atlas Risks**:
- **Accuracy**: Unrepresentative data
- **Fairness**: Data bias

**Demographic Analysis**: N/A

## üîí Ethical and Legal Considerations

**Privacy And Anonymity**: Not Applicable

**Data Licensing**: Not Applicable

**Consent Procedures**: Not Applicable

**Compliance With Regulations**: Not Applicable
