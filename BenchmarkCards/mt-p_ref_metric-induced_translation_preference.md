# MT-P REF (Metric-induced Translation PREFerence)

## üìä Benchmark Details

**Name**: MT-P REF (Metric-induced Translation PREFerence)

**Overview**: The dataset comprises 18k instances covering 18 language directions, using texts sourced from multiple domains post-2022. It leverages sentence-level quality assessments from professional linguists on translations generated by multiple high-quality MT systems to better align machine translation with human preferences.

**Data Type**: translation preference pairs

**Domains**:
- Natural Language Processing

**Languages**:
- English
- German
- Chinese
- Russian
- Portuguese
- Italian
- French
- Spanish
- Korean
- Dutch

**Similar Benchmarks**:
- ALMA-R-P REF

**Resources**:
- [Resource](https://huggingface.co/datasets/sardinelab/MT-pref)

## üéØ Purpose and Intended Users

**Goal**: To create a high-quality preference dataset for machine translation that better reflects human translation preferences by utilizing both automatic metrics and human evaluations.

**Target Audience**:
- ML Researchers
- Model Developers

**Tasks**:
- Machine Translation

**Limitations**: N/A

## üíæ Data

**Source**: Sentence-level quality assessments gathered from translations evaluated by professional linguists, relying on outputs from multiple high-quality MT models.

**Size**: 18,000 instances

**Format**: N/A

**Annotation**: Annotations were performed by professional linguists evaluating translations on a continuous scale of quality.

## üî¨ Methodology

**Methods**:
- Human evaluation
- Automatic metrics

**Metrics**:
- COMET
- XCOMET
- CHRF

**Calculation**: Metrics were calculated based on the correlation of automatic evaluations with human assessments of translation quality.

**Interpretation**: The results are interpreted based on the correlation of automatic metrics with human judgments, indicating the effectiveness of the translations.

**Baseline Results**: Models aligned with the MT-P REF dataset significantly outperform baseline models on WMT23 and FLORES benchmarks, demonstrating improved translation quality.

**Validation**: The dataset was validated through evaluations on competing translation models across different language pairs.

## ‚ö†Ô∏è Targeted Risks

**Risk Categories**:
- Bias
- Accuracy
- Safety

**Atlas Risks**:
- **Fairness**: Output bias
- **Accuracy**: Unrepresentative data, Poor model accuracy
- **Societal Impact**: Impact on education: bypassing learning, Impact on cultural diversity

**Demographic Analysis**: N/A

**Potential Harm**: Risk of generating biased translations or reflecting socio-cultural biases inherent in training data.

## üîí Ethical and Legal Considerations

**Privacy And Anonymity**: Not Applicable

**Data Licensing**: Not Applicable

**Consent Procedures**: Not Applicable

**Compliance With Regulations**: Not Applicable
