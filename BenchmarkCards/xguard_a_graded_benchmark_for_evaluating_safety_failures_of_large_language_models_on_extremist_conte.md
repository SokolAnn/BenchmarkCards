# XGUARD (A Graded Benchmark for Evaluating Safety Failures of Large Language Models on Extremist Content)

## üìä Benchmark Details

**Name**: XGUARD (A Graded Benchmark for Evaluating Safety Failures of Large Language Models on Extremist Content)

**Overview**: XGUARD is a benchmark and evaluation framework designed to assess the severity of extremist content generated by LLMs. It categorizes model outputs into five danger levels (0‚Äì4), enabling a more nuanced analysis of both the frequency and severity of failures, using 3,840 red-teaming prompts sourced from real-world data.

**Data Type**: text

**Domains**:
- Natural Language Processing

**Languages**:
- English

**Similar Benchmarks**:
- Agent-SafetyBench
- WalledEval
- BingoGuard
- RealHarm

**Resources**:
- [Resource](https://arxiv.org/abs/2506.00973)

## üéØ Purpose and Intended Users

**Goal**: To provide a graded assessment framework for extremist content generated by LLMs, allowing for better understanding of model vulnerabilities and defense mechanisms.

**Target Audience**:
- ML Researchers
- AI Safety Practitioners
- Policy Makers

**Tasks**:
- Content Moderation
- Terrorism Detection

**Limitations**: The dataset relies on semi-template prompt generation, which may not fully capture the complexity and variability of real user inputs.

## üíæ Data

**Source**: 3,840 red-teaming prompts sourced from real-world data such as social media and news.

**Size**: 3,840 examples

**Format**: JSON

**Annotation**: Manual annotation by experts to categorize responses into five danger levels.

## üî¨ Methodology

**Methods**:
- Automated metrics
- Model-based evaluation

**Metrics**:
- Attack Success Rate (ASR)
- Classification Success Rate (CSR)
- Attack Severity Curve (ASC)

**Calculation**: Metrics are calculated based on the proportion of prompts that successfully elicited extremist content and the accuracy of categorization.

**Interpretation**: Higher scores in ASR indicate more frequent generation of extremist content, while CSR reflects the effectiveness of content classification.

**Baseline Results**: ASR and CSR metrics were derived from evaluations of six popular LLMs.

**Validation**: Manual human validation was performed to ensure the reliability of the dataset and evaluative metrics.

## ‚ö†Ô∏è Targeted Risks

**Risk Categories**:
- Safety
- Accuracy

**Atlas Risks**:
- **Fairness**: Data bias
- **Accuracy**: Unrepresentative data

**Demographic Analysis**: The dataset includes diverse prompts aimed at evaluating different forms of extremist texts.

**Potential Harm**: The potential harm includes the dissemination of extremist narratives that could incite violence or promote radicalization.

## üîí Ethical and Legal Considerations

**Privacy And Anonymity**: Not Applicable

**Data Licensing**: Licensed for academic research; commercial use is not permitted.

**Consent Procedures**: Not Applicable

**Compliance With Regulations**: Not Applicable
