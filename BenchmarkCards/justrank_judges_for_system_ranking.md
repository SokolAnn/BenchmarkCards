# JuStRank (Judges for System Ranking)

## 📊 Benchmark Details

**Name**: JuStRank (Judges for System Ranking)

**Overview**: JuStRank compares judges by their ability to correctly rank models, based on agreement with a ground-truth model ranking.

**Data Type**: judgment scores

**Domains**:
- Natural Language Processing

**Languages**:
- English

**Similar Benchmarks**:
- RewardBench
- JudgeBench

**Resources**:
- [Resource](https://arxiv.org/abs/2412.09569)

## 🎯 Purpose and Intended Users

**Goal**: To evaluate the performance of LLM judges for ranking target systems.

**Target Audience**:
- ML Researchers
- Industry Practitioners

**Tasks**:
- System Ranking

**Limitations**: The gold reference data does not include user instructions or responses, which may limit direct comparisons.

## 💾 Data

**Source**: Arena Hard v0.1 dataset

**Size**: 1.5M judgment scores

**Format**: N/A

**Annotation**: Scores are generated by LLMs and reward models based on multiple system outputs.

## 🔬 Methodology

**Methods**:
- Ranking agreement analysis
- Statistical analysis

**Metrics**:
- Kendall's Tau correlation

**Calculation**: Correlation between the judge's ranking and the gold standard ranking.

**Interpretation**: Higher correlations indicate better ranking performance by the judge.

**Baseline Results**: Judges' performances were compared against the Chatbot Arena rankings.

**Validation**: Evaluated judges via system-level performance against human rankings.

## ⚠️ Targeted Risks

**Risk Categories**:
- Bias
- Accuracy

**Atlas Risks**:
- **Fairness**: Output bias
- **Accuracy**: Unrepresentative data

**Demographic Analysis**: Analysis of judge behaviors and biases towards specific systems.

**Potential Harm**: Potential unfair treatment of specific systems by judges.

## 🔒 Ethical and Legal Considerations

**Privacy And Anonymity**: Not Applicable

**Data Licensing**: Not Applicable

**Consent Procedures**: Not Applicable

**Compliance With Regulations**: Not Applicable
