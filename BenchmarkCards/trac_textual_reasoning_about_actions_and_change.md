# TRAC (Textual Reasoning about Actions and Change)

## üìä Benchmark Details

**Name**: TRAC (Textual Reasoning about Actions and Change)

**Overview**: We propose four essential RAC tasks as a comprehensive textual benchmark and generate problems in a way that minimizes the influence of other linguistic requirements (e.g., grounding) to focus on RAC. The resulting benchmark, TRAC, encompassing problems of various complexities, facilitates a more granular evaluation of LMs, precisely targeting the structural generalization ability much needed for RAC.

**Data Type**: text classification pairs (context-query pairs with true/false labels)

**Domains**:
- Natural Language Processing
- Artificial Intelligence

**Languages**:
- English

**Similar Benchmarks**:
- SQuAD
- Winograd Schema Challenge
- bAbI
- ProPara
- SuperGLUE

**Resources**:
- [Resource](https://arxiv.org/abs/2211.13930)

## üéØ Purpose and Intended Users

**Goal**: To provide a comprehensive textual benchmark of four RAC tasks (Projection, Executability, Planning, Goal-Recognition) to evaluate language models' ability to reason about actions and change and to test structural generalization.

**Tasks**:
- Projection
- Executability
- Planning
- Goal-Recognition

**Limitations**: Action domains are limited to deterministic and noise-free settings; goals are limited to either a literal (an atom or its negation) or a conjunction of two; the provided proof-of-concept uses the blocks world domain which is simplistic (one kind of object, three types of actions, three predicates).

**Out of Scope Uses**:
- Grounding and language variance are avoided and thus out-of-scope for this benchmark (clean-room evaluation).

## üíæ Data

**Source**: Synthetic datasets generated by a framework: symbolic problems generated from a STRIPS-based blocks world action domain and then translated into textual form in English via handcrafted templates.

**Size**: Twelve primary datasets (four tasks √ó three action-length variants), each dataset contains 15,000 label-balanced examples (12,000 training examples, of which 2,000 are used as a dev set, and 3,000 testing examples). Additionally, twenty more datasets were created for generalization experiments.

**Format**: N/A

**Annotation**: Automatically generated labels via symbolic generation following STRIPS semantics (queries labeled true/false based on symbolic execution); datasets are label-balanced.

## üî¨ Methodology

**Methods**:
- Automated metrics
- Model-based evaluation (baseline transformer models: RoBERTa, GPT-2, T5)

**Metrics**:
- Accuracy
- Mean and standard deviation

**Calculation**: Models are trained to predict the truth of the query using cross-entropy loss; evaluation reports Accuracy (percentage) and the mean and standard deviation over five repeated runs with different seeds.

**Interpretation**: Accuracy is used to evaluate model performance; the paper reports that baselines require at least 3,000 training samples to reach acceptable accuracies (above 80%) on standard datasets; higher accuracy indicates better ability to solve TRAC tasks and generalize structurally.

**Baseline Results**: Baseline mean accuracies (percent) from Table 2: RoBERTa ‚Äî Projection 87.36 (std 0.0396), Executability 99.73 (std 0.0013), Planning 87.63 (std 0.0158), Goal-Recognition 96.82 (std 0.0044). GPT-2 ‚Äî Projection 85.13 (std 0.0336), Executability 99.37 (std 0.0037), Planning 90.09 (std 0.0157), Goal-Recognition 97.44 (std 0.0021). T5 ‚Äî Projection 82.99 (std 0.0227), Executability 98.83 (std 0.0024), Planning 87.73 (std 0.0110), Goal-Recognition 94.04 (std 0.0082).

**Validation**: Each dataset split into 12,000 training examples (including 2,000 dev) and 3,000 test examples; experiments repeated five times to compute mean and standard deviation; additional out-of-distribution generalization evaluations (GE1-GE4) were used to validate structural generalization.

## ‚ö†Ô∏è Targeted Risks

**Atlas Risks**:
No specific atlas risks defined

## üîí Ethical and Legal Considerations

**Privacy And Anonymity**: Not Applicable

**Data Licensing**: Datasets and code released under the CRAPL license (the Community Research and Academic Programming License).

**Consent Procedures**: Not Applicable

**Compliance With Regulations**: Not Applicable
