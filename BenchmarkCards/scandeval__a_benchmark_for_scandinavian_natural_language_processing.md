# ScandEval: A Benchmark for Scandinavian Natural Language Processing

## üìä Benchmark Details

**Name**: ScandEval: A Benchmark for Scandinavian Natural Language Processing

**Overview**: This paper introduces a Scandinavian benchmarking platform, ScandEval, which can benchmark any pretrained model on four different tasks in the Scandinavian languages. The datasets used in two of the tasks, linguistic acceptability and question answering, are new. We develop and release a Python package and command-line interface, scandeval, which can benchmark any model that has been uploaded to the Hugging Face Hub, with reproducible results.

**Data Type**: question-answering pairs; sentences for linguistic acceptability (binary classification); token-level labeled text for named entity recognition; text classification examples for sentiment; text documents used for inference speed measurements

**Domains**:
- Natural Language Processing

**Languages**:
- Danish
- Norwegian
- Swedish
- Icelandic
- Faroese

**Similar Benchmarks**:
- SuperLim
- XGLUE
- GLUE
- SuperGLUE

**Resources**:
- [Resource](https://scandeval.github.io)
- [GitHub Repository](https://github.com/saattrupdan/ScandEval)
- [GitHub Repository](https://github.com/ScandEval/scandeval.github.io)
- [Resource](https://huggingface.co/ScandEval)
- [Resource](https://huggingface.co/datasets/alexandrainst/scandi-qa)
- [GitHub Repository](https://github.com/alexandrainst/ScandiQA)

## üéØ Purpose and Intended Users

**Goal**: Provide a benchmarking framework and leaderboard for Scandinavian language models, including two newly constructed datasets (ScaLA and ScandiQA), a Python package and CLI (scandeval), uniformised datasets on the Hugging Face Hub, and reproducible benchmarking of pretrained models.

**Target Audience**:
- Language Researchers
- Industry Practitioners
- Model Developers

**Tasks**:
- Named Entity Recognition
- Sentiment Classification
- Linguistic Acceptability (binary classification)
- Question Answering
- Inference Speed Measurement

**Limitations**: The ScandiQA dataset is a translated version of MKQA/NQ and "is not a perfect representation of the Mainland Scandinavian languages, as many of the questions and answers are concerned with topics specific to the USA."

## üíæ Data

**Source**: New datasets: ScaLA (Scandinavian Linguistic Acceptability) constructed from Universal Dependencies for Danish, Norwegian Bokm√•l, Norwegian Nynorsk, Swedish, Icelandic and Faroese; ScandiQA (question answering) based on MKQA (which is based on Natural Questions) with added context paragraphs located in NQ HTML and translated to Mainland Scandinavian languages. Existing datasets used/supplementing the benchmark: DaNE (Danish NER), NorNE (Norwegian NER), SUC3 (Swedish NER), AngryTweets (Danish sentiment), NoReC (Norwegian sentiment), SweReC (Swedish sentiment), WikiANN (Faroese NER subset), MIM-GOLD-NER (Icelandic NER), MKQA, Natural Questions.

**Size**: ScaLA: per language 1,024 training samples, 256 validation samples, 2,048 test samples. ScandiQA: original full dataset counts 7,810 Danish samples, 7,798 Swedish samples, 7,813 Norwegian samples. ScandEval (uniformised) uses 1,024/256/2,048 train/val/test per dataset as described in Section 3.4.

**Format**: DaNE and NorNE available in CONLL format; SUC3 provided in XML (with <name> tags for NER) converted for benchmarking; datasets and uniformised splits released on the Hugging Face Hub (ScandEval datasets).

**Annotation**: ScaLA: positive examples assumed from Universal Dependencies; negative examples generated by single-word removal or swapping under POS-based restrictions (conservative approach using gold-standard POS). AngryTweets: crowdsourced annotations (tweets anonymised by replacing user mentions with @USER and links with [LINK] and shuffling). NER datasets: NER-tagged dependency treebanks (gold-standard annotations). MKQA: human translations of questions and short answers. ScandiQA: English contexts located/extracted from NQ HTML and translated to Danish/Swedish via DeepL and to Norwegian via Google Translation; answer candidates extracted from translated contexts; samples without translated answer candidates discarded.

## üî¨ Methodology

**Methods**:
- Automated metrics
- Bootstrapping evaluation (10 iterations with sampling with replacement of test set)
- Finetuning with fixed hyperparameters via transformers package
- Reproducible benchmarking via scandeval Python package and CLI

**Metrics**:
- Micro-average F1-score (Named Entity Recognition)
- No-misc micro-average F1-score (NER, auxiliary)
- Matthew's Correlation Coefficient (Sentiment Classification, primary)
- Macro-average F1-score (Sentiment Classification, secondary)
- Mean inferences per second (Inference speed)
- 95% confidence intervals for reported scores

**Calculation**: For each model and dataset: repeat 10 times (fix a random seed per iteration; finetune on the training set; evaluate on a bootstrapped version of the test set). The reported evaluation score is the mean of these 10 scores, with a 95% confidence interval computed from the distribution of the 10 scores as described in Section 3.2.

**Interpretation**: Compute language-specific task scores as the mean of scores for (model, language, task) triples; aggregate to language scores by averaging language-specific task scores across tasks; final ScandEval score is the average of the language scores to emphasise performance across Scandinavian languages rather than monolingual performance.

**Baseline Results**: Benchmarked more than 100 pretrained models. Top-performing models reported include NB-BERT-large (best in Norwegian and overall), DFM-encoder-large-v1 (best Danish), and KB-BERT-large (best Swedish). Results and per-model scores with confidence intervals are published in the online leaderboard.

**Validation**: Reproducibility ensured through fixed random seeds and the scandeval package; robustness of reported scores assessed via bootstrapping the test set and reporting 95% confidence intervals over 10 runs.

## ‚ö†Ô∏è Targeted Risks

**Risk Categories**:
- Privacy
- Data Laws
- Societal Impact

**Atlas Risks**:
- **Privacy**: Personal information in data
- **Data Laws**: Data usage restrictions
- **Societal Impact**: Impact on the environment

## üîí Ethical and Legal Considerations

**Privacy And Anonymity**: To comply with Twitter's Terms of Use AngryTweets tweets were fully anonymised by replacing all user mentions with @USER and all links with [LINK], and by shuffling the tweets.

**Data Licensing**: Not Applicable

**Consent Procedures**: Not Applicable

**Compliance With Regulations**: Complies with Twitter's Terms of Use for AngryTweets by anonymising tweets; other regulatory compliance (e.g., GDPR) is not explicitly discussed.
