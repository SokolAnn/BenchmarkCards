# RACE: Large-scale ReAding Comprehension Dataset From Examinations

## üìä Benchmark Details

**Name**: RACE: Large-scale ReAding Comprehension Dataset From Examinations

**Overview**: We present RACE, a new dataset for benchmark evaluation of methods in the reading comprehension task. Collected from the English exams for middle and high school Chinese students (ages 12‚Äì18), RACE consists of near 28,000 passages and near 100,000 questions generated by human experts (English instructors), and covers a variety of topics designed to evaluate understanding and reasoning. The proportion of questions that require reasoning is much larger in RACE than in other benchmark datasets, and there is a significant gap between the performance of the state-of-the-art models (~43% accuracy) and the ceiling human performance (~95%).

**Data Type**: text: passages with multiple-choice question-answering pairs (four options per question)

**Domains**:
- Natural Language Processing
- Education

**Languages**:
- English

**Similar Benchmarks**:
- MCTest
- CNN/Daily Mail
- SQuAD
- NEWSQA
- Childrens Book Test
- Book Test
- Who Did What
- MS MARCO
- TriviaQA

**Resources**:
- [Resource](http://www.cs.cmu.edu/~glai1/data/race/)
- [GitHub Repository](https://github.com/qizhex/RACE_AR_baselines)
- [Resource](https://arxiv.org/abs/1704.04683)
- [GitHub Repository](https://github.com/tesseract-ocr)
- [Resource](https://www.abbyy.com/FineReader)
- [Resource](https://www.mturk.com/mturk/welcome)

## üéØ Purpose and Intended Users

**Goal**: Provide a large, high-quality dataset for evaluating and training methods in the reading comprehension (machine comprehension) task, with emphasis on reasoning-heavy questions.

**Target Audience**:
- Machine Learning Researchers
- Natural Language Processing Researchers
- Model Developers

**Tasks**:
- Reading Comprehension
- Question Answering
- Multiple-choice Question Answering

**Limitations**: Since the articles and questions are selected and designed to test Chinese students learning English as a foreign language, the vocabulary size and the complexity of the language constructs are simpler than news articles and Wikipedia articles in other QA datasets.

## üíæ Data

**Source**: Collected from English examinations designed for middle-school and high-school Chinese students (ages 12‚Äì18), extracted from three large free public websites in China.

**Size**: Final cleaned dataset: 27,933 passages and 97,687 questions. Raw data before cleaning: 137,918 passages and 519,878 questions.

**Format**: N/A

**Annotation**: Questions and candidate answers generated by human experts (English instructors). Reasoning-type labels for a sampled subset (500 questions) were annotated by two Amazon Mechanical Turk crowdworkers per question (restricted to master turkers).

## üî¨ Methodology

**Methods**:
- Automated metrics (Accuracy)
- Baseline rule-based model (Sliding Window)
- Neural model evaluation (Stanford Attentive Reader)
- Neural model evaluation (Gated-Attention Reader)
- Human evaluation via Amazon Mechanical Turk

**Metrics**:
- Accuracy

**Calculation**: Accuracy computed as the percentage of correctly answered multiple-choice questions (one correct option out of four).

**Interpretation**: Lower model accuracy indicates more challenging questions; the large gap between model accuracy (~43%) and ceiling human performance (~94.5%) indicates substantial room for improvement.

**Baseline Results**: Random baseline (RACE overall): 24.9% accuracy; Sliding Window: 32.2%; Stanford Attentive Reader: 43.3%; Gated-Attention Reader: 44.1%; Amazon Turkers (sampled subset): 73.3%; Ceiling human performance: 94.5%.

**Validation**: Dataset split: 5% development and 5% test for RACE-M and RACE-H respectively. Human annotation validation: sampled 500 questions (50 passages from each of RACE-M and RACE-H) labeled by two crowdworkers per question; ceiling human performance estimated by manually labeling proportion of valid questions (94.5% valid).

## ‚ö†Ô∏è Targeted Risks

**Risk Categories**:
- Intellectual Property

**Atlas Risks**:
- **Intellectual Property**: Data usage rights restrictions, Copyright infringement

**Demographic Analysis**: Dataset collected from English exams for Chinese students aged 12‚Äì18; split into RACE-M (middle school) and RACE-H (high school). RACE-H passages have greater length and larger vocabulary size than RACE-M, reflecting higher difficulty.

## üîí Ethical and Legal Considerations

**Privacy And Anonymity**: Not Applicable

**Data Licensing**: We checked that our dataset does not include example questions of exams with copyright, such as SSAT, SAT, TOEFL and GRE.

**Consent Procedures**: Not Applicable

**Compliance With Regulations**: Not Applicable
