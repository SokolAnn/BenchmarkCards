# Scope Ambiguities in Large Language Models

## 📊 Benchmark Details

**Name**: Scope Ambiguities in Large Language Models

**Overview**: This paper investigates how different versions of autoregressive language models treat scope-ambiguous sentences, introducing novel datasets that contain almost 1,000 unique scope-ambiguous sentences annotated for human judgments.

**Data Type**: text

**Domains**:
- Natural Language Processing

**Languages**:
- English

**Resources**:
- [GitHub Repository](https://github.com/McGill-NLP/scope-ambiguity)

## 🎯 Purpose and Intended Users

**Goal**: To investigate how large language models interpret scope ambiguities and compare their performance to human judgments.

**Target Audience**:
- ML Researchers
- Model Developers

**Tasks**:
- Scope Ambiguity Resolution

**Limitations**: N/A

## 💾 Data

**Source**: Constructed from existing datasets and expanded using human validation and model-generated sentences.

**Size**: 1,000 unique sentences

**Format**: JSON

**Annotation**: Annotated for human judgments regarding preferred readings of scope-ambiguous sentences.

## 🔬 Methodology

**Methods**:
- Human evaluation
- Automated metrics

**Metrics**:
- Accuracy

**Calculation**: Accuracy calculated based on the extent to which language models align with human preferences.

**Interpretation**: Higher accuracy indicates better alignment with human judgments.

**Validation**: Human validation through Prolific to determine the preferred readings of sentences.

## ⚠️ Targeted Risks

**Atlas Risks**:
No specific atlas risks defined

## 🔒 Ethical and Legal Considerations

**Privacy And Anonymity**: Not Applicable

**Data Licensing**: Not Applicable

**Consent Procedures**: Not Applicable

**Compliance With Regulations**: Not Applicable
