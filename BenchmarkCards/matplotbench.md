# MatPlotBench

## 📊 Benchmark Details

**Name**: MatPlotBench

**Overview**: MatPlotBench is a high-quality benchmark consisting of 100 human-verified test cases (query, raw data, ground-truth figure) designed to quantitatively evaluate AI methods for scientific data visualization. The benchmark supports an automatic scoring approach based on GPT-4V and is intended to enable automatic quantitative evaluation of AI methods in this task.

**Data Type**: multimodal (user queries: text; raw data: tabular/CSV; ground-truth figures: images)

**Domains**:
- Scientific Data Visualization

**Similar Benchmarks**:
- DS-1000
- Qwen-Agent Code Interpreter Benchmark

**Resources**:
- [GitHub Repository](https://github.com/thunlp/MatPlotAgent)

## 🎯 Purpose and Intended Users

**Goal**: Enable automatic quantitative evaluation of AI methods designed for scientific data visualization.

**Tasks**:
- Scientific Data Visualization
- Figure Generation

**Limitations**: MatPlotBench is developed for general scientific data visualization and may not encompass all domain-specific requirements, potentially restricting its applicability to certain fields.

## 💾 Data

**Source**: 75 original examples from the Matplotlib Gallery and 25 original examples from the OriginLab GraphGallery; examples were modified (data replacement for Matplotlib examples), converted to (query, raw data, ground-truth figure) triples, and human-verified.

**Size**: 100 examples

**Format**: Raw data: CSV files; Ground-truth figures: images (extracted from OriginLab or plotted and saved as PNG); Queries: text

**Annotation**: Preliminary queries generated by LLMs and revised by humans; human modification performed by annotators with a minimum of three years of coding/NLP experience, each query refined by two independent annotators; final verification by three NLP researchers.

## 🔬 Methodology

**Methods**:
- Automated evaluation using GPT-4V (scoring 0-100)
- Human evaluation (human annotators scoring generated plots)

**Metrics**:
- Score (0-100)
- Pearson correlation coefficient
- p-value
- Accuracy of Code Execution Results (%)

**Calculation**: Automatic score: GPT-4V is prompted to give a score from 0 to 100 comparing the model-generated plot and the ground-truth figure. For correlation with human evaluation: for each model, sample subsets of size n=25, repeat k=100 times to obtain average automatic scores A and average human scores H for each subset; compute Pearson correlation coefficient r and p-value using scipy.

**Interpretation**: Higher score (0-100) indicates closer match to the ground truth. Human evaluation guide maps score ranges to qualitative categories (Exact Match 90-100, High Resemblance 70-89, Moderate Resemblance 50-69, Low Resemblance 30-49, Poor Match 10-29, No Resemblance 1-9, Failure to Generate 0). Correlation interpretation: r > 0.8 and p < 0.05 indicates a strong correlation between automatic and human evaluation.

**Baseline Results**: Direct decoding baselines on MatPlotBench: GPT-4: 48.86; GPT-3.5: 38.03; Magicoder-S-DS-6.7B: 38.49; Deepseek-coder-6.7B-instruct: 31.53; CodeLlama-34B-Instruct: 16.54; Deepseek-coder-33B-instruct: 30.88; WizardCoder-Python-33B-V1.1: 36.94.

**Validation**: Human verification of (query, raw data, ground-truth) triples by three NLP researchers; human evaluation performed by recruited annotators; automatic evaluation validated by computing Pearson correlation between GPT-4V scores and human-annotated scores (for GPT-4: r=0.876, p=7.41e-33; for GPT-3.5: r=0.836, p=2.67e-27).

## ⚠️ Targeted Risks

**Atlas Risks**:
No specific atlas risks defined

**Demographic Analysis**: N/A

**Potential Harm**: N/A

## 🔒 Ethical and Legal Considerations

**Privacy And Anonymity**: Human annotators were informed that the collected data will be used solely for academic research purposes and their personal information will not be disclosed.

**Data Licensing**: Not Applicable

**Consent Procedures**: Human annotators were informed about the research use of collected data; annotators were compensated for their work.

**Compliance With Regulations**: Not Applicable
