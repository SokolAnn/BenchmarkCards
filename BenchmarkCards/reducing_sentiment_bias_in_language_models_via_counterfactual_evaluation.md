# Reducing Sentiment Bias in Language Models via Counterfactual Evaluation

## üìä Benchmark Details

**Name**: Reducing Sentiment Bias in Language Models via Counterfactual Evaluation

**Overview**: Quantify and reduce sentiment bias in language models by performing counterfactual evaluation: measuring how the sentiment distribution of generated text changes when sensitive attribute values (e.g., country names, occupations, genders, person names) in the conditioning context are changed, and proposing regularization methods to reduce such bias while retaining comparable perplexity and semantic similarity.

**Data Type**: text (generated continuations conditioned on input prefixes/templates)

**Domains**:
- Natural Language Processing

**Languages**:
- English

**Resources**:
- [Resource](https://arxiv.org/abs/1911.03064)
- [Resource](https://cloud.google.com/natural-language/)
- [Resource](http://data.statmt.org/news-crawl/)

## üéØ Purpose and Intended Users

**Goal**: To quantify counterfactual sentiment bias in texts generated by language models using Wasserstein-1 based fairness metrics, and to propose and evaluate methods (embedding regularization and sentiment-regularization) to reduce such bias while maintaining language model performance.

**Target Audience**:
- ML Researchers
- Model Developers

**Tasks**:
- Language Modeling
- Text Generation
- Sentiment Analysis
- Fairness Evaluation

**Limitations**: Employing specific templates for model evaluation can lack context-sensitivity and such evaluation is necessarily limited and not comprehensive. The sentiment classiÔ¨Åer used as a proxy might not be perfect and might exhibit some biases. There exists a trade-off between fairness and semantic relevance (strong regularization can reduce semantic relevance).

## üíæ Data

**Source**: Generated text continuations sampled from TransformerXL language models trained on WMT-19 (news articles) and WikiText-103 (Wikipedia). Evaluation conditions use designed templates (10 templates per sensitive attribute) and predefined sensitive attribute value lists (Country, Occupation, Name) to form conditioning prefixes; sentiment scores obtained via external sentiment classifiers and human annotation.

**Size**: Training data: WMT-19: 14,635,198 English news articles (paper uses last 10,000 for evaluation, 1,000 validation, 9,000 test); WikiText-103: 28,591 articles. Sentiment-classifier training subsets: 28,957,245 sentences for WMT-19 and 369,594 sentences for WikiText-103 (absolute sentiment score > 0.7 by Google Cloud API). Evaluation sampling: 1,000 generated continuations per template per sensitive attribute value; 10 templates per attribute; Country: 10 country names; Occupation: 29 occupations; Name: 17 female and 17 male names.

**Format**: Raw text (generated continuations, plain text); evaluation templates and sensitive token lists (text) described in Appendix A.

**Annotation**: Sentiment labels for training the sentiment projection obtained using the Google Cloud sentiment API; additional sentiment classifiers used: BERT-based fine-tuned on SST (92.7% validation accuracy) and an opinion-word-based classifier. Human evaluation: 19 annotators, each sentence labeled by 2 annotators for sentiment and semantic relevance (used for validation and correlation analysis).

## üî¨ Methodology

**Methods**:
- Automated metrics (Wasserstein-1 based fairness metrics, Perplexity, Semantic Similarity)
- Model-based evaluation (sentiment classifiers: Google Cloud sentiment API, BERT-based classifier, opinion-word-based classifier)
- Human evaluation (annotators labeling sentiment and semantic relevance)

**Metrics**:
- Wasserstein-1 distance
- Average Individual Fairness (I.F.)
- Average Group Fairness (G.F.)
- Perplexity
- Subset Perplexity (PPLs)
- Semantic Similarity (S.S.) via Universal Sentence Encoder cosine similarity
- Semantic Similarity conditioned (S.S.c) (fraction of continuations mentioning the sensitive attribute token)
- Spearman's correlation (between automatic metrics and human judgments)
- Cosine similarity

**Calculation**: Counterfactual sentiment bias measured as the Wasserstein-1 distance W1(PS(x), PS(cf(x;a;~a))) between sentiment score distributions of generated text for original input x and counterfactual input ~x (Eq.1). Individual fairness (I.F.) is the average W1 between PS(x_m) and PS(~x_m) across templates and unordered pairs of sensitive attribute values (Eq.3). Group fairness (G.F.) is the average W1 between subgroup sentiment distribution P_a_S and overall evaluation distribution PÃÑ_S (Eq.4). Perplexity reported on full test set and subset containing sensitive tokens (PPL and PPLs). Semantic similarity computed as cosine similarity between prefix and generated continuation embeddings using the Universal Sentence Encoder; above-threshold fraction defines S.S.; S.S.c is fraction mentioning sensitive token.

**Interpretation**: Lower I.F. and G.F. (Wasserstein-1) indicate less counterfactual sentiment bias. Lower Perplexity (PPL, PPLs) indicates better language model performance. Higher Semantic Similarity (S.S., S.S.c) indicates generated continuations are more semantically relevant to the prefix. Higher Spearman's correlation values indicate stronger agreement between automatic metrics and human judgments. The paper reports a trade-off: stronger fairness regularization tends to reduce I.F./G.F. but can decrease semantic relevance.

**Baseline Results**: Baseline (pre-debiasing) example results for Occupation (from Table 1): WMT-19 baseline PPL=17.9, PPLs=18.0, S.S.=17.9, S.S.c=9.9; WikiText-103 baseline PPL=18.9, PPLs=21.4, S.S.=40.3, S.S.c=24.3. (Figures and tables provide baseline I.F. and G.F. values and detailed comparisons.)

**Validation**: Model selection based on validation losses and validation perplexity; sentiment projection classifier trained on high-magnitude Google Cloud sentiment API labeled subset and reported accuracies of 98.8% (WMT-19 subset) and 98.7% (WikiText-103 subset). Multiple sentiment classifiers used to mitigate reliance on a single classifier. Human evaluation (19 annotators, 2 annotations per sentence) used to validate automatic sentiment and semantic similarity metrics; reported Spearman's correlations between automatic sentiment scores and human judgments: 0.75 (Google Cloud), 0.79 (BERT-based), 0.67 (opinion-word based). Spearman's correlations for semantic similarity: 0.72 (S.S.) and 0.63 (S.S.c).

## ‚ö†Ô∏è Targeted Risks

**Risk Categories**:
- Bias
- Fairness
- Accuracy

**Atlas Risks**:
- **Fairness**: Data bias, Output bias
- **Transparency**: Lack of training data transparency, Uncertain data provenance
- **Accuracy**: Poor model accuracy
- **Governance**: Lack of testing diversity

**Demographic Analysis**: Evaluation includes subgroup analyses: Country (10 country names), Occupation (29 occupations), Name (17 female and 17 male names). Individual and group fairness metrics computed across these subgroups.

**Potential Harm**: ['Sentiment bias in generated text that can cause unfair treatment or differential affect in downstream applications (e.g., dialogue agents) as stated in the paper.']

## üîí Ethical and Legal Considerations

**Privacy And Anonymity**: Not Applicable

**Data Licensing**: Not Applicable

**Consent Procedures**: Not Applicable

**Compliance With Regulations**: Not Applicable
