# MME: A Comprehensive Evaluation Benchmark for Multimodal Large Language Models

## üìä Benchmark Details

**Name**: MME: A Comprehensive Evaluation Benchmark for Multimodal Large Language Models

**Overview**: MME is the first comprehensive MLLM evaluation benchmark that measures both perception and cognition abilities on a total of 14 subtasks. All instruction-answer pairs are manually designed to avoid data leakage and instructions are concise ('Please answer yes or no') to allow fair comparison and quantitative statistics. 30 advanced MLLMs are evaluated on MME.

**Data Type**: image-based question-answering pairs (manually designed instruction-answer pairs)

**Domains**:
- Computer Vision
- Natural Language Processing

**Languages**:
- English
- Chinese

**Similar Benchmarks**:
- Microsoft COCO
- VQA
- ScienceQA
- MMBench

**Resources**:
- [GitHub Repository](https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models/tree/Evaluation)
- [Resource](https://arxiv.org/abs/2306.13394)

## üéØ Purpose and Intended Users

**Goal**: To provide a universal comprehensive evaluation benchmark for Multimodal Large Language Models (MLLMs) that covers perception and cognition, avoids data leakage by using manually designed instruction-answer pairs, uses concise unified instructions for fair comparison, and enables objective quantitative statistics.

**Target Audience**:
- Academic Researchers

**Tasks**:
- Visual Question Answering (yes/no)
- Optical Character Recognition
- Object Counting
- Object Positioning
- Object Color Recognition
- Movie Poster Recognition
- Celebrity Recognition
- Scene Recognition
- Landmark Recognition
- Artwork Recognition
- Commonsense Reasoning
- Numerical Calculation from images
- Text Translation (image-based Chinese-to-English)
- Code Reasoning (from images)

**Limitations**: This is the v1 version and will be updated. This version uses relatively simple samples and basic problems in some subtasks (e.g., basic translation problems, basic code problems).

## üíæ Data

**Source**: Perception images sampled from Microsoft COCO and other publicly available datasets [references: 19, 34, 35, 44, 58]; OCR images sampled from [30]; fine-grained and cognition images include manually photographed images and images generated by diffusion models. All instruction-answer pairs are manually designed and annotations do not directly use original public dataset annotations.

**Size**: Coarse-grained perception subtasks (existence, count, position, color): 30 images and 60 instruction-answer pairs per subtask. Fine-grained recognition: poster 147 images, celebrity 170 images, scene 200 images, landmark 200 images, artwork 200 images. OCR: 20 images and 40 instruction-answer pairs. Commonsense reasoning: 70 images and 140 instruction-answer pairs. Numerical calculation: 20 images and 40 instruction-answer pairs. Text translation: 20 images and 40 instruction-answer pairs. Code reasoning: 20 images and 40 instruction-answer pairs.

**Format**: N/A

**Annotation**: All instruction-answer pairs are manually designed.

## üî¨ Methodology

**Methods**:
- Zero-shot evaluation
- Automated metrics (Accuracy, Accuracy+)
- Leaderboard ranking (subtask scores and aggregated perception/cognition scores)

**Metrics**:
- Accuracy
- Accuracy+
- Subtask score (sum of Accuracy and Accuracy+)
- Perception score (sum of perception subtask scores)
- Cognition score (sum of cognition subtask scores)

**Calculation**: Accuracy is calculated per question. Accuracy+ is calculated per image where both of the two questions (one with ground-truth 'yes' and one with ground-truth 'no') must be answered correctly. Random accuracy baselines are 50% for Accuracy and 25% for Accuracy+. A subtask score = Accuracy + Accuracy+. Perception full score is 2000 and cognition full score is 800.

**Interpretation**: Accuracy+ is a stricter measurement that better reflects comprehensive understanding of an image. Random baselines are 50% (Accuracy) and 25% (Accuracy+); higher scores indicate better image understanding and instruction following.

**Baseline Results**: 30 advanced MLLMs were evaluated. Example results reported: overall perception top models include WeMM, InfMLLM, and SPHINX; GPT-4V leads many cognition tasks (e.g., commonsense reasoning score reported as 142.14, code reasoning score reported as 170). For object existence, several models (Otter, Lynx, WeMM, Muffin, SPHINX) achieved score 195 (98.33% Accuracy, 96.67% Accuracy+).

**Validation**: Instruction-answer pairs are manually designed to avoid data leakage; no formal validation procedures (e.g., inter-annotator agreement) are described.

## ‚ö†Ô∏è Targeted Risks

**Risk Categories**:
- Robustness
- Accuracy
- Value Alignment

**Atlas Risks**:
- **Robustness**: Hallucination
- **Accuracy**: Data contamination, Poor model accuracy
- **Value Alignment**: Over- or under-reliance

## üîí Ethical and Legal Considerations

**Privacy And Anonymity**: Not Applicable

**Data Licensing**: MME is collected by Xiamen University for academic research only.

**Consent Procedures**: Not Applicable

**Compliance With Regulations**: Not Applicable
