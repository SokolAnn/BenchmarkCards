# MegaSpeech

## üìä Benchmark Details

**Name**: MegaSpeech

**Overview**: A large-scale resource of 1 million realistic hate and non-hate text sequences, produced by fine-tuning GPT-2 on existing hate speech datasets and filtering generated samples using a fine-tuned BERT classifier, intended to improve generalization of DL-based hate speech detectors.

**Data Type**: text sequences (short text sequences, ‚â§30 tokens)

**Domains**:
- Natural Language Processing
- Social Media

**Similar Benchmarks**:
- DV
- FN
- WS
- WH
- SE

**Resources**:
- [Resource](N/A)

## üéØ Purpose and Intended Users

**Goal**: Significantly increase the generalization capabilities of hate-speech detection by generating and using a large-scale dataset (MegaSpeech) of 1M generated hate and non-hate text sequences to augment existing training sets and improve detection sensitivity (recall).

**Tasks**:
- Text Classification
- Hate Speech Detection

**Limitations**: Generated hate sequences were perceived as hate by humans 65% of the time and generated non-hate sequences were perceived as non-hate 86% of the time; generated data can introduce label noise and may decrease precision for some datasets; resource availability: will be made available for research upon request.

## üíæ Data

**Source**: Automatically generated by fine-tuned GPT-2 models using five public hate speech datasets (DV, FN, WS, WH, SE) as seed training examples; generated candidates were filtered and labeled using a fine-tuned BERT classifier.

**Size**: 1,000,000 text sequences

**Format**: N/A

**Annotation**: Automatically labeled via a fine-tuned BERT classifier (top-scoring 100K sequences per source dataset and class retained). A random sample of 1,000 generated sequences was manually annotated by the authors; inter-annotator Fleiss kappa on 250 co-annotated examples = 0.712.

## üî¨ Methodology

**Methods**:
- Automated metrics (Accuracy, Precision, Recall, F1)
- Model-based evaluation using a DL hate speech detector (Convolution-GRU architecture)
- Manual human annotation (sample of generated sequences)

**Metrics**:
- Accuracy
- Precision
- Recall
- F1 Score

**Calculation**: Metrics computed on held-out test sets (20% per dataset) with respect to the hate class. Classification threshold œÅ fixed at 0.78 across experiments. F1 is the harmonic mean of precision and recall.

**Interpretation**: High Recall (sensitivity) is prioritized as crucial for hate speech detection to reduce false negatives; there is a trade-off between precision and recall, and augmenting training data with generated sequences increases recall (often at the expense of precision).

**Baseline Results**: Average cross-dataset (Table 4 average): Baseline -> Augmented: Accuracy 0.740 -> 0.791 (+6.95%), Precision 0.539 -> 0.559 (+3.50%), Recall 0.151 -> 0.429 (+182.81%), F1 0.168 -> 0.470 (+179.71%).

**Validation**: Each dataset randomly split 80% training / 20% testing. Data generation and all training performed using the training sets. Manual validation: sample of 1,000 generated sequences manually labeled; Fleiss kappa 0.712 on 250 co-annotated examples.

## ‚ö†Ô∏è Targeted Risks

**Risk Categories**:
- Bias
- Accuracy
- Transparency
- Societal Impact

**Atlas Risks**:
- **Fairness**: Data bias
- **Accuracy**: Unrepresentative data, Poor model accuracy
- **Transparency**: Uncertain data provenance
- **Societal Impact**: Impact on affected communities

**Demographic Analysis**: N/A

**Potential Harm**: ['Spread of hatred', 'Igniting physical violence', 'Undetected hate speech (false negatives)']

## üîí Ethical and Legal Considerations

**Privacy And Anonymity**: Not Applicable

**Data Licensing**: Not Applicable

**Consent Procedures**: Not Applicable

**Compliance With Regulations**: Not Applicable
