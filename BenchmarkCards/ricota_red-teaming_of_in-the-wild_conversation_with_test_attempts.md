# RICoTA (Red-teaming of In-the-wild Conversation with Test Attempts)

## 📊 Benchmark Details

**Name**: RICoTA (Red-teaming of In-the-wild Conversation with Test Attempts)

**Overview**: A Korean red teaming dataset that consists of 609 prompts challenging LLMs with in-the-wild user-made dialogues capturing jailbreak attempts.

**Data Type**: text

**Domains**:
- Natural Language Processing

**Languages**:
- Korean

**Resources**:
- [GitHub Repository](https://github.com/boychaboy/RICoTA)

## 🎯 Purpose and Intended Users

**Goal**: To evaluate LLMs’ ability to identify conversation types and users’ testing purposes, and derive chatbot design implications for mitigating jailbreaking risks.

**Target Audience**:
- ML Researchers
- Chatbot Developers

**Tasks**:
- Intent Detection
- Conversation Type Classification

**Limitations**: The dataset focuses solely on Korean language interactions.

## 💾 Data

**Source**: User-generated dialogue screenshots collected from a Korean Reddit-like community.

**Size**: 609 examples

**Format**: JSON

**Annotation**: Annotated by three Korean L1 speakers for conversation types and testing purposes.

## 🔬 Methodology

**Methods**:
- Multiple choice question answering

**Metrics**:
- Accuracy
- F1 Score

**Calculation**: Evaluated using multiple choice answers generated by the model compared to human-annotated ground truth labels.

**Interpretation**: The model's ability to identify conversation types and user testing purposes is assessed based on response accuracy.

**Validation**: Evaluated against a human-annotated gold standard dataset.

## ⚠️ Targeted Risks

**Risk Categories**:
- Safety
- Accuracy
- Fairness

**Atlas Risks**:
No specific atlas risks defined

## 🔒 Ethical and Legal Considerations

**Privacy And Anonymity**: Collected dialogues contain potentially sensitive content; a disclaimer will accompany dataset distribution.

**Data Licensing**: CC BY-SA 4.0

**Consent Procedures**: Not Applicable

**Compliance With Regulations**: Not Applicable
