# SEVENLLM (Security Event Language Model)

## üìä Benchmark Details

**Name**: SEVENLLM (Security Event Language Model)

**Overview**: SEVENLLM introduces a framework to benchmark, elicit, and improve cybersecurity incident analysis and response abilities in large language models (LLMs). It creates a high-quality bilingual instruction corpus and an evaluation benchmark called SEVENLLM-Bench, designed to evaluate the performance of LLMs in cyber threat intelligence.

**Data Type**: question-answering pairs

**Domains**:
- Cybersecurity

**Languages**:
- English
- Chinese

**Resources**:
- [GitHub Repository](https://github.com/CSJianYang/SEevenLLM)

## üéØ Purpose and Intended Users

**Goal**: To enhance the capabilities of LLMs in analyzing and responding to cybersecurity incidents through a comprehensive benchmarking framework.

**Target Audience**:
- Cybersecurity Analysts
- ML Researchers
- Domain Experts

**Tasks**:
- Key Entity Recognition
- Main Relation Extraction
- Important Event Extraction
- Malware Feature Extraction
- Cybersecurity Event Classification
- Attack Tool Identification
- Domain Intelligence Acquisition
- Time Element Acquisition
- Network Protocol Utilization
- Enc-Dec Algorithm Identification
- Vulnerability Information Extraction
- Attacker Information Extraction
- Attack Target Intelligence Gathering
- Vulnerability Exploitation Analysis
- Attack Means Analysis
- Attack Strategy Analysis
- Correlation Analysis
- Attack Intent Analysis
- Threat Analysis
- Risk Assessment
- Impact Scope
- Trend Prediction
- Behavioral Pattern Analysis
- Protection Strategy Research
- Incident Response Planning
- Security Policy Audit
- Summary Generation
- Security Alert Generation

**Limitations**: The primary data source focuses on English, limiting multilingual capabilities.

## üíæ Data

**Source**: Cybersecurity incident websites and reports collected through web crawling.

**Size**: 6,706 English and 1,779 Chinese high-quality reports

**Format**: Text

**Annotation**: Human experts corrected tasks from raw candidate tasks generated by LLMs.

## üî¨ Methodology

**Methods**:
- Human evaluation
- Automated metrics
- Model-based evaluation

**Metrics**:
- Accuracy
- Rouge-L Score

**Calculation**: Metrics calculated based on the generated response and predefined criteria such as correctness and fluency.

**Interpretation**: Evaluation scores are rated on a scale of 1 to 5, with higher scores indicating better correctness and fluency.

**Baseline Results**: Compared performance against various models including GPT-3.5.

**Validation**: Results include manual verification and scoring by experts for accuracy.

## ‚ö†Ô∏è Targeted Risks

**Risk Categories**:
- Fairness
- Privacy
- Accuracy

**Atlas Risks**:
No specific atlas risks defined

**Demographic Analysis**: The benchmark's tasks are presented in both English and Chinese, assessing performance across different languages.

## üîí Ethical and Legal Considerations

**Privacy And Anonymity**: All data handling complied with ethical standards and legal regulations.

**Data Licensing**: Not Applicable

**Consent Procedures**: Not Applicable

**Compliance With Regulations**: Not Applicable
