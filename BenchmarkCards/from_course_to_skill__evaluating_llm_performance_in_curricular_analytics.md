# From Course to Skill: Evaluating LLM Performance in Curricular Analytics

## üìä Benchmark Details

**Name**: From Course to Skill: Evaluating LLM Performance in Curricular Analytics

**Overview**: This paper systematically evaluates the performance of large language models (LLMs) versus traditional NLP methods in the task of skill extraction from curriculum documents, demonstrating the varying performance of LLMs based on model selection and prompting strategies.

**Data Type**: curriculum documents

**Domains**:
- Natural Language Processing
- Higher Education

**Languages**:
- English

**Similar Benchmarks**:
- N/A

**Resources**:
- [GitHub Repository](https://github.com/AEQUITAS-Lab/Evaluation-of-LLM-in-CA-AIED-2025)

## üéØ Purpose and Intended Users

**Goal**: To evaluate and benchmark the performance of LLMs in curricular analytics against traditional NLP methods for skill extraction.

**Target Audience**:
- Researchers
- Educators

**Tasks**:
- Skill Extraction

**Limitations**: N/A

## üíæ Data

**Source**: Curriculum Documents from Open Syllabus and a public two-year college catalog.

**Size**: 400 curriculum documents

**Format**: N/A

**Annotation**: Human-Large Language Model collaborative evaluation framework.

## üî¨ Methodology

**Methods**:
- Human evaluation
- Automated metrics

**Metrics**:
- Precision
- Mean
- Normalized Discounted Cumulative Gain (NDCG)

**Calculation**: Performance metrics calculated based on human assessments of skill alignment.

**Interpretation**: Higher scores indicate better skill alignment from extracted skills.

**Baseline Results**: N/A

**Validation**: Used Cohen's Kappa and ICC for reliability analysis.

## ‚ö†Ô∏è Targeted Risks

**Risk Categories**:
- Accuracy

**Atlas Risks**:
- **Accuracy**: Poor model accuracy

**Demographic Analysis**: N/A

**Potential Harm**: N/A

## üîí Ethical and Legal Considerations

**Privacy And Anonymity**: Not Applicable

**Data Licensing**: Not Applicable

**Consent Procedures**: Not Applicable

**Compliance With Regulations**: Not Applicable
