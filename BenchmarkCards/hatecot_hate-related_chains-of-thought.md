# HateCOT (Hate-related Chains-of-Thought)

## üìä Benchmark Details

**Name**: HateCOT (Hate-related Chains-of-Thought)

**Overview**: We release HateCOT, a dataset of over 52,000 samples consisting of input text, a hate speech label, and an explanation for that label. This corpus is constructed by merging eight datasets with explanations created by using GPT-3.5-Turbo to augment human annotations.

**Data Type**: text

**Domains**:
- Natural Language Processing

**Languages**:
- English

**Similar Benchmarks**:
- HateCheck
- HateXplain
- Latent_Hate

**Resources**:
- [GitHub Repository](https://github.com/hnghiem-usc/hatecot)

## üéØ Purpose and Intended Users

**Goal**: To enhance the detection of offensive speech through a dataset that improves model performance in both zero-shot and few-shot settings while also providing explanations for detections.

**Target Audience**:
- ML Researchers
- Industry Practitioners
- Model Developers

**Tasks**:
- Offensive Speech Detection

**Limitations**: Limited to English corpora; the approach may not generalize to other languages without similar resources.

## üíæ Data

**Source**: Merged and curated from eight existing datasets focused on offensive speech detection.

**Size**: 52,137 samples

**Format**: JSON

**Annotation**: Annotated by human evaluators with additional explanations generated by GPT-3.5-Turbo.

## üî¨ Methodology

**Methods**:
- Automated metrics
- Human evaluation

**Metrics**:
- F1 Score
- Accuracy

**Calculation**: Calculated based on the predictive performance of models finetuned on HateCOT against various benchmarks.

**Interpretation**: Higher F1 Scores and accuracy indicate better model performance in detecting offensive speech.

**Baseline Results**: Compared against baseline models without pretraining on HateCOT.

**Validation**: Validation performed through testing on three different datasets under diverse conditions.

## ‚ö†Ô∏è Targeted Risks

**Risk Categories**:
- Fairness
- Accuracy

**Atlas Risks**:
- **Fairness**: Data bias
- **Accuracy**: Unrepresentative data

**Demographic Analysis**: The dataset may include biases inherent to its source texts.

**Potential Harm**: ['Propagating implicit biases in offensive speech detection.']

## üîí Ethical and Legal Considerations

**Privacy And Anonymity**: Anonymized posts by replacing user handles with '<user>'.

**Data Licensing**: Not Applicable

**Consent Procedures**: Not Applicable

**Compliance With Regulations**: Not Applicable
