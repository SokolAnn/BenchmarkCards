# TypeEvalPy: A Micro-benchmarking Framework for Python Type Inference Tools

## üìä Benchmark Details

**Name**: TypeEvalPy: A Micro-benchmarking Framework for Python Type Inference Tools

**Overview**: This paper evaluates the application of Large Language Models (LLMs) in static analysis tasks on Python programs using micro-benchmarks for call graph analysis and type inference.

**Data Type**: code snippets

**Domains**:
- Software Engineering

**Languages**:
- Python

**Resources**:
- [GitHub Repository](https://github.com/secure-software-engineering/TypeEvalPy)

## üéØ Purpose and Intended Users

**Goal**: To investigate the role of LLMs in improving call graph analysis and type inference in static analysis.

**Target Audience**:
- Software Engineers
- ML Researchers

**Tasks**:
- Call Graph Analysis
- Type Inference

**Limitations**: N/A

## üíæ Data

**Source**: Micro-benchmarks based on Python programs.

**Size**: 245 total code snippets

**Format**: N/A

**Annotation**: N/A

## üî¨ Methodology

**Methods**:
- Human evaluation
- Automated metrics

**Metrics**:
- Completeness
- Soundness
- Exact Matches

**Calculation**: Evaluated metrics based on the number of true positives and false positives in static analysis outputs.

**Interpretation**: Higher scores indicate better performance in identifying correct call graphs and type annotations.

**Validation**: Evaluated results using multiple protocols to ensure reliability.

## ‚ö†Ô∏è Targeted Risks

**Risk Categories**:
- Accuracy

**Atlas Risks**:
No specific atlas risks defined

**Demographic Analysis**: N/A

**Potential Harm**: N/A

## üîí Ethical and Legal Considerations

**Privacy And Anonymity**: Not Applicable

**Data Licensing**: Not Applicable

**Consent Procedures**: Not Applicable

**Compliance With Regulations**: Not Applicable
