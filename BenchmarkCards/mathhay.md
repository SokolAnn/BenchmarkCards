# MATHHAY

## üìä Benchmark Details

**Name**: MATHHAY

**Overview**: MATHHAY is an automated benchmark designed to evaluate long-context mathematical reasoning capabilities of LLMs, built through document collection, question generation, quality control, and haystack construction.

**Data Type**: mathematical reasoning tasks

**Domains**:
- Natural Language Processing

**Languages**:
- English

**Similar Benchmarks**:
- LongBench
- Needle in a Haystack
- NeedleBench

**Resources**:
- [Resource](N/A)

## üéØ Purpose and Intended Users

**Goal**: To evaluate the long-context mathematical reasoning abilities of LLMs in more real-world scenarios.

**Target Audience**:
- ML Researchers
- Industry Practitioners
- Model Developers

**Tasks**:
- Mathematical Reasoning

**Limitations**: N/A

## üíæ Data

**Source**: Constructed through automated document collection, question generation, and quality control.

**Size**: 673 questions

**Format**: N/A

**Annotation**: Questions verified for correctness by authors.

## üî¨ Methodology

**Methods**:
- Automated metrics

**Metrics**:
- Accuracy

**Calculation**: Accuracy calculated based on the proportion of correct answers from the models tested.

**Interpretation**: Higher accuracy indicates better mathematical reasoning capabilities in long contexts.

**Validation**: The benchmark is validated through comparison of answers generated via different methodologies.

## ‚ö†Ô∏è Targeted Risks

**Risk Categories**:
- Accuracy

**Atlas Risks**:
- **Accuracy**: Poor model accuracy

**Demographic Analysis**: N/A

**Potential Harm**: N/A

## üîí Ethical and Legal Considerations

**Privacy And Anonymity**: Not Applicable

**Data Licensing**: Not Applicable

**Consent Procedures**: Not Applicable

**Compliance With Regulations**: Not Applicable
