# MLE-bench

## 📊 Benchmark Details

**Name**: MLE-bench

**Overview**: MLE-bench is a benchmark for measuring how well AI agents perform at machine learning engineering. It consists of 75 ML engineering-related competitions curated from Kaggle to test real-world ML engineering skills.

**Data Type**: machine learning engineering tasks

**Domains**:
- Natural Language Processing
- Computer Vision
- Signal Processing

**Languages**:
- English

**Similar Benchmarks**:
- MLAgentBench
- SWE-bench
- DSBench

**Resources**:
- [GitHub Repository](https://github.com/openai/mle-bench/)

## 🎯 Purpose and Intended Users

**Goal**: To evaluate AI agents on ML engineering tasks using Kaggle competitions.

**Target Audience**:
- ML Researchers
- Industry Practitioners
- Model Developers

**Tasks**:
- Machine Learning Engineering

**Limitations**: N/A

## 💾 Data

**Source**: 75 Kaggle competitions curated to reflect core ML engineering skills.

**Size**: 75 competitions

**Format**: CSV

**Annotation**: Manually annotated problem types and complexity levels.

## 🔬 Methodology

**Methods**:
- Automated metrics
- Local grading

**Metrics**:
- Leaderboard rankings
- Medal achievement percentage

**Calculation**: Percentage of attempts awarded any medal (bronze and above).

**Interpretation**: Achieving medals indicates successful performance relative to human competitors.

**Baseline Results**: N/A

**Validation**: Local validation server checks the validity of submissions.

## ⚠️ Targeted Risks

**Risk Categories**:
- Fairness
- Accuracy
- Robustness
- Transparency

**Atlas Risks**:
- **Fairness**: Data bias
- **Accuracy**: Unrepresentative data
- **Robustness**: Data poisoning
- **Transparency**: Lack of training data transparency

**Demographic Analysis**: N/A

**Potential Harm**: N/A

## 🔒 Ethical and Legal Considerations

**Privacy And Anonymity**: Not Applicable

**Data Licensing**: Not Applicable

**Consent Procedures**: Not Applicable

**Compliance With Regulations**: Not Applicable
