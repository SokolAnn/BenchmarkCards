# CinePile: A Long Video Question Answering Dataset and Benchmark

## üìä Benchmark Details

**Name**: CinePile: A Long Video Question Answering Dataset and Benchmark

**Overview**: CinePile is a novel dataset and benchmark designed for long-form video understanding, comprising approximately 305,000 multiple-choice questions (MCQs) that cover various aspects such as temporal comprehension, human-object interactions, and event reasoning within scenes.

**Data Type**: question-answer pairs

**Domains**:
- Natural Language Processing
- Computer Vision

**Languages**:
- English

**Similar Benchmarks**:
- MovieQA
- TVQA
- MSRVTT-QA
- NExT-QA
- Perception Test

**Resources**:
- [Resource](https://hf.co/datasets/tomg-group-umd/cinepile)

## üéØ Purpose and Intended Users

**Goal**: The primary objective of CinePile is to evaluate models on long-form video understanding tasks, emphasizing authentic comprehension challenges rather than superficial analysis.

**Target Audience**:
- ML Researchers
- Model Developers
- Industry Practitioners

**Tasks**:
- Question Answering

**Limitations**: The dataset may have weaknesses in character grounding in time and might not match the complexity of questions generated by a motivated human.

## üíæ Data

**Source**: The data is sourced from approximately 9,396 clips obtained from English-language films available on YouTube, aligned with audio descriptions from audio descriptions collections for enhanced quality.

**Size**: 305,000 multiple-choice questions

**Format**: JSON

**Annotation**: Automated question generation and verification using human-generated data and LLM approaches.

## üî¨ Methodology

**Methods**:
- Automated metrics
- Model-based evaluation

**Metrics**:
- Accuracy

**Calculation**: Metrics are calculated based on the ability of models to select the correct answer from multiple-choice questions.

**Interpretation**: Performance is assessed by comparing model answers against a set of choices, determining accuracy based on correct identification.

**Validation**: The dataset's effectiveness was validated through human evaluations and automated model assessments.

## ‚ö†Ô∏è Targeted Risks

**Risk Categories**:
- Bias
- Robustness
- Accuracy

**Atlas Risks**:
- **Fairness**: Data bias
- **Robustness**: Prompt injection attack
- **Accuracy**: Unrepresentative data

**Demographic Analysis**: N/A

## üîí Ethical and Legal Considerations

**Privacy And Anonymity**: Not Applicable

**Data Licensing**: Not Applicable

**Consent Procedures**: Not Applicable

**Compliance With Regulations**: Not Applicable
