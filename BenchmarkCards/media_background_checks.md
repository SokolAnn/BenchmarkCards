# Media Background Checks

## 📊 Benchmark Details

**Name**: Media Background Checks

**Overview**: This paper introduces a novel task focused on generating Media Background Checks (MBCs) that summarise indicators of trustworthiness and tendency for media sources, to support source-critical reasoning by both humans and NLP models.

**Data Type**: text

**Domains**:
- Natural Language Processing

**Languages**:
- English

**Resources**:
- [GitHub Repository](https://github.com/MichSchli/MediaBackgroundChecks)

## 🎯 Purpose and Intended Users

**Goal**: To facilitate source-critical reasoning in NLP models and to provide a dataset for evaluating the generation of Media Background Checks.

**Target Audience**:
- NLP Researchers
- Model Developers

**Tasks**:
- Source-Critical Reasoning

**Limitations**: N/A

## 💾 Data

**Source**: Media Bias / Fact Check

**Size**: 6,709 examples

**Format**: Structured text

**Annotation**: Automatically generated by models leveraging retrieval-augmented generation.

## 🔬 Methodology

**Methods**:
- Automated metrics
- Human evaluation

**Metrics**:
- Accuracy
- F1 Score

**Calculation**: Metrics are calculated based on fact recall and error rates in generated background checks.

**Interpretation**: Higher fact recall indicates better performance in recalling the necessary trust indicators for media sources.

**Validation**: Human evaluation shows that MBCs improve the cognitive load for users and the correctness of responses in NLP models.

## ⚠️ Targeted Risks

**Risk Categories**:
- Bias
- Accuracy

**Atlas Risks**:
- **Fairness**: Data bias
- **Accuracy**: Unrepresentative data

**Demographic Analysis**: N/A

## 🔒 Ethical and Legal Considerations

**Privacy And Anonymity**: The dataset is based on publicly available data and does not contain personal information.

**Data Licensing**: CC-BY-NC-4.0

**Consent Procedures**: Not Applicable

**Compliance With Regulations**: Not Applicable
