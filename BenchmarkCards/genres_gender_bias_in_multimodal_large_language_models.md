# GENRES (Gender Bias in Multimodal Large Language Models)

## üìä Benchmark Details

**Name**: GENRES (Gender Bias in Multimodal Large Language Models)

**Overview**: GENRES is a novel benchmark designed to evaluate gender bias in multimodal large language models (MLLMs) through the lens of social relationships in generated narratives.

**Data Type**: narrative generation tasks

**Domains**:
- Natural Language Processing

**Languages**:
- English

**Similar Benchmarks**:
- VisBias
- SB-Bench
- SocialCounterfactuals

**Resources**:
- [GitHub Repository](https://github.com/Savannah2000/Genres.git)

## üéØ Purpose and Intended Users

**Goal**: To provide a structured benchmark for evaluating gender bias in narrative content generated by multimodal large language models (MLLMs).

**Target Audience**:
- ML Researchers
- AI Developers
- Social Scientists

**Tasks**:
- Bias Evaluation
- Narrative Generation

**Limitations**: While GENRES includes 1,440 NEPs, the scale could be further expanded in terms of story diversity and sample numbers per relationship type.

## üíæ Data

**Source**: Constructed through a semi-automated pipeline, utilizing diverse narrative elements to create Narrative Elicitation Pairs (NEPs).

**Size**: 1,440 NEPs

**Format**: JSON

**Annotation**: NEPs are generated intentionally to capture rich interpersonal dynamics and evaluated using both LLM-based and NLP-based tools.

## üî¨ Methodology

**Methods**:
- LLM-based analysis
- NLP techniques
- Statistical evaluation

**Metrics**:
- Profile Assignment Bias (PAB)
- Agency and Role Bias (ARB)
- Emotional Expression Bias (EEB)
- Narrative Framing Bias (NFB)

**Calculation**: Metrics are calculated based on model's output disparities between male and female representative character profiles.

**Interpretation**: A positive score on bias metrics indicates male-favoring bias, while a negative score indicates female-favoring bias.

**Baseline Results**: Using six MLLMs, results reveal persistent, context-sensitive gender biases.

**Validation**: The benchmark is validated by applying the metrics across multiple models and evaluating the consistency of bias patterns.

## ‚ö†Ô∏è Targeted Risks

**Risk Categories**:
- Bias
- Fairness
- Transparency

**Atlas Risks**:
- **Fairness**: Data bias
- **Transparency**: Lack of training data transparency

**Demographic Analysis**: N/A

**Potential Harm**: ['Reinforcement of gender stereotypes through biased model outputs.']

## üîí Ethical and Legal Considerations

**Privacy And Anonymity**: Not Applicable

**Data Licensing**: Not Applicable

**Consent Procedures**: Not Applicable

**Compliance With Regulations**: Not Applicable
