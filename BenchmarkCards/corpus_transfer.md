# corpus transfer

## üìä Benchmark Details

**Name**: corpus transfer

**Overview**: A methodology to link emergent and natural languages by pre-training models on a corpus of emergent language generated by an emergent communication (EC) speaker and fine-tuning on downstream natural language tasks (corpus transfer). The paper also proposes a translation-based metric (train a translation model mapping emergent messages to grounded natural captions and use ROUGE-L) to predict the transferability of emergent languages.

**Data Type**: multimodal: emergent message text paired with images and natural language captions (emergent message corpora and image-caption pairs)

**Domains**:
- Natural Language Processing
- Computer Vision

**Languages**:
- English
- Danish
- Basque
- Japanese
- Romanian
- Finnish
- Indonesian
- Kazakh
- Hebrew
- Urdu
- Persian

**Similar Benchmarks**:
- Game accuracy
- Topographic similarity

**Resources**:
- [GitHub Repository](https://github.com/ysymyth/ec-nl)
- [GitHub Repository](https://github.com/cambridgeltl/ECNMT/tree/master/ECPRETRAIN)
- [GitHub Repository](https://github.com/toizzy/tilt-transfer/tree/master/corpora/create_wiki_corpus)
- [GitHub Repository](https://github.com/huggingface/transformers/blob/v4.4.2/examples/language-modeling/run_clm.py)
- [GitHub Repository](https://github.com/microsoft/Oscar/blob/master/VinVL_DOWNLOAD.md)
- [GitHub Repository](https://github.com/krasserm/fairseq-image-captioning)

## üéØ Purpose and Intended Users

**Goal**: Investigate whether pretraining on a corpus of emergent language (generated by an EC speaker) provides transferable benefits for downstream natural language tasks (language modeling and image captioning), and develop a cheaper translation-based metric to predict emergent language transferability.

**Target Audience**:
- Emergent Communication researchers
- Natural Language Processing researchers

**Tasks**:
- Language Modeling
- Image Captioning
- Machine Translation (emergent to natural language)

**Limitations**: The pre-training and fine-tuning pipeline used as an evaluation scheme is computationally expensive as a scalable evaluation scheme for a population of emergent languages. Transfer benefits are most significant in low-resource downstream setups and may not scale linearly with larger pre-training corpora.

## üíæ Data

**Source**: Emergent language corpora (EC) generated by a trained emergent communication (speaker) on the Conceptual Captions dataset; comparison source corpora include a Spanish Wikipedia corpus (es) and a synthetic regular language (paren-zipf). Fine-tuning datasets: Wikipedia corpora for ten downstream languages and MS-COCO for image captioning.

**Size**: Conceptual Captions: more than 2.8 million images used to train EC speaker; source pre-training corpora sizes varied at 2 million, 5 million, 10 million, 15 million, and 30 million tokens; downstream fine-tuning Wikipedia corpora: 2 million tokens per language; MS-COCO: ~500,000 image-caption pairs (also subsets of 5,000 and 50,000 samples used). Specific experiments used 15 million EC tokens for some transfer experiments and 2 million pre-training tokens for low-resource language modeling experiments.

**Format**: Raw tokenized text corpora (emergent messages and natural captions), pre-processed with scripts (vocabulary culled to 50,000 for Wikipedia corpora); image features used (ResNet-18, detection features from Faster R-CNN/ResNet-101) for grounding.

**Annotation**: Emergent messages are automatically generated by the trained EC speaker. Natural language captions are from existing datasets (Conceptual Captions and MS-COCO) and used as ground-truth captions for translation metric and fine-tuning.

## üî¨ Methodology

**Methods**:
- Corpus transfer: pre-train language or captioning models on emergent language corpora generated by EC speaker, then fine-tune on downstream natural language data
- Translation-based evaluation: train a seq2seq model to translate emergent messages to corresponding natural captions (emergent -> natural) and score with ROUGE-L
- Automated evaluation metrics for downstream tasks (perplexity for language modeling; BLEU-4, ROUGE-L, CIDEr for image captioning)
- Correlation analysis (Pearson correlation) between proposed metrics and downstream performance across checkpoints and trials
- Ablation studies (random speaker, random inputs, permuted EC, model transfer vs corpus transfer)

**Metrics**:
- Perplexity
- BLEU-4
- ROUGE-L
- CIDEr
- Topographic similarity (Spearman rank correlation between message edit distance and input feature similarity)
- Game accuracy (referential game accuracy)
- Pearson correlation (between metrics and downstream performance)

**Calculation**: Test perplexity reported at best validation loss. Topographic similarity computed as Spearman rank correlation between Levenshtein distances of messages and negative cosine similarity of image features. Translation metric computed as ROUGE-L between translated emergent messages and ground-truth natural captions. Correlations (Pearson rho) computed between metrics (validation accuracy, topographic similarity, ROUGE-L translation score) and downstream performance (negated perplexity) across 200 checkpoints from multiple EC setups and trials.

**Interpretation**: Higher ROUGE-L translation scores predict better downstream transfer (e.g., Pearson rho = 0.83 on Hebrew); topographic similarity shows negligible correlation with downstream performance (e.g., rho = 0.003). A reduction in perplexity indicates beneficial transfer (e.g., pre-training on EC reduced perplexity by 24.6% on average across ten languages in a 2M-token low-resource setup).

**Baseline Results**: Language modeling: pre-training on emergent corpus reduces test perplexity by 24.6% on average versus training from scratch in a 2 million token low-resource setup across ten languages. Ablation (from Table 2): LM (ro) EC pretrain 198 (4) vs from scratch 265 (1); LM (he) EC pretrain 375 (14) vs from scratch 446 (8). Image captioning (Table 1, COCO 50k): BLEU-4 base 26.3, +ec 27.2, +nl 28.2; ROUGE-L base 50.0, +ec 50.7, +nl 51.5; CIDEr base 78.3, +ec 82.8, +nl 88.8. Model transfer with GRU performed worse than corpus transfer in reported ablations.

**Validation**: Test perplexity reported at best validation loss. Pre-training checkpoints selected (grid search found transfer checkpoint at 3000 pre-training steps for language modeling experiments). Correlation analyses computed over 200 checkpoints derived from 5 EC setups √ó 4 trials √ó 10 checkpoints; translation models for metric training used a held-out set of 50,000 captioned images and were trained for 2 epochs for metric evaluation.

## ‚ö†Ô∏è Targeted Risks

**Atlas Risks**:
No specific atlas risks defined

**Demographic Analysis**: N/A

**Potential Harm**: N/A

## üîí Ethical and Legal Considerations

**Privacy And Anonymity**: Not Applicable

**Data Licensing**: Not Applicable

**Consent Procedures**: Not Applicable

**Compliance With Regulations**: Not Applicable
