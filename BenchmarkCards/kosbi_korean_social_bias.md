# KOSBI (Korean Social Bias)

## üìä Benchmark Details

**Name**: KOSBI (Korean Social Bias)

**Overview**: KOSBI is a new social bias dataset of 34k pairs of contexts and sentences in Korean covering 72 demographic groups in 15 categories. It provides context-sentence pairs labeled as safe or unsafe (and unsafe subtypes: stereotypes, prejudice, discrimination, other) to measure and mitigate social biases in large language model generated content.

**Data Type**: text (context-sentence pairs)

**Domains**:
- Natural Language Processing

**Languages**:
- Korean
- English

**Similar Benchmarks**:
- BEEP!
- APEACH
- KOLD
- HateScore
- Unsmile
- K-MHaS

**Resources**:
- [GitHub Repository](https://github.com/naver-ai/korean-safety-benchmarks)
- [Resource](https://arxiv.org/abs/2305.17701)

## üéØ Purpose and Intended Users

**Goal**: To address social biases against a comprehensive set of demographic groups in South Korea by providing context-sentence pairs labeled as safe or unsafe so that LLMs can be trained/evaluated to behave safely when discussing demographic groups.

**Target Audience**:
- AI Safety Researchers
- Research groups on AI safety
- ML Researchers

**Tasks**:
- Text Classification
- Safety Evaluation
- Bias Mitigation
- Bias Detection

**Limitations**: KOSBI addresses social bias based on Korean culture with the Korean language. This Korean-specific property might restrict the effectiveness of our dataset outside Korea and similar cultures. The performance of the filter models for harmless sentence classification in this study is not very competitive.

**Out of Scope Uses**:
- Explicit hate speech detection (the dataset focuses on social bias without explicit hate speech)

## üíæ Data

**Source**: Generated by HyperCLOV A (LLM) via in-context few-shot prompting and demonstration-based prompting; initial demonstration pool manually curated by authors; generated contexts and sentences were filtered and then annotated by crowd workers. Demographic categories/groups were selected based on the Universal Declaration of Human Rights (UDHR) and the National Human Rights Commission of Korea (NHRCK).

**Size**: 34,214 context-sentence pairs (Train: 27,370; Validation: 3,421; Test: 3,423)

**Format**: N/A

**Annotation**: Crowd-sourced annotation by 200 crowd workers. Each instance labeled by three annotators and final label decided by majority vote. Labels: safe/unsafe and unsafe subtypes (stereotypes, prejudice, discrimination, other). Inter-annotator agreement reported via Krippendorff's alpha.

## üî¨ Methodology

**Methods**:
- Automated metrics (classifier-based safety scoring)
- Human evaluation (crowd workers)
- Filter-based moderation (rejection sampling / choosing safest among over-generated candidates)
- Fine-tuning classifiers (KLUE-BERT, KcELECTRa, KcBERT) for safe sentence classification

**Metrics**:
- Empirical probability of generating a safe sentence (safe score ‚â• 0.5) at least once over k generations
- Expected average safety score of safe sentences over k generations
- Macro F1
- Accuracy

**Calculation**: Empirical probability: probability that at least one generated sentence is classified as safe (safe score ‚â• 0.5) over k generations. Expected average safety: average safety score of safe sentences over k=8 generations. Classifier metrics reported as Macro F1 and Accuracy for fine-tuned models.

**Interpretation**: Higher empirical probability indicates a model more frequently generates at least one safe continuation given a context. Higher expected average safety indicates that generated sentences are, on average, safer. Macro F1 and Accuracy indicate performance of safe sentence classifiers.

**Baseline Results**: Table 4 shows Macro F1 (%) on the test set: BEEP! (KcBERT) 52.90; APEACH (KcBERT) 48.82; KOLD (KLUE-BERT) 38.15; Hatescore (KcBERT) 40.28; Unsmile (KcBERT) 48.02; Ours (KLUE-BERT) 69.94; Ours (KcELECTRa) 71.21.

**Validation**: Data split into Train/Validation/Test with 80%/10%/10% considering balance of social group distribution. Human evaluation: three crowd workers annotated contexts and sentences; final labels by majority vote. Inter-annotator agreement reported via Krippendorff's alpha for various labeling tasks. An augmented test set (6,801 pairs) was used for additional evaluation and human studies.

## ‚ö†Ô∏è Targeted Risks

**Risk Categories**:
- Bias
- Safety
- Fairness

**Atlas Risks**:
- **Fairness**: Data bias
- **Societal Impact**: Human exploitation

**Demographic Analysis**: Annotation demographics of 200 crowd workers are reported (gender, age groups, domestic area of origin, education, sexual orientation, disability), as shown in Table 12.

**Potential Harm**: ['Stereotypes (cognitive bias)', 'Prejudice (emotional bias)', 'Discrimination (behavioral bias)', 'Other (implicit/explicit)']

## üîí Ethical and Legal Considerations

**Privacy And Anonymity**: Not Applicable

**Data Licensing**: Not Applicable

**Consent Procedures**: Not Applicable

**Compliance With Regulations**: Study approved by the public institutional review board (IRB) affiliated with the Ministry of Health and Welfare of South Korea (P01-202211-01-016). Crowd worker compensation and measures to minimize stress exposure reported.
