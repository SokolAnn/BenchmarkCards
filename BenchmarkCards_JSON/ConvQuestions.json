{
  "benchmark_details": {
    "is_benchmark": true,
    "name": "ConvQuestions",
    "abbreviation": "N/A",
    "overview": "ConvQuestions is a crowdsourced benchmark with 11,200 distinct conversations from five domains for evaluating conversational question answering over knowledge graphs. Answers in the benchmark are grounded in Wikidata entities to enable fair comparison across diverse methods.",
    "data_type": "conversational question-answering pairs (questions and answers grounded to Wikidata entities)",
    "domains": [
      "Books",
      "Movies",
      "Soccer",
      "Music",
      "TV Series"
    ],
    "languages": [],
    "similar_benchmarks": [
      "WebQuestions",
      "SimpleQuestions",
      "WikiMovies",
      "ComplexWebQuestions",
      "ComQA",
      "CSQA",
      "QBLink",
      "CoQA",
      "ShARC",
      "SQA"
    ],
    "resources": [
      "http://qa.mpi-inf.mpg.de/convex/",
      "http://bit.ly/2QhsSDC",
      "https://www.mturk.com/help#what_are_masters"
    ]
  },
  "purpose_and_intended_users": {
    "goal": "Provide a realistic benchmark (ConvQuestions) to evaluate conversational question answering over knowledge graphs, with conversations grounded in Wikidata.",
    "audience": [],
    "tasks": [
      "Question Answering",
      "Conversational Question Answering"
    ],
    "limitations": "N/A",
    "out_of_scope_uses": [
      "Opinionated questions (e.g., \"best film by this actor?\")",
      "Non-factoid questions (causal, procedural, etc.)"
    ]
  },
  "data": {
    "source": "Crowdsourced from Amazon Mechanical Turk (AMT) Master Workers. Turkers created conversations (questions and answers), provided textual surface forms and Wikidata links for seed entities and answers, and paraphrases; answers are grounded in Wikidata entities.",
    "size": "11,200 conversations (35,840 questions); originally 350 seed conversations produced by 70 AMT workers (350 × 32 = 11,200).",
    "format": "N/A",
    "annotation": "Crowdsourced via Amazon Mechanical Turk (AMT) Master Workers; Turkers provided questions, paraphrases, textual surface forms and Wikidata links for seed entities and answers. Authors performed manual quality control and spam prevention and verified random utterances and alignments."
  },
  "methodology": {
    "methods": [
      "Automated metrics (P@1, MRR, Hit@5)",
      "Baseline comparisons (QAnswer, Platypus, Naive, Star/Chain models, Oracle, D2A)",
      "Model-based evaluation (Convex applied as an enabler to stand-alone KG-QA systems)"
    ],
    "metrics": [
      "Precision at top rank (P@1)",
      "Mean Reciprocal Rank (MRR)",
      "Hit@5"
    ],
    "calculation": "P@1: Precision at the top rank. MRR: Mean Reciprocal Rank. Hit@5: fraction of times a correct answer was retrieved within the top-5 positions.",
    "interpretation": "N/A",
    "baseline_results": "As reported: QAnswer alone scores about 0.011–0.064 (MRR) on follow-up incomplete utterances; QAnswer + Convex improves to approximately 0.172–0.264 (MRR) across domains. Similar improvements reported for Platypus when augmented with Convex.",
    "validation": "A random 20% of the 11k conversations was held out for tuning model parameters (development set), and the remaining 80% was used for testing. The development set was generated from a separate set of seed conversations to prevent leakage."
  },
  "targeted_risks": {
    "risk_categories": [],
    "atlas_risks": {
      "risks": null
    },
    "demographic_analysis": null,
    "harm": null
  },
  "ethical_and_legal_considerations": {
    "privacy_and_anonymity": "N/A",
    "data_licensing": "N/A",
    "consent_procedures": "N/A",
    "compliance_with_regulations": "N/A"
  }
}