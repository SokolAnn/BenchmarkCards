{
  "benchmark_details": {
    "is_benchmark": true,
    "name": "StoryBench: A Dynamic Benchmark for Evaluating Long-Term Memory with Multi Turns",
    "abbreviation": "N/A",
    "overview": "StoryBench is a dynamic benchmark framework based on interactive fiction games that evaluates long-term memory (LTM) capabilities of large language models (LLMs) by simulating complex, multi-turn interactions requiring knowledge retention and sequential reasoning.",
    "data_type": "text",
    "domains": [
      "Natural Language Processing"
    ],
    "languages": [
      "English"
    ],
    "similar_benchmarks": [],
    "resources": [
      "N/A"
    ]
  },
  "purpose_and_intended_users": {
    "goal": "To systematically evaluate long-term memory capabilities of LLMs through complex, dynamic, and multi-turn narrative environments.",
    "audience": [
      "ML Researchers",
      "Model Developers"
    ],
    "tasks": [
      "Sequential Reasoning",
      "Knowledge Retention"
    ],
    "limitations": "The benchmark scenarios are derived from a single interactive fiction domain, and the number of turns and length of the context are limited.",
    "out_of_scope_uses": []
  },
  "data": {
    "source": "The dataset is constructed from the interactive fiction game The Invisible Guardian.",
    "size": "311 scene nodes and 86 choice nodes",
    "format": "JSON",
    "annotation": "Manual annotation preserves the game's branching logic and causal relationships, ensuring chronological ordering."
  },
  "methodology": {
    "methods": [
      "Systematic evaluations on advanced LLMs",
      "Comparative analysis across different benchmark tasks"
    ],
    "metrics": [
      "Overall Accuracy",
      "First-Try Accuracy",
      "Longest Consecutive Correct Sequence",
      "Accuracy by Difficulty"
    ],
    "calculation": "Metrics are calculated based on the proportion of correct decisions made during the interactions.",
    "interpretation": "Higher scores in metrics indicate better knowledge retention and sequential reasoning capabilities.",
    "baseline_results": null,
    "validation": "The benchmark's effectiveness is validated through detailed experiments and in-depth failure analysis."
  },
  "targeted_risks": {
    "risk_categories": [
      "Accuracy",
      "Robustness"
    ],
    "atlas_risks": {
      "risks": []
    },
    "demographic_analysis": "N/A",
    "harm": "N/A"
  },
  "ethical_and_legal_considerations": {
    "privacy_and_anonymity": "N/A",
    "data_licensing": "N/A",
    "consent_procedures": "N/A",
    "compliance_with_regulations": "N/A"
  }
}