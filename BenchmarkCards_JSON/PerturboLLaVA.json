{
  "benchmark_details": {
    "name": "PerturboLLaVA",
    "overview": "This benchmark addresses hallucinations in Multimodal Large Language Models (MLLMs) particularly for dense image captioning tasks, introducing HalFscore, a novel metric for evaluation.",
    "data_type": "Images and text captions",
    "domains": [
      "Multimodal AI",
      "Image Captioning"
    ],
    "languages": [
      "English"
    ],
    "similar_benchmarks": [
      "CHAIR",
      "MMHalBench",
      "HallusionBench"
    ],
    "resources": [
      "Open-source implementation available"
    ]
  },
  "purpose_and_intended_users": {
    "goal": "To improve the fidelity and accuracy of image descriptions generated by MLLMs while mitigating hallucinations.",
    "audience": [
      "AI researchers",
      "Multimodal system developers",
      "Practitioners in computer vision and NLP"
    ],
    "tasks": [
      "Dense image captioning",
      "Evaluation of hallucinations in MLLMs"
    ],
    "limitations": "N/A",
    "out_of_scope_uses": [
      "Non-multimodal tasks",
      "Applications unrelated to image captioning"
    ]
  },
  "data": {
    "source": "Densely Captioned Images (DCI) dataset",
    "size": "1,000 images",
    "format": "Images with annotated captions",
    "annotation": "Manually annotated and densely captioned"
  },
  "methodology": {
    "methods": [
      "Introduction of HalFscore metric",
      "Incorporation of adversarially perturbed text during training"
    ],
    "metrics": [
      "Precision",
      "Recall",
      "HalFscore"
    ],
    "calculation": "HalFscore = 2 × (Precision × Recall) / (Precision + Recall)",
    "interpretation": "HalFscore aggregates the precision and recall for a comprehensive metric of captioning quality.",
    "baseline_results": "N/A",
    "validation": "Comparative analysis against established methods"
  },
  "targeted_risks": {
    "risk_categories": [
      "Hallucination evaluation",
      "Bias in model predictions"
    ],
    "atlas_risks": {
      "risks": [
        {
          "category": "Explainability",
          "subcategory": [
            "Unexplainable output"
          ]
        },
        {
          "category": "Robustness",
          "subcategory": [
            "Data poisoning"
          ]
        },
        {
          "category": "Transparency",
          "subcategory": [
            "Lack of training data transparency"
          ]
        }
      ]
    },
    "demographic_analysis": "N/A",
    "harm": "N/A"
  },
  "ethical_and_legal_considerations": {
    "privacy_and_anonymity": "N/A",
    "data_licensing": "N/A",
    "consent_procedures": "N/A",
    "compliance_with_regulations": "N/A"
  }
}