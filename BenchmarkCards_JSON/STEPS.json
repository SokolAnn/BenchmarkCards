{
  "benchmark_details": {
    "is_benchmark": true,
    "name": "STEPS: A Benchmark for Order Reasoning in Sequential Tasks",
    "abbreviation": "STEPS",
    "overview": "To verify the order reasoning capability of current neural models in sequential tasks, we propose a challenging benchmark, named STEPS. STEPS involves two subtask settings: (1) a classification setting that determines the rationality of a given next step in recipes given previous steps, and (2) a multi-choice setting that selects the reasonable next step from two candidate steps. The paper describes data construction, task formulations, and benchmarks of several large language models.",
    "data_type": "text (previous steps and candidate next-step pairs for classification; previous steps with two candidate next-step options for multi-choice)",
    "domains": [
      "Natural Language Processing",
      "Robotics"
    ],
    "languages": [],
    "similar_benchmarks": [
      "Piqa",
      "Commonsenseqa",
      "Lila"
    ],
    "resources": [
      "https://github.com/Victorwz/STEPS"
    ]
  },
  "purpose_and_intended_users": {
    "goal": "Evaluate the action order (sequence order) commonsense reasoning capability of language models on sequential tasks (recipes) via two subtasks: next-step rationality classification and multi-choice next-step selection.",
    "audience": [],
    "tasks": [
      "Text Classification",
      "Multiple-Choice Question Answering",
      "Commonsense Reasoning (Order reasoning)"
    ],
    "limitations": "Recipes with fewer than four action steps or more than ten action steps were filtered out due to input context length limitations of language models.",
    "out_of_scope_uses": []
  },
  "data": {
    "source": "FOOD.COM RECIPES dataset (web-crawled recipes from 'food.com' collected and released by Majumder et al., 2019). The STEPS datasets are constructed from these recipes using the original train/dev/test splits.",
    "size": "STEPS-CLS: 2.68M train examples, 1,000 dev examples, 1,000 test examples; STEPS-MC: 298K train examples, 1,000 dev examples, 1,000 test examples.",
    "format": "N/A",
    "annotation": "Automatically constructed from recipe step sequences: for classification, true samples are (previous steps, ground-truth next step) and false samples are (previous steps, later steps); for multi-choice each sample is (previous steps, true next step, randomly selected later step)."
  },
  "methodology": {
    "methods": [
      "Zero-shot prompting",
      "One-shot prompting",
      "Few-shot in-context learning (K=4 demonstrations; evaluation averaged over six random seeds)",
      "Full fine-tuning of models",
      "Perplexity-based scoring for multi-choice (language modeling scorer)"
    ],
    "metrics": [
      "Accuracy",
      "Sensitivity",
      "Specificity",
      "Geometric Mean (G-Mean) of Sensitivity and Specificity",
      "Perplexity (used as scorer for multi-choice)"
    ],
    "calculation": "For classification: Sensitivity = class-wise accuracy on positive class; Specificity = class-wise accuracy on negative class; G-Mean = geometric mean(Sensitivity, Specificity). For multi-choice: compute language modeling perplexity PPL(q) for each (previous steps + option) query and select the option with lower PPL; report Accuracy.",
    "interpretation": "Higher Accuracy, Sensitivity, Specificity, and G-Mean indicate better order reasoning capability. Authors note that LLM performance on these tasks lies in the 70%-80% accuracy range, which may indicate a potential performance upper-bound for pre-trained LLMs without additional knowledge sources.",
    "baseline_results": "Multi-Choice Accuracy (selected baselines): GPT2-Small 59.3; GPT2-Medium 63.5; GPT2-Large 65.1; OPT-1.3B 66.0; GPT2-XL 65.4; BLOOM-3B 68.4; BLOOM-7b1 69.8; OPT-13B 71.3; OPT-30B 71.7. Classification: in zero-shot many GPT-2 models predicted the positive class for all test samples (perfect Sensitivity but near-zero Specificity); fine-tuning improved Specificity substantially (e.g., authors report a 73% Specificity increase for GPT2-Large compared with zero-shot).",
    "validation": "Kept original train/dev/test splits from the source recipes. Filtered recipes with fewer than four or more than ten steps. For few-shot ICL, evaluation averaged over six random seeds for demonstration selection. For fine-tuning: up-sampling positive samples to balance classes, truncate resampled training set into segments of length 1024 tokens, fine-tune for 6000 updates with batch size 8, validate every 300 updates and save checkpoint with best validation performance."
  },
  "targeted_risks": {
    "risk_categories": [
      "Bias",
      "Accuracy"
    ],
    "atlas_risks": {
      "risks": [
        {
          "category": "Fairness",
          "subcategory": [
            "Data bias",
            "Output bias"
          ]
        },
        {
          "category": "Accuracy",
          "subcategory": [
            "Poor model accuracy"
          ]
        }
      ]
    },
    "demographic_analysis": "N/A",
    "harm": "N/A"
  },
  "ethical_and_legal_considerations": {
    "privacy_and_anonymity": "N/A",
    "data_licensing": "N/A",
    "consent_procedures": "N/A",
    "compliance_with_regulations": "N/A"
  }
}