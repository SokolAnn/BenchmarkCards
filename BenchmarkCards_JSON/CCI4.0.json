{
  "benchmark_details": {
    "is_benchmark": true,
    "name": "CCI4.0 (A Bilingual Pretraining Dataset for Enhancing Reasoning in Large Language Models)",
    "abbreviation": "CCI4.0",
    "overview": "CCI4.0 is a large-scale bilingual pre-training dataset designed to enhance reasoning capabilities in large language models through superior data quality and diverse reasoning trajectories.",
    "data_type": "text",
    "domains": [
      "Natural Language Processing"
    ],
    "languages": [
      "Chinese",
      "English"
    ],
    "similar_benchmarks": [],
    "resources": [
      "https://data.commoncrawl.org/contr/Nemotron/Nemotron-CC/index.html",
      "https://huggingface.co/datasets/BAAI/CCI2-Data",
      "https://huggingface.co/datasets/BAAI/CCI-Data",
      "https://huggingface.co/datasets/BAAI/CCI3.0-Data",
      "https://huggingface.co/datasets/allenai/dolma/blob/main/urls/v1_6.txt",
      "https://huggingface.co/datasets/OpenCoder-LLM/opc-fineweb-code-corpus",
      "https://huggingface.co/datasets/OpenCoder-LLM/opc-fineweb-math-corpus",
      "https://huggingface.co/datasets/allenai/dolma/blob/main/urls/v1_7.txt"
    ]
  },
  "purpose_and_intended_users": {
    "goal": "To provide high-quality and diverse corpora for large language model training, specifically focused on enhancing logical and commonsense reasoning capacities.",
    "audience": [
      "ML Researchers",
      "Model Developers"
    ],
    "tasks": [],
    "limitations": "Supports only Chinese and English languages; may not be suitable for researchers with limited computational resources.",
    "out_of_scope_uses": []
  },
  "data": {
    "source": "Multiple sources including Nemotron-CC and various open-source datasets",
    "size": "35TB",
    "format": "N/A",
    "annotation": "Human curation, multi-classifier quality scoring, various filtering methods"
  },
  "methodology": {
    "methods": [
      "Human evaluation",
      "Automated metrics"
    ],
    "metrics": [
      "Accuracy",
      "F1 Score"
    ],
    "calculation": "Empirical evaluations based on performance across various benchmarks.",
    "interpretation": "Demonstrates significant performance improvements on reasoning-intensive benchmarks.",
    "baseline_results": "Various benchmarks like MMLU and ARC-Challenge used for comparison.",
    "validation": "Results validated through empirical evaluations against existing benchmarks."
  },
  "targeted_risks": {
    "risk_categories": [
      "Bias",
      "Safety"
    ],
    "atlas_risks": {
      "risks": []
    },
    "demographic_analysis": "N/A",
    "harm": "N/A"
  },
  "ethical_and_legal_considerations": {
    "privacy_and_anonymity": "Applied privacy-preserving techniques and multiple toxicity filtering strategies.",
    "data_licensing": "N/A",
    "consent_procedures": "N/A",
    "compliance_with_regulations": "N/A"
  }
}