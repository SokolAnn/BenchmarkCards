{
  "benchmark_details": {
    "name": "Benchmark for reli ABilitYand jail Break ha LlUcination Evaluation (BABYBLUE)",
    "overview": "BABYBLUE introduces a specialized validation framework including various evaluators to enhance existing jailbreak benchmarks, ensuring outputs are useful malicious instructions. It aims to evaluate the true potential of jailbroken LLM outputs to cause harm to human society.",
    "data_type": "N/A",
    "domains": [
      "Artificial Intelligence",
      "Natural Language Processing",
      "Safety Evaluation"
    ],
    "languages": [
      "English"
    ],
    "similar_benchmarks": [
      "AdvBench",
      "HarmBench"
    ],
    "resources": [
      "https://github.com/Meirtz/BabyBLUE-llm"
    ]
  },
  "purpose_and_intended_users": {
    "goal": "To provide a comprehensive benchmark for evaluating the true threats posed by LLM jailbreaks, distinguishing between genuine threats and hallucinations.",
    "audience": [
      "Researchers",
      "AI Safety Analysts",
      "Developers of Language Models"
    ],
    "tasks": [
      "Assessing AI safety",
      "Evaluating jailbreak vulnerabilities",
      "Improving robustness against adversarial prompts"
    ],
    "limitations": null,
    "out_of_scope_uses": [
      "Harming individuals or groups",
      "Promoting illegal activities"
    ]
  },
  "data": {
    "source": "Augmented dataset with examples of existing malicious behaviors.",
    "size": "N/A",
    "format": "N/A",
    "annotation": "Meticulously curated examples that include both new behaviors and enhancements or modifications to existing behaviors."
  },
  "methodology": {
    "methods": [
      "Reasoning-based classification",
      "Textual quality evaluation",
      "Functionality evaluation"
    ],
    "metrics": [
      "Attack Success Rate (ASR)",
      "Precision",
      "Recall",
      "F1 Score"
    ],
    "calculation": "Evaluators calculate metrics based on classifications and assessments of each completion.",
    "interpretation": "Results are interpreted to differentiate between hallucinatory completions and genuinely harmful behaviors.",
    "baseline_results": "N/A",
    "validation": "Validation through comparison against human expert evaluations."
  },
  "targeted_risks": {
    "risk_categories": [
      "Privacy",
      "Safety",
      "Security"
    ],
    "atlas_risks": {
      "risks": [
        {
          "category": "Fairness",
          "subcategory": [
            "Data bias",
            "Output bias"
          ]
        },
        {
          "category": "Robustness",
          "subcategory": [
            "Prompt injection attack",
            "Extraction attack"
          ]
        },
        {
          "category": "Explainability",
          "subcategory": [
            "Inaccessible training data"
          ]
        }
      ]
    },
    "demographic_analysis": null,
    "harm": [
      "Potential misuse of outputs for harmful instructions",
      "Inducing hallucinatory behavior leading to false safety messages"
    ]
  },
  "ethical_and_legal_considerations": {
    "privacy_and_anonymity": "Maintains privacy and follows legal guidelines in dataset construction to avoid disclosing sensitive information.",
    "data_licensing": "N/A",
    "consent_procedures": "N/A",
    "compliance_with_regulations": "N/A"
  }
}