{
  "benchmark_details": {
    "is_benchmark": true,
    "name": "MMDocBench",
    "abbreviation": "N/A",
    "overview": "MMDocBench is a benchmark designed to holistically assess the fine-grained visual understanding capability of Large Vision-Language Models (LVLMs) through various OCR-free document understanding tasks. It consists of 15 main tasks and 48 sub-tasks, involving 2,400 document images, 4,338 QA pairs, and 11,353 supporting regions.",
    "data_type": "question-answering pairs",
    "domains": [
      "Natural Language Processing",
      "Computer Vision"
    ],
    "languages": [
      "English"
    ],
    "similar_benchmarks": [
      "MMVet",
      "MME",
      "MMT-Bench"
    ],
    "resources": [
      "https://MMDocBench.github.io/"
    ]
  },
  "purpose_and_intended_users": {
    "goal": "The primary objective of the benchmark is to comprehensively evaluate LVLMsâ€™ fine-grained visual perception and reasoning capabilities via various OCR-free document understanding tasks.",
    "audience": [
      "ML Researchers",
      "Industry Practitioners",
      "Model Developers"
    ],
    "tasks": [
      "Text Recognition",
      "Table Recognition",
      "Text Localization",
      "Table Cell Localization",
      "Key Information Extraction",
      "Document Forgery Detection",
      "Document Question Answering",
      "Chart Question Answering",
      "Infographic Question Answering",
      "Arithmetic Reasoning",
      "Logical Reasoning",
      "Spatial Reasoning",
      "Comparison",
      "Sorting",
      "Counting"
    ],
    "limitations": "N/A",
    "out_of_scope_uses": []
  },
  "data": {
    "source": "The data comes from 21 document understanding datasets covering a variety of document image types, including research papers, receipts, financial reports, and infographics.",
    "size": "4,338 QA pairs and 11,353 supporting regions",
    "format": "N/A",
    "annotation": "Annotations of supporting regions in the images for each QA pair are provided."
  },
  "methodology": {
    "methods": [
      "Human evaluation",
      "Automated metrics"
    ],
    "metrics": [
      "Exact Match (EM)",
      "F1 Score",
      "Intersection over Union (IOU)"
    ],
    "calculation": "Metrics are calculated based on the performance of LVLMs on the provided QA pairs across various tasks.",
    "interpretation": "Results are interpreted using EM, F1 scores, and IOU scores to gauge the effectiveness in answer and region predictions.",
    "baseline_results": "Struggles in identifying supporting regions with the best model achieving only 11.44% in IOU.",
    "validation": "N/A"
  },
  "targeted_risks": {
    "risk_categories": [
      "Bias",
      "Accuracy",
      "Fairness"
    ],
    "atlas_risks": {
      "risks": []
    },
    "demographic_analysis": "N/A",
    "harm": "N/A"
  },
  "ethical_and_legal_considerations": {
    "privacy_and_anonymity": "N/A",
    "data_licensing": "N/A",
    "consent_procedures": "N/A",
    "compliance_with_regulations": "N/A"
  }
}