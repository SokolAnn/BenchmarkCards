{
  "benchmark_details": {
    "is_benchmark": true,
    "name": "SURDS (Benchmarking Spatial Understanding and Reasoning in Driving Scenarios with Vision Language Models)",
    "abbreviation": "SURDS",
    "overview": "SURDS is a large-scale benchmark designed to systematically evaluate the spatial reasoning capabilities of vision language models (VLMs) through 41,080 training instances and 9,250 evaluation samples spanning six spatial categories: orientation, depth estimation, pixel-level localization, pairwise distance, lateral ordering, and front–behind relations.",
    "data_type": "vision–question–answer pairs",
    "domains": [
      "Computer Vision"
    ],
    "languages": [
      "English"
    ],
    "similar_benchmarks": [],
    "resources": [
      "https://github.com/XiandaGuo/Drive-MLLM"
    ]
  },
  "purpose_and_intended_users": {
    "goal": "To systematically evaluate and improve the spatial reasoning capabilities of vision language models (VLMs) in realistic driving scenarios.",
    "audience": [
      "ML Researchers",
      "Industry Practitioners",
      "Model Developers"
    ],
    "tasks": [
      "Distance Estimation",
      "Front/Behind Determination",
      "Left/Right Determination",
      "Depth Range Determination",
      "Yaw Angle Determination",
      "Pixel Location Estimation"
    ],
    "limitations": "N/A",
    "out_of_scope_uses": []
  },
  "data": {
    "source": "The nuScenes dataset, which is a large-scale public dataset specifically designed for autonomous driving research.",
    "size": "41,080 training instances and 9,250 evaluation instances",
    "format": "N/A",
    "annotation": "Multi-stage filtering pipeline including label-based and description-based strategies."
  },
  "methodology": {
    "methods": [
      "Supervised Fine-Tuning",
      "Reinforcement Learning"
    ],
    "metrics": [
      "Overall score",
      "Centerness-based metric for Pixel Localization"
    ],
    "calculation": "Scores are determined based on average performance across defined tasks.",
    "interpretation": "An overall score of VLMs is considered good if it significantly exceeds prior models, as shown in benchmark evaluations.",
    "baseline_results": "Qwen2.5-VL-72B-SFT-GRPO achieved a score of 40.80, outperforming models like GPT-4o and Gemini.",
    "validation": "Extensive comparative and ablation experiments demonstrate the performance of existing models against SURDS."
  },
  "targeted_risks": {
    "risk_categories": [
      "Bias",
      "Safety",
      "Robustness",
      "Fairness",
      "Accuracy"
    ],
    "atlas_risks": {
      "risks": []
    },
    "demographic_analysis": "N/A",
    "harm": []
  },
  "ethical_and_legal_considerations": {
    "privacy_and_anonymity": "N/A",
    "data_licensing": "N/A",
    "consent_procedures": "N/A",
    "compliance_with_regulations": "N/A"
  }
}