{
  "benchmark_details": {
    "is_benchmark": true,
    "name": "KMMLU (Measuring Massive Multitask Language Understanding in Korean)",
    "abbreviation": "KMMLU",
    "overview": "KMMLU is a new Korean benchmark consisting of 35,030 expert-level multiple-choice questions across 45 subjects, collected from original Korean exams to capture linguistic and cultural aspects of the Korean language. The dataset, prompts, and evaluation code are made publicly available on the Hugging Face Hub and integrated into EleutherAI's Language Model Evaluation Harness.",
    "data_type": "question-answering pairs (multiple-choice)",
    "domains": [
      "Natural Language Processing",
      "Humanities and Social Science",
      "Science, Technology, Engineering, and Mathematics",
      "Applied Science"
    ],
    "languages": [
      "Korean"
    ],
    "similar_benchmarks": [
      "MMLU (Massive Multitask Language Understanding)",
      "CMMLU",
      "KLUE",
      "Ko-H5",
      "KoBBQ",
      "HAE-RAE Benchmark",
      "CLIcK",
      "KorNAT"
    ],
    "resources": [
      "https://huggingface.co/datasets/HAERAE-HUB/KMMLU",
      "https://github.com/EleutherAI/lm-evaluation-harness/tree/main/lm_eval/tasks/kmmlu"
    ]
  },
  "purpose_and_intended_users": {
    "goal": "Provide a comprehensive Korean benchmark of expert-level multiple-choice questions across 45 subjects sourced from original Korean exams to evaluate and track large language model performance on Korean language and Korea-specific knowledge and reasoning.",
    "audience": [
      "Korean NLP Community",
      "Researchers",
      "Model Developers"
    ],
    "tasks": [
      "Question Answering",
      "Knowledge and Reasoning Evaluation (multiple-choice)"
    ],
    "limitations": "Copyright-related removals created coverage gaps (notably in Korean language, medical, and financial domains). Traditional benchmarks may not assess generative abilities and instruction-following skills. Potential misuse of benchmarks may produce models that perform poorly in real-world applications.",
    "out_of_scope_uses": [
      "Assessing generative abilities and instruction-following skills (the authors note KMMLU is a traditional knowledge benchmark and recommend future work to expand to generative evaluation)"
    ]
  },
  "data": {
    "source": "Collected from 533 diverse sources including Public Service Aptitude Test (PSAT), Korean License Tests, and the College Scholastic Ability Test (CSAT).",
    "size": "Total post-filtering collection: 243,777 questions; Training set: 208,522 questions; Validation set: 225 questions; Test set: 35,030 questions.",
    "format": "N/A",
    "annotation": "Original exam answer keys used as labels. Human accuracy data collected from actual test-takers when available (approximately 90% of exams include human performance data). Chain-of-Thought (CoT) exemplars elicited from GPT-4 and HyperCLOVA X with majority voting and manual selection/revision by authors; two workers per CoT question with ~87% initial agreement and iterative validation."
  },
  "methodology": {
    "methods": [
      "Few-shot evaluation (5-shot)",
      "Direct prompting with greedy decoding",
      "Chain-of-Thought (CoT) prompting (greedy decoding for CoT)",
      "Contamination checks using paraphrasing and perplexity / n-gram accuracy",
      "Evaluation using EleutherAI's LM-Eval-Harness"
    ],
    "metrics": [
      "Accuracy",
      "Macro-average Accuracy (across subjects/categories)",
      "Perplexity",
      "n-gram accuracy",
      "Human accuracy (collected from exam takers)"
    ],
    "calculation": "Accuracy is reported as macro-average accuracy across subjects within categories in a 5-shot setting; random guessing baseline is 25% (four choices). Perplexity is computed on original and paraphrased versions of samples. n-gram accuracy for proprietary models computed by sampling prompt prefixes and measuring prediction match for target n-grams as described in the paper.",
    "interpretation": "Average human accuracy reported as 62.6%; many license exams require an 80% passing score, so achieving over 80% on KMMLU is considered equivalent to minimum human expert performance. Random baseline is 25%. Reported model accuracies should be interpreted against these baselines and human performance.",
    "baseline_results": "Selected results (Direct method, 5-shot, macro/micro averages as reported): GPT-4 average accuracy 59.95%; HYPER CLOVA X average accuracy 53.40%. The paper notes the best public model scores approximately 50.5%.",
    "validation": "Public release of dataset for six months for community feedback (five issues reported; 741 instances modified). Manual review of Test and Validation sets for copyright with 147 test instances replaced. Contamination checks performed via paraphrasing and perplexity/n-gram accuracy analysis. CoT exemplars validated with two workers per question (~87% initial agreement) and iterative conflict resolution."
  },
  "targeted_risks": {
    "risk_categories": [
      "Accuracy",
      "Transparency",
      "Data Laws",
      "Intellectual Property",
      "Explainability",
      "Misuse",
      "Governance"
    ],
    "atlas_risks": {
      "risks": [
        {
          "category": "Accuracy",
          "subcategory": [
            "Data contamination",
            "Unrepresentative data"
          ]
        },
        {
          "category": "Transparency",
          "subcategory": [
            "Lack of training data transparency"
          ]
        },
        {
          "category": "Data Laws",
          "subcategory": [
            "Data usage restrictions"
          ]
        },
        {
          "category": "Intellectual Property",
          "subcategory": [
            "Copyright infringement",
            "Data usage rights restrictions"
          ]
        },
        {
          "category": "Explainability",
          "subcategory": [
            "Unexplainable output"
          ]
        },
        {
          "category": "Misuse",
          "subcategory": [
            "Improper usage"
          ]
        },
        {
          "category": "Governance",
          "subcategory": [
            "Lack of data transparency"
          ]
        }
      ]
    },
    "demographic_analysis": "The authors note variation in test origins and test-taker populations; comparing human accuracy directly may not be desirable due to these variations (human accuracy averages 62.6% but varies by exam source).",
    "harm": [
      "The authors state that potential misuse of benchmarks may pose societal risks and caution that optimizing solely for benchmarks may produce models that perform poorly in real-world applications."
    ]
  },
  "ethical_and_legal_considerations": {
    "privacy_and_anonymity": "Authors report that manual review of Test and Validation subsets did not identify personally identifiable information or offensive content.",
    "data_licensing": "Dataset released under a CC-BY-ND license. Evaluation code and prompts released under the MIT license (via EleutherAI's LM-Eval-Harness).",
    "consent_procedures": "N/A",
    "compliance_with_regulations": "N/A"
  }
}