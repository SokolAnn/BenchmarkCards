{
  "benchmark_details": {
    "is_benchmark": true,
    "name": "Fleurs-SLU: A Massively Multilingual Benchmark for Spoken Language Understanding",
    "abbreviation": "Fleurs-SLU",
    "overview": "Fleurs-SLU is a multilingual SLU benchmark that encompasses 692 hours of speech for topical utterance classification in 102 languages and multiple-choice question answering via listening comprehension spanning 944 hours of speech across 92 languages.",
    "data_type": "speech utterance classification and multiple-choice question answering",
    "domains": [
      "Natural Language Processing"
    ],
    "languages": [
      "English",
      "Afrikaans",
      "Amharic",
      "Modern Standard Arabic",
      "Assamese",
      "North Azerbaijani",
      "Bengali",
      "Bulgarian",
      "Catalan",
      "Cebuano",
      "Czech",
      "Central Kurdish",
      "Danish",
      "German",
      "Greek",
      "French",
      "Nigerian Fulfulde",
      "West Central Oromo",
      "Gujarati",
      "Hausa",
      "Hebrew",
      "Hindi",
      "Croatian",
      "Hungarian",
      "Armenian",
      "Igbo",
      "Indonesian",
      "Icelandic",
      "Italian",
      "Javanese",
      "Japanese",
      "Kannada",
      "Georgian",
      "Kazakh",
      "Kabuverdianu",
      "Halh",
      "Khmer",
      "Kyrgyz",
      "Korean",
      "Lao",
      "Lingala",
      "Lithuanian",
      "Ganda",
      "Luo",
      "Latvian",
      "Malayalam",
      "Marathi",
      "Macedonian",
      "Maltese",
      "Maori",
      "Burmese",
      "Dutch",
      "Norwegian Bokm√•l",
      "Nepali",
      "Northern Sotho",
      "Nyanja",
      "Occitan",
      "Odia",
      "Eastern Panjabi",
      "Southern Pashto",
      "Western Persian",
      "Polish",
      "Portuguese",
      "Romanian",
      "Russian",
      "Slovak",
      "Slovenian",
      "Shona",
      "Sindhi",
      "Somali",
      "Spanish",
      "Serbian",
      "Swedish",
      "Swahili",
      "Tamil",
      "Telugu",
      "Tajik",
      "Tagalog",
      "Thai",
      "Turkish",
      "Ukrainian",
      "Urdu",
      "Northern Uzbek",
      "Vietnamese",
      "Wolof",
      "Xhosa",
      "Yoruba",
      "Chinese (Simplified)",
      "Chinese (Traditional)",
      "Standard Malay",
      "Zulu"
    ],
    "similar_benchmarks": [],
    "resources": [
      "https://huggingface.co/datasets/SIB-Fleurs",
      "https://huggingface.co/datasets/Belebele-Fleurs"
    ]
  },
  "purpose_and_intended_users": {
    "goal": "To evaluate massively multilingual speech models in end-to-end speech classification and multiple-choice question answering.",
    "audience": [
      "ML Researchers",
      "Industry Practitioners",
      "Model Developers"
    ],
    "tasks": [
      "Utterance Classification",
      "Multiple-Choice Question Answering"
    ],
    "limitations": "N/A",
    "out_of_scope_uses": []
  },
  "data": {
    "source": "Merged datasets derived from Flores, SIB-200, and Belebele.",
    "size": "692 hours for utterance classification, 944 hours for multiple-choice question answering",
    "format": "N/A",
    "annotation": "N/A"
  },
  "methodology": {
    "methods": [
      "End-to-End Evaluation",
      "Cascaded Systems"
    ],
    "metrics": [
      "Accuracy",
      "F1 Score"
    ],
    "calculation": "Metrics calculated based on the performance of models across various languages and tasks.",
    "interpretation": "Higher scores indicate better model performance in classification and understanding tasks.",
    "baseline_results": "N/A",
    "validation": "Models evaluated on separate validation and test splits to ensure robustness."
  },
  "targeted_risks": {
    "risk_categories": [
      "Accuracy",
      "Fairness"
    ],
    "atlas_risks": {
      "risks": []
    },
    "demographic_analysis": "N/A",
    "harm": "N/A"
  },
  "ethical_and_legal_considerations": {
    "privacy_and_anonymity": "N/A",
    "data_licensing": "N/A",
    "consent_procedures": "N/A",
    "compliance_with_regulations": "N/A"
  }
}