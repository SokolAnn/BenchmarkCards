{
  "benchmark_details": {
    "is_benchmark": true,
    "name": "MegaSpeech",
    "abbreviation": "N/A",
    "overview": "A large-scale resource of 1 million realistic hate and non-hate text sequences, produced by fine-tuning GPT-2 on existing hate speech datasets and filtering generated samples using a fine-tuned BERT classifier, intended to improve generalization of DL-based hate speech detectors.",
    "data_type": "text sequences (short text sequences, ≤30 tokens)",
    "domains": [
      "Natural Language Processing",
      "Social Media"
    ],
    "languages": [],
    "similar_benchmarks": [
      "DV",
      "FN",
      "WS",
      "WH",
      "SE"
    ],
    "resources": [
      "N/A"
    ]
  },
  "purpose_and_intended_users": {
    "goal": "Significantly increase the generalization capabilities of hate-speech detection by generating and using a large-scale dataset (MegaSpeech) of 1M generated hate and non-hate text sequences to augment existing training sets and improve detection sensitivity (recall).",
    "audience": [],
    "tasks": [
      "Text Classification",
      "Hate Speech Detection"
    ],
    "limitations": "Generated hate sequences were perceived as hate by humans 65% of the time and generated non-hate sequences were perceived as non-hate 86% of the time; generated data can introduce label noise and may decrease precision for some datasets; resource availability: will be made available for research upon request.",
    "out_of_scope_uses": []
  },
  "data": {
    "source": "Automatically generated by fine-tuned GPT-2 models using five public hate speech datasets (DV, FN, WS, WH, SE) as seed training examples; generated candidates were filtered and labeled using a fine-tuned BERT classifier.",
    "size": "1,000,000 text sequences",
    "format": "N/A",
    "annotation": "Automatically labeled via a fine-tuned BERT classifier (top-scoring 100K sequences per source dataset and class retained). A random sample of 1,000 generated sequences was manually annotated by the authors; inter-annotator Fleiss kappa on 250 co-annotated examples = 0.712."
  },
  "methodology": {
    "methods": [
      "Automated metrics (Accuracy, Precision, Recall, F1)",
      "Model-based evaluation using a DL hate speech detector (Convolution-GRU architecture)",
      "Manual human annotation (sample of generated sequences)"
    ],
    "metrics": [
      "Accuracy",
      "Precision",
      "Recall",
      "F1 Score"
    ],
    "calculation": "Metrics computed on held-out test sets (20% per dataset) with respect to the hate class. Classification threshold ρ fixed at 0.78 across experiments. F1 is the harmonic mean of precision and recall.",
    "interpretation": "High Recall (sensitivity) is prioritized as crucial for hate speech detection to reduce false negatives; there is a trade-off between precision and recall, and augmenting training data with generated sequences increases recall (often at the expense of precision).",
    "baseline_results": "Average cross-dataset (Table 4 average): Baseline -> Augmented: Accuracy 0.740 -> 0.791 (+6.95%), Precision 0.539 -> 0.559 (+3.50%), Recall 0.151 -> 0.429 (+182.81%), F1 0.168 -> 0.470 (+179.71%).",
    "validation": "Each dataset randomly split 80% training / 20% testing. Data generation and all training performed using the training sets. Manual validation: sample of 1,000 generated sequences manually labeled; Fleiss kappa 0.712 on 250 co-annotated examples."
  },
  "targeted_risks": {
    "risk_categories": [
      "Bias",
      "Accuracy",
      "Transparency",
      "Societal Impact"
    ],
    "atlas_risks": {
      "risks": [
        {
          "category": "Fairness",
          "subcategory": [
            "Data bias"
          ]
        },
        {
          "category": "Accuracy",
          "subcategory": [
            "Unrepresentative data",
            "Poor model accuracy"
          ]
        },
        {
          "category": "Transparency",
          "subcategory": [
            "Uncertain data provenance"
          ]
        },
        {
          "category": "Societal Impact",
          "subcategory": [
            "Impact on affected communities"
          ]
        }
      ]
    },
    "demographic_analysis": "N/A",
    "harm": [
      "Spread of hatred",
      "Igniting physical violence",
      "Undetected hate speech (false negatives)"
    ]
  },
  "ethical_and_legal_considerations": {
    "privacy_and_anonymity": "N/A",
    "data_licensing": "N/A",
    "consent_procedures": "N/A",
    "compliance_with_regulations": "N/A"
  }
}