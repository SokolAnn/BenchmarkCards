{
  "benchmark_details": {
    "is_benchmark": true,
    "name": "CAUS (Curious About Uncertain Scene)",
    "abbreviation": "CAUS",
    "overview": "The CAUS dataset is designed to enable Large Language Models to emulate human cognitive processes for resolving uncertainties by generating pertinent questions related to given scene descriptions embedded with uncertainties.",
    "data_type": "scene description, reasoning sentences, inquisitive question sentences",
    "domains": [
      "Natural Language Processing"
    ],
    "languages": [
      "English"
    ],
    "similar_benchmarks": [],
    "resources": [
      "https://github.com/lbaa2022/CAUS"
    ]
  },
  "purpose_and_intended_users": {
    "goal": "To provide a dataset that emulates human cognitive processes for resolving uncertainty and to enhance the questioning capabilities of AI models.",
    "audience": [
      "ML Researchers",
      "AI Practitioners",
      "Cognitive Scientists"
    ],
    "tasks": [
      "Question Generation"
    ],
    "limitations": "N/A",
    "out_of_scope_uses": []
  },
  "data": {
    "source": "The dataset consists of automatically generated scene descriptions and questions backed by human reasoning.",
    "size": "1,000 scene descriptions, 1,000 reasoning sentences, 5,000 questions",
    "format": "text",
    "annotation": "Generated and classified using GPT-4"
  },
  "methodology": {
    "methods": [
      "Human evaluation",
      "Automated metrics"
    ],
    "metrics": [
      "Accuracy"
    ],
    "calculation": "The dataset was evaluated based on classification agreements between human and automated metrics.",
    "interpretation": "Results are interpreted based on how well the questions generated resolve the uncertainties presented in the scenes.",
    "baseline_results": null,
    "validation": "Data was validated by cross-checking question classifications between human annotators."
  },
  "targeted_risks": {
    "risk_categories": [
      "Bias",
      "Privacy"
    ],
    "atlas_risks": {
      "risks": [
        {
          "category": "Fairness",
          "subcategory": [
            "Data bias",
            "Output bias"
          ]
        }
      ]
    },
    "demographic_analysis": "N/A",
    "harm": []
  },
  "ethical_and_legal_considerations": {
    "privacy_and_anonymity": "N/A",
    "data_licensing": "N/A",
    "consent_procedures": "N/A",
    "compliance_with_regulations": "N/A"
  }
}