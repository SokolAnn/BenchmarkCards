{
  "benchmark_details": {
    "is_benchmark": true,
    "name": "N/A - Dataset not named in paper",
    "abbreviation": "N/A",
    "overview": "The paper introduces a new dataset of Bulgarian multiple-choice reading-comprehension questions (2,633 questions) collected from 12th grade matriculation exams and online history quizzes, and studies zero-shot transfer from English (models fine-tuned on RACE) to Bulgarian using Multilingual and Slavic BERT. The paper also proposes a pipeline to extract relevant contexts from Wikipedia using information retrieval and evaluates various indexing and pre-training strategies.",
    "data_type": "text (multiple-choice question-answer pairs)",
    "domains": [
      "Natural Language Processing",
      "Education"
    ],
    "languages": [
      "Bulgarian"
    ],
    "similar_benchmarks": [
      "SQuAD",
      "CoQA",
      "MS Macro",
      "RACE",
      "MCTest",
      "MS MARCO",
      "ARC"
    ],
    "resources": [
      "http://github.com/mhardalov/bg-reason-BERT",
      "http://github.com/deepmipt/Slavic-BERT-NER",
      "http://www.elastic.co/",
      "http://dumps.wikimedia.org/"
    ]
  },
  "purpose_and_intended_users": {
    "goal": "To build a task and dataset for a low-resource language (Bulgarian) to evaluate zero-shot multilingual transfer for multiple-choice reading comprehension, and to provide a pipeline for extracting relevant contexts from Wikipedia.",
    "audience": [],
    "tasks": [
      "Multiple-Choice Reading Comprehension"
    ],
    "limitations": "The dataset contains multiple-choice questions without explanatory contexts; resource scarceness in low-resource languages is noted as a challenge.",
    "out_of_scope_uses": []
  },
  "data": {
    "source": "Twelfth grade matriculation exams created by the Ministry of Education of Bulgaria (2008â€“2019) and online history quizzes; Wikipedia (Bulgarian dump) was used as external corpus for context retrieval.",
    "size": "2,633 multiple-choice questions",
    "format": "N/A",
    "annotation": "Correct answers as provided in the original matriculation exams and online quizzes."
  },
  "methodology": {
    "methods": [
      "Model-based evaluation (fine-tuned Multilingual BERT and Slavic BERT)",
      "Automated metrics (Accuracy)",
      "Information retrieval for context retrieval",
      "Ablation study and per-category analysis"
    ],
    "metrics": [
      "Accuracy"
    ],
    "calculation": "Accuracy calculated as the proportion of correctly answered multiple-choice questions, following the notation from Lai et al. (2017) (RACE).",
    "interpretation": "Higher accuracy indicates better performance; the paper reports that an accuracy of 42.23% is well above the random baseline of 24.89% and other non-random baselines.",
    "baseline_results": "Random baseline: 24.89% accuracy; Basic IR + BERT baseline: 29.62% accuracy; Best model (paragraph split + described query fields): 42.23% accuracy; Slavic pre-training result: 33.27% accuracy.",
    "validation": "Validation via ablation study, per-category analysis, and comparison to random and non-random baselines; evaluation performed on a Bulgarian testset and on RACE dev for English pre-training experiments."
  },
  "targeted_risks": {
    "risk_categories": [],
    "atlas_risks": {
      "risks": null
    },
    "demographic_analysis": null,
    "harm": null
  },
  "ethical_and_legal_considerations": {
    "privacy_and_anonymity": "N/A",
    "data_licensing": "N/A",
    "consent_procedures": "N/A",
    "compliance_with_regulations": "N/A"
  }
}