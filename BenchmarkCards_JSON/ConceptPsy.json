{
  "benchmark_details": {
    "is_benchmark": true,
    "name": "ConceptPsy",
    "abbreviation": "N/A",
    "overview": "ConceptPsy is a comprehensive benchmark designed to evaluate the complex reasoning and knowledge of Chinese Large Language Models (LLMs) in the field of psychology, encompassing 12 core subjects and 1,383 concepts generated from official exams.",
    "data_type": "multiple-choice questions",
    "domains": [
      "Natural Language Processing"
    ],
    "languages": [
      "Chinese"
    ],
    "similar_benchmarks": [
      "CMMLU",
      "C-EVAL"
    ],
    "resources": [
      "https://arxiv.org/abs/2311.09861"
    ]
  },
  "purpose_and_intended_users": {
    "goal": "To provide a comprehensive evaluation of LLMs' knowledge and reasoning abilities in psychology, addressing concept bias present in previous benchmarks.",
    "audience": [
      "ML Researchers",
      "Psychology Professionals",
      "Model Developers"
    ],
    "tasks": [
      "Question Answering"
    ],
    "limitations": "N/A",
    "out_of_scope_uses": []
  },
  "data": {
    "source": "Generated questions based on concepts from the National Post-graduate Entrance Examination in China, validated by psychology professionals.",
    "size": "4,573 questions",
    "format": "JSON",
    "annotation": "Questions validated by professional psychologists."
  },
  "methodology": {
    "methods": [
      "Human evaluation",
      "Automated metrics"
    ],
    "metrics": [
      "Accuracy"
    ],
    "calculation": "Metrics are calculated as the average accuracy across different conceptual topics evaluated.",
    "interpretation": "Higher accuracy scores indicate better understanding and reasoning ability across psychology concepts.",
    "baseline_results": null,
    "validation": "Reviewed by psychology professionals for content accuracy."
  },
  "targeted_risks": {
    "risk_categories": [
      "Bias",
      "Accuracy"
    ],
    "atlas_risks": {
      "risks": [
        {
          "category": "Fairness",
          "subcategory": [
            "Data bias",
            "Output bias"
          ]
        },
        {
          "category": "Accuracy",
          "subcategory": [
            "Unrepresentative data"
          ]
        }
      ]
    },
    "demographic_analysis": "N/A",
    "harm": []
  },
  "ethical_and_legal_considerations": {
    "privacy_and_anonymity": "N/A",
    "data_licensing": "N/A",
    "consent_procedures": "N/A",
    "compliance_with_regulations": "N/A"
  }
}