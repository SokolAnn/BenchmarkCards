{
  "benchmark_details": {
    "is_benchmark": true,
    "name": "LEXBENCH",
    "abbreviation": "N/A",
    "overview": "LEXBENCH is a comprehensive evaluation suite that tests language models on ten semantic phrase processing tasks, including idiomatic expression detection, noun compound identification, and lexical collocation extraction.",
    "data_type": "text",
    "domains": [
      "Natural Language Processing"
    ],
    "languages": [
      "English"
    ],
    "similar_benchmarks": [],
    "resources": [
      "https://github.com/jacklanda/LexBench"
    ]
  },
  "purpose_and_intended_users": {
    "goal": "To provide a robust evaluation framework for measuring the performance of language models on semantic phrase processing tasks.",
    "audience": [
      "ML Researchers",
      "Model Developers"
    ],
    "tasks": [
      "Idiomatic Expression Detection",
      "Idiomatic Expression Extraction",
      "Idiomatic Expression Interpretation",
      "Noun Compound Compositionality",
      "Noun Compound Extraction",
      "Noun Compound Interpretation",
      "Lexical Collocation Categorization",
      "Lexical Collocation Extraction",
      "Lexical Collocation Interpretation",
      "Verbal Multiword Expression Extraction"
    ],
    "limitations": "N/A",
    "out_of_scope_uses": []
  },
  "data": {
    "source": "Multiple datasets curated from existing sources, including ID10M and PIE datasets.",
    "size": "10 datasets across various tasks",
    "format": "N/A",
    "annotation": "Manually annotated and curated for specific semantic tasks."
  },
  "methodology": {
    "methods": [
      "Human evaluation",
      "Automated metrics"
    ],
    "metrics": [
      "Exact Match (EM)",
      "Token-level F1 Score",
      "Phrase-level Accuracy",
      "ROUGE-L",
      "BERT Score"
    ],
    "calculation": "Each metric calculated based on task-specific instructions for evaluating model outputs against gold standards.",
    "interpretation": "Higher scores indicate better performance in understanding and processing semantic phrases.",
    "baseline_results": "Performance metrics were compared across multiple language models including GPT-4 and Claude-3.",
    "validation": "Evaluation included comparisons with human performance."
  },
  "targeted_risks": {
    "risk_categories": [
      "Accuracy",
      "Fairness"
    ],
    "atlas_risks": {
      "risks": []
    },
    "demographic_analysis": "N/A",
    "harm": "N/A"
  },
  "ethical_and_legal_considerations": {
    "privacy_and_anonymity": "N/A",
    "data_licensing": "N/A",
    "consent_procedures": "N/A",
    "compliance_with_regulations": "N/A"
  }
}