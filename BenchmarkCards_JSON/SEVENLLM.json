{
  "benchmark_details": {
    "is_benchmark": true,
    "name": "SEVENLLM (Security Event Language Model)",
    "abbreviation": "SEVENLLM",
    "overview": "SEVENLLM introduces a framework to benchmark, elicit, and improve cybersecurity incident analysis and response abilities in large language models (LLMs). It creates a high-quality bilingual instruction corpus and an evaluation benchmark called SEVENLLM-Bench, designed to evaluate the performance of LLMs in cyber threat intelligence.",
    "data_type": "question-answering pairs",
    "domains": [
      "Cybersecurity"
    ],
    "languages": [
      "English",
      "Chinese"
    ],
    "similar_benchmarks": [],
    "resources": [
      "https://github.com/CSJianYang/SEevenLLM"
    ]
  },
  "purpose_and_intended_users": {
    "goal": "To enhance the capabilities of LLMs in analyzing and responding to cybersecurity incidents through a comprehensive benchmarking framework.",
    "audience": [
      "Cybersecurity Analysts",
      "ML Researchers",
      "Domain Experts"
    ],
    "tasks": [
      "Key Entity Recognition",
      "Main Relation Extraction",
      "Important Event Extraction",
      "Malware Feature Extraction",
      "Cybersecurity Event Classification",
      "Attack Tool Identification",
      "Domain Intelligence Acquisition",
      "Time Element Acquisition",
      "Network Protocol Utilization",
      "Enc-Dec Algorithm Identification",
      "Vulnerability Information Extraction",
      "Attacker Information Extraction",
      "Attack Target Intelligence Gathering",
      "Vulnerability Exploitation Analysis",
      "Attack Means Analysis",
      "Attack Strategy Analysis",
      "Correlation Analysis",
      "Attack Intent Analysis",
      "Threat Analysis",
      "Risk Assessment",
      "Impact Scope",
      "Trend Prediction",
      "Behavioral Pattern Analysis",
      "Protection Strategy Research",
      "Incident Response Planning",
      "Security Policy Audit",
      "Summary Generation",
      "Security Alert Generation"
    ],
    "limitations": "The primary data source focuses on English, limiting multilingual capabilities.",
    "out_of_scope_uses": []
  },
  "data": {
    "source": "Cybersecurity incident websites and reports collected through web crawling.",
    "size": "6,706 English and 1,779 Chinese high-quality reports",
    "format": "Text",
    "annotation": "Human experts corrected tasks from raw candidate tasks generated by LLMs."
  },
  "methodology": {
    "methods": [
      "Human evaluation",
      "Automated metrics",
      "Model-based evaluation"
    ],
    "metrics": [
      "Accuracy",
      "Rouge-L Score"
    ],
    "calculation": "Metrics calculated based on the generated response and predefined criteria such as correctness and fluency.",
    "interpretation": "Evaluation scores are rated on a scale of 1 to 5, with higher scores indicating better correctness and fluency.",
    "baseline_results": "Compared performance against various models including GPT-3.5.",
    "validation": "Results include manual verification and scoring by experts for accuracy."
  },
  "targeted_risks": {
    "risk_categories": [
      "Fairness",
      "Privacy",
      "Accuracy"
    ],
    "atlas_risks": {
      "risks": []
    },
    "demographic_analysis": "The benchmark's tasks are presented in both English and Chinese, assessing performance across different languages.",
    "harm": []
  },
  "ethical_and_legal_considerations": {
    "privacy_and_anonymity": "All data handling complied with ethical standards and legal regulations.",
    "data_licensing": "N/A",
    "consent_procedures": "N/A",
    "compliance_with_regulations": "N/A"
  }
}