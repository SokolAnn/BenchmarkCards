{
  "benchmark_details": {
    "is_benchmark": true,
    "name": "GraphEval36K",
    "abbreviation": "N/A",
    "overview": "GraphEval36K is a comprehensive graph dataset designed to evaluate the graph coding and reasoning capabilities of large language models (LLMs). It comprises 40 graph coding problems and 36,900 test cases, categorized into eight primary categories and four sub-categories to ensure thorough evaluation.",
    "data_type": "graph coding problems",
    "domains": [
      "Natural Language Processing"
    ],
    "languages": [
      "English"
    ],
    "similar_benchmarks": [
      "CLRS-30",
      "BIG-Bench",
      "ProofWriter",
      "PrOntoQA",
      "GraCoRe",
      "HGB"
    ],
    "resources": [
      "https://grapheval36k.github.io/"
    ]
  },
  "purpose_and_intended_users": {
    "goal": "To evaluate the graph-solving capabilities of LLMs through a collection of structured coding problems.",
    "audience": [
      "ML Researchers",
      "Model Developers"
    ],
    "tasks": [
      "Graph Reasoning",
      "Graph Coding"
    ],
    "limitations": "The dataset includes 40 coding problems and 36,900 graph samples, which is smaller compared to other LLM evaluation datasets.",
    "out_of_scope_uses": []
  },
  "data": {
    "source": "Generated using problems sourced from LeetCode and custom problem definitions.",
    "size": "36,900 test cases",
    "format": "N/A",
    "annotation": "N/A"
  },
  "methodology": {
    "methods": [
      "Automated metrics",
      "Code execution testing"
    ],
    "metrics": [
      "Passing rates"
    ],
    "calculation": "The passing rate is calculated based on the correctness of the generated code against the provided test cases.",
    "interpretation": "A higher passing rate indicates better performance of the LLM on graph problem-solving tasks.",
    "baseline_results": "Various baseline models are benchmarked against the dataset, including GPT-3.5, GPT-4, GPT-4o, and others.",
    "validation": "The dataset ensures comprehensive coverage by including multiple categories and difficulty levels."
  },
  "targeted_risks": {
    "risk_categories": [
      "Bias",
      "Accuracy",
      "Robustness"
    ],
    "atlas_risks": {
      "risks": []
    },
    "demographic_analysis": "N/A",
    "harm": []
  },
  "ethical_and_legal_considerations": {
    "privacy_and_anonymity": "N/A",
    "data_licensing": "MIT license",
    "consent_procedures": "N/A",
    "compliance_with_regulations": "N/A"
  }
}