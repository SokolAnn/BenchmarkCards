{
  "benchmark_details": {
    "is_benchmark": true,
    "name": "OpenUnlearning",
    "abbreviation": "N/A",
    "overview": "OpenUnlearning is a standardized and extensible framework designed explicitly for benchmarking LLM unlearning methods and metrics. It integrates unlearning algorithms and diverse evaluations across established benchmarks, facilitating comparative analysis and reproducibility in machine unlearning research.",
    "data_type": "unlearning methods and metrics evaluation",
    "domains": [
      "Natural Language Processing"
    ],
    "languages": [
      "English"
    ],
    "similar_benchmarks": [
      "TOFU",
      "MUSE",
      "WMDP"
    ],
    "resources": [
      "https://github.com/locuslab/open-unlearning",
      "https://huggingface.co/open-unlearning"
    ]
  },
  "purpose_and_intended_users": {
    "goal": "To establish a unified framework for benchmarking and improving LLM unlearning methods and metrics, enabling rigorous and reproducible research.",
    "audience": [
      "ML Researchers",
      "Model Developers",
      "Industry Practitioners"
    ],
    "tasks": [
      "Unlearning Evaluation",
      "Benchmarking Unlearning Methods"
    ],
    "limitations": "N/A",
    "out_of_scope_uses": []
  },
  "data": {
    "source": "Open-sourced model checkpoints and data generated for the benchmarks.",
    "size": "450+ models",
    "format": "N/A",
    "annotation": "N/A"
  },
  "methodology": {
    "methods": [
      "Unlearning Evaluation",
      "Meta-Evaluation",
      "Benchmarking"
    ],
    "metrics": [
      "Extraction Strength",
      "Exact Memorization",
      "Truth Ratio",
      "Probability",
      "ROUGE"
    ],
    "calculation": "Metrics are calculated based on evaluations conducted on various unlearned models under consistent conditions.",
    "interpretation": "High scores in metrics indicate effective unlearning and retention of model utility.",
    "baseline_results": "Results involve comparisons based on multiple unlearning methods with established benchmarks.",
    "validation": "Model behavior is validated against standard metrics like Truth Ratio and Mean Utility."
  },
  "targeted_risks": {
    "risk_categories": [
      "Privacy",
      "Accuracy",
      "Robustness"
    ],
    "atlas_risks": {
      "risks": [
        {
          "category": "Privacy",
          "subcategory": [
            "Personal information in data"
          ]
        },
        {
          "category": "Accuracy",
          "subcategory": [
            "Unrepresentative data",
            "Poor model accuracy"
          ]
        },
        {
          "category": "Robustness",
          "subcategory": [
            "Evasion attack",
            "Data poisoning"
          ]
        }
      ]
    },
    "demographic_analysis": "N/A",
    "harm": "N/A"
  },
  "ethical_and_legal_considerations": {
    "privacy_and_anonymity": "N/A",
    "data_licensing": "MIT License",
    "consent_procedures": "N/A",
    "compliance_with_regulations": "N/A"
  }
}