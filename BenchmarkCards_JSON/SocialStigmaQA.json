{
  "benchmark_details": {
    "is_benchmark": true,
    "name": "SocialStigmaQA: A Benchmark to Uncover Stigma Amplification in Generative Language Models",
    "abbreviation": "SocialStigmaQA",
    "overview": "SocialStigmaQA is a comprehensive benchmark designed to capture the amplification of social bias via stigmas in generative language models. It consists of roughly 10,360 prompts involving various prompt styles to systematically test for both social bias and model robustness.",
    "data_type": "question-answering pairs",
    "domains": [
      "Natural Language Processing"
    ],
    "languages": [
      "English"
    ],
    "similar_benchmarks": [
      "BBQ",
      "UnQover"
    ],
    "resources": [
      "https://arxiv.org/abs/2312.07492"
    ]
  },
  "purpose_and_intended_users": {
    "goal": "To measure biases against 93 social stigmas in a question-answering format and to uncover trends in stigma amplification in language models.",
    "audience": [
      "ML Researchers",
      "Model Auditors",
      "Ethics Researchers"
    ],
    "tasks": [
      "Question Answering"
    ],
    "limitations": "N/A",
    "out_of_scope_uses": []
  },
  "data": {
    "source": "Curated from a documented list of 93 US-centric stigmas, constructed into question-answering formats.",
    "size": "10,360 prompts",
    "format": "JSON",
    "annotation": "Manually curated and crafted templates by the authors."
  },
  "methodology": {
    "methods": [
      "Automated metrics",
      "Human evaluation"
    ],
    "metrics": [
      "Bias proportion"
    ],
    "calculation": "Quantified by calculating the proportion of socially biased output from responses to the prompt based on different decoding strategies.",
    "interpretation": "Higher proportions indicate greater amplification of bias, with varying results based on prompt styles.",
    "baseline_results": "Proportion of socially biased outputs ranges from 45% to 59% depending on the model and prompt style used.",
    "validation": "Results validated via manual inspection of generated outputs."
  },
  "targeted_risks": {
    "risk_categories": [
      "Bias",
      "Fairness",
      "Privacy"
    ],
    "atlas_risks": {
      "risks": [
        {
          "category": "Fairness",
          "subcategory": [
            "Data bias"
          ]
        },
        {
          "category": "Privacy",
          "subcategory": [
            "Personal information in prompt"
          ]
        }
      ]
    },
    "demographic_analysis": "The benchmark includes a demographic breakdown concerning the stigmas assessed.",
    "harm": "The benchmark is designed to address the potential harmful effects of biased outputs generated by language models."
  },
  "ethical_and_legal_considerations": {
    "privacy_and_anonymity": "The dataset focuses on social stigmas, hence it deals primarily with public perceptions rather than private details.",
    "data_licensing": "N/A",
    "consent_procedures": "N/A",
    "compliance_with_regulations": "N/A"
  }
}