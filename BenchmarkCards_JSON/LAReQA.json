{
  "benchmark_details": {
    "is_benchmark": true,
    "name": "LAReQA: Language-agnostic answer retrieval from a multilingual pool",
    "abbreviation": "LAReQA",
    "overview": "LAReQA is a challenging new benchmark for language-agnostic answer retrieval from a multilingual candidate pool. Unlike previous cross-lingual tasks, LAReQA tests for \"strong\" cross-lingual alignment, requiring semantically related cross-language pairs to be closer in representation space than unrelated same-language pairs.",
    "data_type": "question-answering pairs (text)",
    "domains": [
      "Natural Language Processing",
      "Information Retrieval"
    ],
    "languages": [
      "Arabic",
      "German",
      "Greek",
      "English",
      "Spanish",
      "Hindi",
      "Russian",
      "Thai",
      "Turkish",
      "Vietnamese",
      "Chinese"
    ],
    "similar_benchmarks": [
      "XQuAD",
      "MLQA",
      "XNLI",
      "XTREME",
      "BUCC",
      "Tatoeba",
      "BLI",
      "ReQA",
      "SQuAD",
      "USE-QA"
    ],
    "resources": [
      "https://arxiv.org/abs/2004.05484",
      "https://en.wikipedia.org/wiki/Wikipedia:Size_of_Wikipedia",
      "https://pypi.org/project/thai-segmenter",
      "https://tfhub.dev/google/universal-sentence-encoder-multilingual-qa"
    ]
  },
  "purpose_and_intended_users": {
    "goal": "To evaluate language-agnostic answer retrieval from a multilingual candidate pool and to test for \"strong\" cross-lingual alignment and language bias in representations.",
    "audience": [
      "ML Researchers",
      "Model Developers",
      "Industry Practitioners"
    ],
    "tasks": [
      "Question Answering",
      "Information Retrieval",
      "Cross-lingual Retrieval"
    ],
    "limitations": "Thai sentence breaking is noted as an outlier (around 70% the sentences per paragraph of other languages) due to lack of explicit sentence boundary markers, which affects candidate counts. The authors also note that Translate-Test baseline relies on an external machine translation system and thus sidesteps the alignment problem.",
    "out_of_scope_uses": []
  },
  "data": {
    "source": "Constructed by converting existing cross-lingual extractive QA tasks XQuAD and MLQA into retrieval tasks (XQuAD-R and MLQA-R) by breaking contextual paragraphs into sentences and including all sentences across the datasets as candidate answers; a sentence is correct if it contains the target answer span for that question or an equivalent question in another language (as identified by qasid).",
    "size": "XQuAD-R: 11 languages with 1,190 questions per language (13,090 total questions); candidates per language as reported in Table 1 range from 852 to 1,276. MLQA-R: per-language question and candidate counts as reported in Table 1 (e.g., English: 1,148 questions, 6,264 candidates; see Table 1 in paper).",
    "format": "N/A",
    "annotation": "Annotated with sentence boundaries as generated by an internal sentence breaker; sentences labeled as correct answers if they contain the target answer span (mapping across languages via qasid)."
  },
  "methodology": {
    "methods": [
      "Automated metrics (mean average precision)",
      "Model-based evaluation using dual-encoder embedding retrieval baselines (mBERT variants)",
      "Zero-shot monolingual retrieval evaluation and ablation analyses (remove-one-target, single-answer retrieval)"
    ],
    "metrics": [
      "Mean Average Precision (mAP)",
      "Precision@k",
      "Mean Reciprocal Rank (MRR)"
    ],
    "calculation": "mAP is defined as mAP = (1/|Q|) * sum_{qi in Q} (1/Ri) * sum_{j=1}^K P@j(qi) * rel(i,j), where Ri is the number of correct answers for question qi, P@j(qi) is Precision@j for qi, and rel(i,j) is 1 if the j-th ranked candidate for qi is correct, 0 otherwise. mAP ranges between 0 and 1.",
    "interpretation": "mAP between 0 and 1; a perfect 1.0 occurs when a model ranks all correct answers in the top C positions. High mAP requires both strong QA retrieval quality and absence of language bias. Single-answer retrieval mAP is equivalent to mean reciprocal rank (MRR) in that setting.",
    "baseline_results": "Main baseline mAPs on XQuAD-R and MLQA-R (Table 2): En-En: XQuAD-R 0.29, MLQA-R 0.36; X-X: XQuAD-R 0.23, MLQA-R 0.26; X-X-mono: XQuAD-R 0.52, MLQA-R 0.49; X-Y: XQuAD-R 0.66, MLQA-R 0.49; Translate-Test: XQuAD-R 0.72, MLQA-R 0.58. USE-QA on restricted set (Table 5): 0.51 on XQuAD-RUSE. Zero-shot monolingual retrieval results reported in Table 4.",
    "validation": "Evaluations conducted on XQuAD-R (primary) and MLQA-R; MLQA dev set used rather than test set for evaluation speed. Multiple analyses (remove-one-target, single-answer retrieval, zero-shot monolingual pool) used to validate language bias characteristics. Training details (e.g., 32 TPU-v3 cores, batch sizes, 100,000 steps) reported and no overfitting observed."
  },
  "targeted_risks": {
    "risk_categories": [
      "Bias",
      "Accuracy"
    ],
    "atlas_risks": {
      "risks": [
        {
          "category": "Fairness",
          "subcategory": [
            "Output bias"
          ]
        },
        {
          "category": "Accuracy",
          "subcategory": [
            "Poor model accuracy"
          ]
        }
      ]
    },
    "demographic_analysis": "N/A",
    "harm": "The paper states that language bias (e.g., same-language bias) is harmful because if the model prefers answers in a given language, it is prone to retrieve irrelevant results in that language over relevant results from another language."
  },
  "ethical_and_legal_considerations": {
    "privacy_and_anonymity": "N/A",
    "data_licensing": "N/A",
    "consent_procedures": "N/A",
    "compliance_with_regulations": "N/A"
  }
}