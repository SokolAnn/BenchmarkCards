{
  "benchmark_details": {
    "is_benchmark": true,
    "name": "BOLAA (Benchmarking and Orchestrating LLM-augmented Autonomous Agents)",
    "abbreviation": "BOLAA",
    "overview": "BOLAA is a new benchmark that systematically evaluates LLM-augmented Autonomous Agents (LAAs) across various agent architectures and LLM backbones. It aims to address the gaps in understanding and orchestrating multiple LAAs for handling complex tasks in simulated environments.",
    "data_type": "task-based interactions",
    "domains": [
      "Artificial Intelligence",
      "Natural Language Processing"
    ],
    "languages": [
      "English"
    ],
    "similar_benchmarks": [
      "AgentBench"
    ],
    "resources": [
      "https://github.com/salesforce/BOLAA"
    ]
  },
  "purpose_and_intended_users": {
    "goal": "To provide a comprehensive comparison and benchmark for LLM-augmented Autonomous Agents (LAAs) in complex task environments.",
    "audience": [
      "ML Researchers",
      "AI Developers",
      "Industry Practitioners"
    ],
    "tasks": [
      "Multi-agent coordination",
      "Task complexity evaluation",
      "Decision-making",
      "Knowledge reasoning"
    ],
    "limitations": "N/A",
    "out_of_scope_uses": []
  },
  "data": {
    "source": "Simulated environments including WebShop and HotPotQA.",
    "size": "900 tasks for WebShop, 300 questions for HotPotQA",
    "format": "N/A",
    "annotation": "N/A"
  },
  "methodology": {
    "methods": [
      "Human evaluation",
      "Automated metrics"
    ],
    "metrics": [
      "F1 Score",
      "Average reward score",
      "Recall performance"
    ],
    "calculation": "Metrics based on the reward score defined as the attribute overlapping ratio and F1 score grading.",
    "interpretation": "Higher average reward scores and recall values indicate better performance of the LAAs.",
    "baseline_results": null,
    "validation": "Performed through extensive experiments in both decision-making and multi-step reasoning environments."
  },
  "targeted_risks": {
    "risk_categories": [
      "Fairness",
      "Accuracy",
      "Robustness"
    ],
    "atlas_risks": {
      "risks": [
        {
          "category": "Fairness",
          "subcategory": [
            "Data bias"
          ]
        },
        {
          "category": "Accuracy",
          "subcategory": [
            "Unrepresentative data"
          ]
        },
        {
          "category": "Robustness",
          "subcategory": [
            "Data poisoning"
          ]
        }
      ]
    },
    "demographic_analysis": "N/A",
    "harm": "N/A"
  },
  "ethical_and_legal_considerations": {
    "privacy_and_anonymity": "N/A",
    "data_licensing": "N/A",
    "consent_procedures": "N/A",
    "compliance_with_regulations": "N/A"
  }
}