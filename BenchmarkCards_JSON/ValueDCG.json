{
  "benchmark_details": {
    "is_benchmark": true,
    "name": "ValueDCG (Value Discriminator-Critique Gap)",
    "abbreviation": "ValueDCG",
    "overview": "ValueDCG is a comprehensive evaluation metric designed to quantitatively assess the understanding of human values by language models, focusing on both the ability to identify values ('know what') and to explain them ('know why').",
    "data_type": "text",
    "domains": [
      "Natural Language Processing"
    ],
    "languages": [
      "English"
    ],
    "similar_benchmarks": [
      "ValueNet",
      "ETHICS"
    ],
    "resources": [
      "https://github.com/zowiezhang/ValueDCG"
    ]
  },
  "purpose_and_intended_users": {
    "goal": "To evaluate the understanding of human values by large language models in a comprehensive manner that considers both identification and explanation capabilities.",
    "audience": [
      "ML Researchers",
      "Model Developers"
    ],
    "tasks": [
      "Value Understanding Assessment"
    ],
    "limitations": "N/A",
    "out_of_scope_uses": []
  },
  "data": {
    "source": "Datasets include ValueNet and ETHICS, which provide human-annotated labels for different values.",
    "size": "N/A",
    "format": "N/A",
    "annotation": "Human annotation by trained evaluators."
  },
  "methodology": {
    "methods": [
      "Automated metrics",
      "Human evaluation"
    ],
    "metrics": [
      "ValueDCG"
    ],
    "calculation": "ValueDCG is calculated by comparing the outputs from language models on 'know what' and 'know why' assessments.",
    "interpretation": "A lower ValueDCG score indicates a better understanding of human values.",
    "baseline_results": "N/A",
    "validation": "Evaluation is based on multiple datasets with human labeling compared against model outputs."
  },
  "targeted_risks": {
    "risk_categories": [
      "Accuracy",
      "Fairness",
      "Safety"
    ],
    "atlas_risks": {
      "risks": [
        {
          "category": "Accuracy",
          "subcategory": [
            "Unrepresentative data"
          ]
        },
        {
          "category": "Fairness",
          "subcategory": [
            "Data bias"
          ]
        },
        {
          "category": "Societal Impact",
          "subcategory": [
            "Impact on affected communities"
          ]
        }
      ]
    },
    "demographic_analysis": "N/A",
    "harm": "N/A"
  },
  "ethical_and_legal_considerations": {
    "privacy_and_anonymity": "N/A",
    "data_licensing": "N/A",
    "consent_procedures": "N/A",
    "compliance_with_regulations": "N/A"
  }
}