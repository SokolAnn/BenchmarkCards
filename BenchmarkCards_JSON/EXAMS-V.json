{
  "benchmark_details": {
    "is_benchmark": true,
    "name": "EXAMS-V (A Multi-Discipline Multilingual Multimodal Exam Benchmark for Evaluating Vision Language Models)",
    "abbreviation": "EXAMS-V",
    "overview": "EXAMS-V is a new challenging multi-discipline multimodal multilingual exam benchmark for evaluating vision language models. It consists of 20,932 multiple-choice questions across 20 school disciplines covering natural science, social science, and other miscellaneous studies. The dataset emphasizes intricate reasoning across diverse languages and relies on region-specific knowledge.",
    "data_type": "question-answering pairs",
    "domains": [
      "Natural Language Processing",
      "Computer Vision",
      "Education"
    ],
    "languages": [
      "Arabic",
      "Bulgarian",
      "Chinese",
      "Croatian",
      "French",
      "German",
      "Hungarian",
      "Italian",
      "Polish",
      "Serbian",
      "English"
    ],
    "similar_benchmarks": [
      "ScienceQA",
      "MMMU",
      "M3Exam"
    ],
    "resources": [
      "https://github.com/RocktimJyotiDas/EXAMS-V"
    ]
  },
  "purpose_and_intended_users": {
    "goal": "To set a new standard in evaluating vision language models through a challenging benchmark that mirrors the complexity and diversity of real-world information processing.",
    "audience": [
      "ML Researchers",
      "Model Developers",
      "Education Researchers"
    ],
    "tasks": [
      "Visual Question Answering"
    ],
    "limitations": "N/A - Not discussed",
    "out_of_scope_uses": []
  },
  "data": {
    "source": "Official state examinations gathered from various countries' ministries of education.",
    "size": "20,932 questions",
    "format": "JSON",
    "annotation": "Manually annotated by authors with specific guidelines for ensuring high quality."
  },
  "methodology": {
    "methods": [
      "Evaluation of multiple-choice questions by measuring accuracy."
    ],
    "metrics": [
      "Accuracy"
    ],
    "calculation": "Calculated based on the correct answer out of all multiple-choice options.",
    "interpretation": "Higher accuracy indicates better performance of the vision language model.",
    "baseline_results": null,
    "validation": "The dataset has been validated by assessing clarity and correctness of visualization."
  },
  "targeted_risks": {
    "risk_categories": [
      "Bias",
      "Safety",
      "Fairness",
      "Accuracy"
    ],
    "atlas_risks": {
      "risks": []
    },
    "demographic_analysis": null,
    "harm": []
  },
  "ethical_and_legal_considerations": {
    "privacy_and_anonymity": "All data collected from public sources, with no privacy issues.",
    "data_licensing": "N/A - Not specified",
    "consent_procedures": "N/A - Not discussed",
    "compliance_with_regulations": "N/A - Not discussed"
  }
}