{
  "benchmark_details": {
    "name": "Hal-Eval",
    "overview": "A universal and fine-grained hallucination evaluation framework for Large Vision Language Models (LVLMs), providing a comprehensive assessment of various hallucination types, including unique categories such as event hallucinations.",
    "data_type": "Image-caption pairs",
    "domains": [
      "Computer Vision",
      "Natural Language Processing"
    ],
    "languages": null,
    "similar_benchmarks": [
      "POPE",
      "NOPE",
      "CIEM",
      "AMBER"
    ],
    "resources": [
      "https://doi.org/10.1145/3664647.3680576"
    ]
  },
  "purpose_and_intended_users": {
    "goal": "To provide a comprehensive evaluation of hallucinations in LVLMs and refine their outputs.",
    "audience": [
      "Researchers in AI",
      "Developers of LVLMs",
      "Academics focused on NLP and CV"
    ],
    "tasks": [
      "Evaluate hallucinations in LVLM outputs",
      "Analyze model performance based on hallucination types",
      "Fine-tune models using annotated data"
    ],
    "limitations": "Only evaluates hallucinations, other evaluation aspects such as accuracy and feasibility of image-text alignment are not covered.",
    "out_of_scope_uses": [
      "General NLP tasks not involving hallucinations",
      "Processing unrelated visual input"
    ]
  },
  "data": {
    "source": "Hal-Data created from COCO, Conceptual Captions, SBU, and web-sourced images.",
    "size": "130K for Hal-Data 130k; 2M for Hal-Data 2M",
    "format": "Image-text pairs with annotated hallucinations",
    "annotation": "Annotated using a fine-grained hallucination annotation pipeline with GPT-4."
  },
  "methodology": {
    "methods": [
      "Discriminative evaluation with uniform question templates",
      "Generative evaluation using the Hal-Evaluator"
    ],
    "metrics": [
      "Accuracy",
      "Precision",
      "Recall",
      "F1 Score"
    ],
    "calculation": "Calculated metrics based on predictions of hallucinations against ground-truth annotations.",
    "interpretation": "Metrics provide insight into the frequency and types of hallucinations present in model outputs.",
    "baseline_results": null,
    "validation": "Validated against human judgments for accuracy of hallucination detection."
  },
  "targeted_risks": {
    "risk_categories": [
      "Fairness",
      "Explainability",
      "Robustness",
      "Privacy"
    ],
    "atlas_risks": {
      "risks": [
        {
          "category": "Transparency",
          "subcategory": [
            "Lack of training data transparency",
            "Uncertain data provenance"
          ]
        },
        {
          "category": "Fairness",
          "subcategory": [
            "Data bias"
          ]
        }
      ]
    },
    "demographic_analysis": "N/A",
    "harm": "Potential misuse in generating misleading or incorrect captions, adversely affecting the reliability of LVLMs."
  },
  "ethical_and_legal_considerations": {
    "privacy_and_anonymity": "Publicly available datasets such as COCO were used; participant consent for data usage was obtained.",
    "data_licensing": "Data is sourced from open datasets under typical usage licenses.",
    "consent_procedures": "All generated content data used for training was annotated by team members, ensuring compliance.",
    "compliance_with_regulations": "N/A"
  }
}