{
  "benchmark_details": {
    "is_benchmark": true,
    "name": "Rank-Allocational-Based Bias Index (RABBI)",
    "abbreviation": "RABBI",
    "overview": "RABBI is a model-agnostic bias measure that assesses potential allocational harms arising from biases in LLM predictions. It compares bias metrics on allocation decision tasks such as resume screening and essay grading, demonstrating its predictive validity across ten LLMs.",
    "data_type": "ranking scores",
    "domains": [
      "Natural Language Processing",
      "Education",
      "Finance"
    ],
    "languages": [
      "English"
    ],
    "similar_benchmarks": [],
    "resources": []
  },
  "purpose_and_intended_users": {
    "goal": "To provide a reliable method for evaluating bias risks associated with algorithmic decision-making in resource allocation contexts.",
    "audience": [
      "ML Researchers",
      "Industry Practitioners",
      "Model Developers"
    ],
    "tasks": [
      "Resource Allocation",
      "Bias Measurement"
    ],
    "limitations": "N/A",
    "out_of_scope_uses": []
  },
  "data": {
    "source": "Constructed datasets based on resume templates from real job positions and essays from the International Corpus Network of Asian Learners of English (ICNALE).",
    "size": "5,600 essays and multiple resume templates for various job positions.",
    "format": "N/A",
    "annotation": "N/A"
  },
  "methodology": {
    "methods": [
      "Predictive Validity Evaluation",
      "Compare Bias Metrics"
    ],
    "metrics": [
      "Correlation with Allocation Outcomes"
    ],
    "calculation": "Metrics compared against demographic parity and equal opportunity fairness criteria.",
    "interpretation": "Higher scores indicate stronger correlation with allocation disparities.",
    "baseline_results": "Comparison with average performance gap metrics and distribution-based metrics.",
    "validation": "Extensive evaluation across ten LLMs."
  },
  "targeted_risks": {
    "risk_categories": [
      "Bias",
      "Accuracy"
    ],
    "atlas_risks": {
      "risks": []
    },
    "demographic_analysis": "Analysis includes gender and racial groups.",
    "harm": []
  },
  "ethical_and_legal_considerations": {
    "privacy_and_anonymity": "N/A",
    "data_licensing": "N/A",
    "consent_procedures": "N/A",
    "compliance_with_regulations": "N/A"
  }
}