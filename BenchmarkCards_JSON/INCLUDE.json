{
  "benchmark_details": {
    "is_benchmark": true,
    "name": "INCLUDE (Evaluating Multilingual Language Understanding with Regional Knowledge)",
    "abbreviation": "INCLUDE",
    "overview": "INCLUDE is a comprehensive knowledge- and reasoning-centric benchmark across 44 written languages that evaluates multilingual LLMs for performance in the actual language environments where they would be deployed. It comprises 197,243 QA pairs collected from local exams and captures regional and cultural knowledge.",
    "data_type": "question-answering pairs",
    "domains": [
      "Natural Language Processing",
      "Education"
    ],
    "languages": [
      "Albanian",
      "Arabic",
      "Armenian",
      "Azerbaijani",
      "Basque",
      "Belarusian",
      "Bengali",
      "Bulgarian",
      "Chinese",
      "Croatian",
      "Czech",
      "Danish",
      "Dutch",
      "Estonian",
      "Finnish",
      "French",
      "Georgian",
      "German",
      "Greek",
      "Hebrew",
      "Hindi",
      "Hungarian",
      "Indonesian",
      "Italian",
      "Japanese",
      "Kazakh",
      "Korean",
      "Lithuanian",
      "Malay",
      "Malayalam",
      "Nepali",
      "Macedonian",
      "Persian",
      "Polish",
      "Portuguese",
      "Russian",
      "Serbian",
      "Spanish",
      "Tagalog",
      "Tamil",
      "Telugu",
      "Turkish",
      "Ukrainian",
      "Urdu",
      "Uzbek",
      "Vietnamese"
    ],
    "similar_benchmarks": [
      "MMLU",
      "ArabicMMLU",
      "ChineseMMLU",
      "TurkishMMLU",
      "VNHSGE",
      "EXAMS"
    ],
    "resources": [
      "https://huggingface.co/datasets/CohereForAI/include-base-44"
    ]
  },
  "purpose_and_intended_users": {
    "goal": "To assess the performance of large language models across a wide range of subjects and languages, focusing on cultural and regional knowledge.",
    "audience": [
      "ML Researchers",
      "AI Developers",
      "Educational Institutions"
    ],
    "tasks": [
      "Question Answering"
    ],
    "limitations": "N/A",
    "out_of_scope_uses": []
  },
  "data": {
    "source": "Collected from local exams and assessments across various educational contexts and regions.",
    "size": "197,243 question-answering pairs",
    "format": "N/A",
    "annotation": "Manually verified by native speakers and annotators."
  },
  "methodology": {
    "methods": [
      "Evaluation using state-of-the-art LLMs",
      "Cross-linguistic performance comparisons"
    ],
    "metrics": [
      "Accuracy"
    ],
    "calculation": "Metrics are calculated based on the overall correct responses to the question-answering pairs.",
    "interpretation": "High scores indicate better performance in understanding regional knowledge across multiple languages.",
    "baseline_results": "Evaluated against the performance of notable LLMs such as GPT-4o and others.",
    "validation": "N/A"
  },
  "targeted_risks": {
    "risk_categories": [
      "Fairness",
      "Accuracy"
    ],
    "atlas_risks": {
      "risks": [
        {
          "category": "Fairness",
          "subcategory": [
            "Data bias"
          ]
        },
        {
          "category": "Accuracy",
          "subcategory": [
            "Poor model accuracy"
          ]
        }
      ]
    },
    "demographic_analysis": "N/A",
    "harm": []
  },
  "ethical_and_legal_considerations": {
    "privacy_and_anonymity": "N/A",
    "data_licensing": "N/A",
    "consent_procedures": "N/A",
    "compliance_with_regulations": "N/A"
  }
}