{
  "benchmark_details": {
    "is_benchmark": true,
    "name": "PlotQA: Reasoning over Scientific Plots",
    "abbreviation": "PlotQA",
    "overview": "Specifically, we propose PlotQA with 28.9 million question-answer pairs over 224,377 plots on data from real-world sources and questions based on crowd-sourced question templates.",
    "data_type": "question-answer pairs over plot images",
    "domains": [
      "Computer Vision",
      "Natural Language Processing"
    ],
    "languages": [
      "English"
    ],
    "similar_benchmarks": [
      "FigureQA",
      "DVQA",
      "VQA",
      "CLEVR",
      "TextVQA",
      "FigureSeer"
    ],
    "resources": [
      "https://bit.ly/PlotQA",
      "https://arxiv.org/abs/1909.00997",
      "https://github.com/tesseract-ocr/tesseract"
    ]
  },
  "purpose_and_intended_users": {
    "goal": "To reduce the gap between existing synthetic plot datasets and real-world plots and question templates by proposing PlotQA, a large dataset that exposes the need to train models for questions that have answers from an Open Vocabulary.",
    "audience": [],
    "tasks": [
      "Question Answering",
      "Visual Question Answering",
      "Optical Character Recognition",
      "Object Detection",
      "Table Question Answering",
      "Numerical Reasoning"
    ],
    "limitations": "N/A",
    "out_of_scope_uses": []
  },
  "data": {
    "source": "Data sourced from World Bank Open Data, Open Government Data, Global Terrorism Database, and similar online data sources (crawled indicator variables and entities).",
    "size": "224,377 plots; 28,952,641 question-answer pairs",
    "format": "N/A",
    "annotation": "Bounding box annotations for legend boxes, legend names, legend markers, axes titles, axes ticks, bars, lines, and title. Questions generated from 74 templates extracted from 7,000 crowd-sourced questions (paraphrased and instantiated)."
  },
  "methodology": {
    "methods": [
      "Automated metrics (Accuracy, mAP, F1-score, IOU)",
      "Human evaluation (human accuracy measured on subset)"
    ],
    "metrics": [
      "Accuracy",
      "Mean Average Precision (mAP)",
      "F1 Score",
      "Intersection over Union (IOU)"
    ],
    "calculation": "Textual answers: exact match. Numeric (floating point) answers: considered correct if within 5% of the correct answer. VED evaluated using mAP at IOU thresholds (0.5, 0.75, 0.9). SIE evaluated using F1-score on extracted tuples. OCR evaluated in oracle and pipeline modes.",
    "interpretation": "Higher Accuracy indicates better model performance; human accuracy on a subset is 80.47% (reference upper-bound). The dataset is challenging as state-of-the-art models achieve substantially lower accuracy than humans.",
    "baseline_results": "On PlotQA (accuracy): IMG-only 4.84%, QUES-only 5.35%, BAN 0.01%, LoRRA 0.02%, SAN 7.76%, Our Model 22.52%. On DVQA: SAN 32.1%, SANDY-OCR 45.77% (reported), Our Model 57.99% (DVQA test).",
    "validation": "Train/Validation/Test splits: 70%/15%/15% on 224,377 plots (Train: 157,070 images, 20,249,479 QA pairs; Validation: 33,650 images, 4,360,648 QA pairs; Test: 33,657 images, 4,342,514 QA pairs). Human evaluation performed on 5,860 questions across 160 images (human accuracy 80.47%). Module-level evaluation includes oracle vs pipeline OCR, VED measured at multiple IOU thresholds, and SIE evaluated via F1 on tuples."
  },
  "targeted_risks": {
    "risk_categories": [
      "Accuracy",
      "Robustness"
    ],
    "atlas_risks": {
      "risks": [
        {
          "category": "Accuracy",
          "subcategory": [
            "Poor model accuracy"
          ]
        },
        {
          "category": "Robustness",
          "subcategory": [
            "Hallucination"
          ]
        }
      ]
    },
    "demographic_analysis": null,
    "harm": null
  },
  "ethical_and_legal_considerations": {
    "privacy_and_anonymity": "N/A",
    "data_licensing": "N/A",
    "consent_procedures": "N/A",
    "compliance_with_regulations": "N/A"
  }
}