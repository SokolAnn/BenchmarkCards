{
  "benchmark_details": {
    "is_benchmark": true,
    "name": "SimpleQuestions",
    "abbreviation": "N/A",
    "overview": "A large-scale dataset of 108,442 natural-language questions written by human English-speaking annotators, each paired with a corresponding Freebase (FB2M) fact that provides the answer and explains it. Introduced to study coverage of existing QA systems and the impact of multitask and transfer learning for simple question answering.",
    "data_type": "question-answering pairs (questions paired with Freebase facts)",
    "domains": [
      "Natural Language Processing"
    ],
    "languages": [
      "English"
    ],
    "similar_benchmarks": [
      "WebQuestions",
      "Reverb"
    ],
    "resources": [
      "http://fb.ai/babi"
    ]
  },
  "purpose_and_intended_users": {
    "goal": "Provide a large-scale dataset for the task of simple question answering to study coverage of QA systems and to investigate multitask and transfer learning for simple QA.",
    "audience": [
      "ML Researchers",
      "Model Developers"
    ],
    "tasks": [
      "Question Answering",
      "Knowledge Base Question Answering"
    ],
    "limitations": "N/A",
    "out_of_scope_uses": []
  },
  "data": {
    "source": "Questions written by human English-speaking annotators paired with facts from FB2M (an extract of Freebase).",
    "size": "108,442 questions (examples). Split: 75,910 training; 10,845 validation; 21,687 test.",
    "format": "N/A",
    "annotation": "Human annotation: annotators were shown selected Freebase facts (from FB2M) and asked to phrase a natural-language question involving the subject and relationship, with the object as the answer."
  },
  "methodology": {
    "methods": [
      "Automated metrics (path-level accuracy)",
      "Automated metrics (F1 Score)"
    ],
    "metrics": [
      "Path-level Accuracy",
      "F1 Score",
      "Accuracy"
    ],
    "calculation": "For WebQuestions: F1-score as defined in Berant and Liang (2014) — the average, over all test questions, of the F1-score of the sets of predicted answers. For SimpleQuestions: path-level accuracy where a prediction is correct if the subject and the relationship were correctly retrieved. For Reverb: accuracy defined as the percentage of questions for which the top-ranked candidate fact is correct (re-ranking task).",
    "interpretation": "High path-level accuracy indicates correct retrieval of subject and relationship; for WebQuestions F1 reflects overlap between predicted and gold answer sets. The paper notes that on SimpleQuestions the supporting fact is in the candidate set for about 86% of questions, indicating the task remains challenging and current models are effective at re-ranking but do not fully solve simple QA.",
    "baseline_results": "Baseline results reported in the paper (test sets): Random guess: WebQuestions F1 1.9%; SimpleQuestions accuracy 4.9%; Reverb accuracy 35%. Berant et al. (2013): WebQuestions F1 31.3%. Bordes et al. (2014b): WebQuestions F1 29.7%; Reverb accuracy 73%. Bordes et al. (2014a) – using path: WebQuestions F1 35.3%; using path+subgraph: 39.2%. Berant and Liang (2014): WebQuestions F1 39.9%. Yang et al. (2014): WebQuestions F1 41.3%. (See Table 4 in paper for full comparative results and Memory Networks variants.)",
    "validation": "Hyperparameters chosen to maximize F1 on the WebQuestions validation set; early stopping performed based on validation F1. The dataset split used for SimpleQuestions is 70% train / 10% validation / 20% test. Training used Adagrad with WARP loss and multi-threaded HogWild!; training time reported as 2-3 hours on 20 threads."
  },
  "targeted_risks": {
    "risk_categories": [],
    "atlas_risks": {
      "risks": null
    },
    "demographic_analysis": null,
    "harm": null
  },
  "ethical_and_legal_considerations": {
    "privacy_and_anonymity": "N/A",
    "data_licensing": "N/A",
    "consent_procedures": "N/A",
    "compliance_with_regulations": "N/A"
  }
}