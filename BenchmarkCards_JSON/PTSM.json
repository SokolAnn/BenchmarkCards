{
  "benchmark_details": {
    "is_benchmark": true,
    "name": "PTSM (Persistent Twitter dataset for social meaning)",
    "abbreviation": "PTSM",
    "overview": "A persistent English Twitter dataset for social meaning (PTSM) that comprises paraphrase-based versions of 17 existing social-meaning Twitter datasets across 10 task categories. The authors train a T5-based paraphrasing model on diverse paraphrase corpora and use it to generate paraphrases of training samples so that models can be trained without requiring original tweet text, mitigating data decay and accessibility issues.",
    "data_type": "text (tweet classification instances and paraphrase pairs)",
    "domains": [
      "Natural Language Processing",
      "Social Media"
    ],
    "languages": [
      "English"
    ],
    "similar_benchmarks": [
      "GLUE",
      "SuperGLUE",
      "TweetEval"
    ],
    "resources": [
      "https://github.com/chiyuzhang94/PTSM",
      "https://developer.twitter.com/",
      "https://huggingface.co/models"
    ]
  },
  "purpose_and_intended_users": {
    "goal": "Alleviate data decay and accessibility issues for Twitter-based social meaning datasets by providing persistent paraphrase-based training data that enable training of social-meaning classifiers without access to original tweets.",
    "audience": [],
    "tasks": [
      "Crisis awareness",
      "Emotion recognition",
      "Hateful and offensive language detection",
      "Humor detection",
      "Irony detection",
      "Sarcasm detection",
      "Offensive language detection",
      "Sentiment Analysis",
      "Stance detection"
    ],
    "limitations": "The paraphrasing model (T5) cannot generate emojis due to closed vocabulary; similarity selection uses a simple tri-gram similarity method which may be suboptimal; annotation bias exists in some original datasets (e.g., Hate Waseem).",
    "out_of_scope_uses": []
  },
  "data": {
    "source": "Original tweets collected via the Twitter API from 17 existing social-meaning datasets: Crisis Oltea; Emo Moham; Hate Bas; Hate Waseem; Hate David; Humor Potash; Humor Meaney; Irony Hee-A; Irony Hee-B; Offense Zamp; Sarc Riloff; Sarc Ptacek; Sarc Rajad; Sarc Bam; Senti Rosen; Senti Thel; Stance Moham. Paraphrases are generated by a T5 Base model fine-tuned on PIT-2015, LanguageNet, Opusparcus, and Quora Question Pairs.",
    "size": "Paraphrase corpora for paraphraser training: 625,097 sentence pairs. PTSM provides per-task paraphrase Train sets (Para1, Para2, Para4, Para5) with sizes reported per task in Table 4 (e.g., Crisis Oltea Para1: 48.0K; Emo Moham Para1: 3.3K; Sarc Ptacek Para1: 71.4K; Senti Thel Para1: 900).",
    "format": "N/A",
    "annotation": "Uses original dataset labels from the cited social-meaning datasets. Paraphrases are generated automatically by the paraphrase model; no additional manual annotation of paraphrases is reported."
  },
  "methodology": {
    "methods": [
      "Fine-tuning pre-trained language models (RoBERTa Base, BERTweet Base) on paraphrase-based Train sets",
      "Automated metrics evaluation (BLEU for paraphraser, macro-averaged F1 for downstream tasks)",
      "Model selection on Dev sets and evaluation on blind Test sets",
      "Three random-seed runs with averaged results"
    ],
    "metrics": [
      "Macro-averaged F1",
      "BLEU Score"
    ],
    "calculation": "Downstream results are reported as the average Test macro-averaged F1 over three runs (the best model per task is selected on the Dev set). Paraphrasing model selection is based on BLEU score on the Dev set (best model 28.18 BLEU reported).",
    "interpretation": "Performance of models trained on paraphrase-based Train sets is close to models trained on original gold Train sets; paraphrase-based models incur marginal performance loss (e.g., RoBERTa-P2 is 1.66 macro F1 lower than RoBERTa-Gold on average; BERTweet-P2 is 1.87 macro F1 lower than BERTweet-Gold).",
    "baseline_results": "RoBERTa-Gold average macro-F1: 75.02; RoBERTa-P2 average macro-F1: 73.36 (1.66 lower). BERTweet-Gold average macro-F1: 76.80; BERTweet-P2 average macro-F1: 74.93 (1.87 lower).",
    "validation": "Models are validated by selecting the best checkpoint on each task's Dev set; downstream results are averaged over three runs with different random seeds; early stopping with patience=5 used during fine-tuning."
  },
  "targeted_risks": {
    "risk_categories": [
      "Privacy",
      "Fairness",
      "Data Laws"
    ],
    "atlas_risks": {
      "risks": [
        {
          "category": "Privacy",
          "subcategory": [
            "Personal information in data"
          ]
        },
        {
          "category": "Fairness",
          "subcategory": [
            "Data bias"
          ]
        },
        {
          "category": "Data Laws",
          "subcategory": [
            "Data usage restrictions"
          ]
        }
      ]
    },
    "demographic_analysis": "N/A",
    "harm": [
      "Privacy harm reduction by avoiding sharing original tweet text and protecting user identity via anonymization and paraphrasing"
    ]
  },
  "ethical_and_legal_considerations": {
    "privacy_and_anonymity": "Usernames and hyperlinks are normalized/replaced with tokens 'USER' and 'URL'; seed hashtags removed when appropriate; paraphrased dataset does not contain actual usernames or hyperlinks. Authors state data are anonymous and follow Twitter distribution policy.",
    "data_licensing": "N/A",
    "consent_procedures": "N/A",
    "compliance_with_regulations": "Data collection and redistribution follow Twitter distribution policy; authors state all data used for model training are anonymous."
  }
}