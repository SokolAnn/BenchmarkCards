{
  "benchmark_details": {
    "is_benchmark": true,
    "name": "SAFEWORLD",
    "abbreviation": "N/A",
    "overview": "SAFEWORLD is a benchmark specifically designed to evaluate LLMs' ability to generate responses that are culturally sensitive and legally compliant across diverse global contexts, comprising 2,342 test user queries grounded in cultural norms and legal policies from 50 countries.",
    "data_type": "text",
    "domains": [
      "Natural Language Processing"
    ],
    "languages": [
      "English"
    ],
    "similar_benchmarks": [
      "Safer-Instruct",
      "BeaverTail",
      "ToxicChat"
    ],
    "resources": [
      "https://github.com/PlusLabNLP/SafeWorld"
    ]
  },
  "purpose_and_intended_users": {
    "goal": "To evaluate LLMs' responses to geo-diverse safety topics and enhance their alignment with cultural norms and legal standards globally.",
    "audience": [
      "ML Researchers",
      "Industry Practitioners",
      "Model Developers"
    ],
    "tasks": [
      "Text Classification"
    ],
    "limitations": "N/A",
    "out_of_scope_uses": []
  },
  "data": {
    "source": "Generated from cultural-legal guidelines across 50 countries and 493 regions/races, validated through machine and human evaluations.",
    "size": "2,342 examples",
    "format": "JSON",
    "annotation": "Human-verified with cultural norms and legal policies."
  },
  "methodology": {
    "methods": [
      "Multi-dimensional evaluation framework",
      "Human evaluation",
      "Automated metrics"
    ],
    "metrics": [
      "Accuracy",
      "Faithfulness",
      "Coverage"
    ],
    "calculation": "Metrics include evaluating response type matching, faithfulness to ground-truth guidelines, and comprehensive coverage of relevant norms.",
    "interpretation": "Higher scores indicate better alignment and appropriateness of responses concerning cultural and legal contexts.",
    "baseline_results": "Outperforms leading models like GPT-4o across multiple dimensions.",
    "validation": "Multi-round validation involving both machine and human evaluators."
  },
  "targeted_risks": {
    "risk_categories": [
      "Fairness",
      "Safety",
      "Accuracy"
    ],
    "atlas_risks": {
      "risks": [
        {
          "category": "Fairness",
          "subcategory": [
            "Data bias"
          ]
        },
        {
          "category": "Accuracy",
          "subcategory": [
            "Poor model accuracy"
          ]
        }
      ]
    },
    "demographic_analysis": "N/A",
    "harm": []
  },
  "ethical_and_legal_considerations": {
    "privacy_and_anonymity": "N/A",
    "data_licensing": "N/A",
    "consent_procedures": "N/A",
    "compliance_with_regulations": "N/A"
  }
}