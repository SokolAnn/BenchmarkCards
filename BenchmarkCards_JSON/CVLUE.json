{
  "benchmark_details": {
    "is_benchmark": true,
    "name": "CVLUE (Chinese Vision-Language Understanding Evaluation)",
    "abbreviation": "CVLUE",
    "overview": "CVLUE is a vision-language understanding benchmark dataset specifically designed for the comprehensive evaluation of vision-language models in Chinese VLU. Its images are representative of Chinese culture, and it includes four distinct tasks: image-text retrieval, visual question answering, visual grounding, and visual dialogue.",
    "data_type": "image-text pairs",
    "domains": [
      "Natural Language Processing",
      "Computer Vision"
    ],
    "languages": [
      "Chinese"
    ],
    "similar_benchmarks": [
      "VLUE",
      "CLiMB",
      "MUGE",
      "Flickr30K-CN",
      "COCO-CN"
    ],
    "resources": [
      "https://github.com/WangYuxuan93/CVLUE"
    ]
  },
  "purpose_and_intended_users": {
    "goal": "The primary objective of CVLUE is to provide a fair and comprehensive evaluation platform for evaluating vision-language models in the context of Chinese culture.",
    "audience": [
      "ML Researchers",
      "Industry Practitioners",
      "Model Developers",
      "Domain Experts"
    ],
    "tasks": [
      "Image-Text Retrieval",
      "Visual Question Answering",
      "Visual Grounding",
      "Visual Dialogue"
    ],
    "limitations": "N/A",
    "out_of_scope_uses": []
  },
  "data": {
    "source": "Images were manually collected from the Chinese Internet, ensuring representation of Chinese culture.",
    "size": "17,920 training examples, 3,116 validation examples, 8,973 test examples for Image-Text Retrieval; 14,362 training examples, 2,571 validation examples, 7,169 test examples for Visual Question Answering; 10,769 training examples, 1,965 validation examples, 5,385 test examples for Visual Grounding; 3,975 training examples, 651 validation examples, 2,036 test examples for Visual Dialogue.",
    "format": "N/A",
    "annotation": "Images were annotated through a combination of expert and crowd-sourced methods, including manual labeling and peer reviews."
  },
  "methodology": {
    "methods": [
      "Human evaluation"
    ],
    "metrics": [
      "Accuracy",
      "Recall at k (R@k)",
      "Intersection over Union (IoU)"
    ],
    "calculation": "Metrics are calculated based on the specific evaluation tasks defined in CVLUE.",
    "interpretation": "Higher scores indicate better performance in understanding and generating responses regarding Chinese visual-language tasks.",
    "baseline_results": "N/A",
    "validation": "N/A"
  },
  "targeted_risks": {
    "risk_categories": [
      "Bias",
      "Cultural representation"
    ],
    "atlas_risks": {
      "risks": [
        {
          "category": "Fairness",
          "subcategory": [
            "Data bias"
          ]
        },
        {
          "category": "Accuracy",
          "subcategory": [
            "Unrepresentative data"
          ]
        }
      ]
    },
    "demographic_analysis": "N/A",
    "harm": []
  },
  "ethical_and_legal_considerations": {
    "privacy_and_anonymity": "Images collected from the Chinese Internet are visually unobtrusive, with sensitive information obscured.",
    "data_licensing": "The proposed dataset will be made publicly available for research purposes under the CC BY-NC-ND 4.0 license.",
    "consent_procedures": "All annotators have given informed consent and are compensated.",
    "compliance_with_regulations": "N/A"
  }
}