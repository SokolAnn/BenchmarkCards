{
  "benchmark_details": {
    "is_benchmark": true,
    "name": "ArguGPT",
    "abbreviation": "ArguGPT",
    "overview": "ArguGPT is a carefully balanced corpus of argumentative essays consisting of 4,038 machine-generated essays (from seven GPT-family models) paired with roughly equal number of human-written essays drawn from in-class/homework exercises (WECCL), TOEFL writing tasks (TOEFL11), and GRE issue tasks. The corpus includes an out-of-distribution test set (machine essays from non-GPT models and additional human essays) and is intended to enable human evaluation, linguistic analysis, and benchmarking of AIGC detectors.",
    "data_type": "text (argumentative essays)",
    "domains": [
      "Natural Language Processing",
      "Education"
    ],
    "languages": [
      "English"
    ],
    "similar_benchmarks": [
      "TuringBench",
      "TOEFL11",
      "WECCL"
    ],
    "resources": [
      "https://github.com/huhailinguist/ArguGPT",
      "https://huggingface.co/spaces/SJTU-CL/argugpt-detector",
      "https://arxiv.org/abs/2304.07666"
    ]
  },
  "purpose_and_intended_users": {
    "goal": "Provide a balanced corpus of human and machine-authored argumentative essays to (1) establish a baseline of ESOL instructors' ability to detect AI-generated essays, (2) analyze lexical and syntactic characteristics of machine-generated essays, and (3) build and benchmark automated AIGC detectors.",
    "audience": [
      "ESOL instructors",
      "Corpus linguists",
      "Computational linguists",
      "Natural Language Processing researchers",
      "AIGC detector developers"
    ],
    "tasks": [
      "Text Classification",
      "Authorship Attribution",
      "Linguistic Analysis"
    ],
    "limitations": null,
    "out_of_scope_uses": []
  },
  "data": {
    "source": "Human essays: WECCL (Written English Corpus of Chinese Learners), TOEFL11, and GRE sample essays collected from GRE-prep materials. Machine essays: generated in response to the same prompts using seven GPT-family models (gpt2-xl, text-babbage-001, text-curie-001, text-davinci-001, text-davinci-002, text-davinci-003, gpt-3.5-turbo). Out-of-distribution (OOD) machine essays generated by other models (gpt-3.5-turbo (additional), gpt-4, claude-instant, bloomz-7b, flan-t5-11b).",
    "size": "4,038 machine-generated essays; 4,115 human-written essays; total 8,153 essays (in-distribution, excluding OOD). Total tokens (w/o OOD): 2,449,790 tokens. OOD test set: 500 machine essays and 500 human essays.",
    "format": "N/A",
    "annotation": "Essays scored and labeled into three levels (low / medium / high) using automated scoring (YouDao). TOEFL11 human essays retain their original three-level labels from dataset owners. GRE human essays scored by YouDao. Each machine-generated essay is also assigned an automated score by YouDao."
  },
  "methodology": {
    "methods": [
      "Human evaluation (two-round Turing-style test with 43 ESL instructors)",
      "Automated classifier evaluation (fine-tuned RoBERTa-large, SVMs with linguistic features)",
      "Off-the-shelf detector evaluation (GPTZero, RoBERTa from Guo et al. 2023)",
      "Zero/few-shot evaluation with gpt-3.5-turbo",
      "Linguistic analyses using Lu (2010, 2012) syntactic and lexical complexity measures",
      "N-gram analysis with log-likelihood"
    ],
    "metrics": [
      "Accuracy",
      "Pearson correlation",
      "Log-likelihood"
    ],
    "calculation": "Accuracy is measured as the proportion of correct classifications on held-out test sets. For GPTZero, a returned probability > 0.65 is considered AI-written (per GPTZero documentation). Pearson correlation coefficients reported for agreement between automated scoring systems (YouDao vs Pigai) (e.g., Pearson r = 0.7570 overall in pilot). Log-likelihood is used to rank n-grams overused by machines vs humans.",
    "interpretation": "Higher Accuracy indicates better detection performance. Human instructors achieved 61.6% accuracy in round 1 and 67.7% after minimal self-training. High document-level accuracy (e.g., RoBERTa-large reported ~99% on in-distribution test) indicates strong in-domain detection; drops in OOD accuracy indicate limited generalization.",
    "baseline_results": "Human instructors: 61.6% accuracy (round 1), 67.7% (round 2). RoBERTa-large fine-tuned on ArguGPT: ~99% document-level accuracy and ~93% sentence-level accuracy (in-distribution). SVM with function-word features: 95.14% document-level accuracy (reported best SVM). GPTZero: reported 90+% accuracy on in-distribution document and sentence levels but poor generalization on some OOD models.",
    "validation": "Dataset split into train/dev/test (dev and test sets each contain 700 essays). Out-of-distribution test set of 500 machine and 500 human essays used to evaluate generalization. Additional evaluations at paragraph- and sentence-level splits."
  },
  "targeted_risks": {
    "risk_categories": [
      "Robustness",
      "Governance",
      "Intellectual Property",
      "Societal Impact",
      "Accuracy"
    ],
    "atlas_risks": {
      "risks": [
        {
          "category": "Governance",
          "subcategory": [
            "Lack of testing diversity",
            "Unrepresentative risk testing"
          ]
        },
        {
          "category": "Intellectual Property",
          "subcategory": [
            "Data usage rights restrictions"
          ]
        },
        {
          "category": "Societal Impact",
          "subcategory": [
            "Impact on education: plagiarism"
          ]
        }
      ]
    },
    "demographic_analysis": "N/A",
    "harm": [
      "Academic integrity violations (students submitting AI-generated essays as their own work; plagiarism/cheating)"
    ]
  },
  "ethical_and_legal_considerations": {
    "privacy_and_anonymity": "N/A",
    "data_licensing": "Human-authored essays are not publicly released due to copyright; only indices of human essays will be released. Machine-authored essays and models are released via GitHub (see resources).",
    "consent_procedures": "N/A",
    "compliance_with_regulations": "N/A"
  }
}