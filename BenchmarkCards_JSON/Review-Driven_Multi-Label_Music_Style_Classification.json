{
  "benchmark_details": {
    "is_benchmark": true,
    "name": "Review-Driven Multi-Label Music Style Classification",
    "abbreviation": "N/A",
    "overview": "This paper explores a new natural language processing task, review-driven multi-label music style classification, which requires the system to identify multiple styles of music based on its reviews on websites. The authors build a new dataset and propose a label-graph based neural network and a soft training mechanism to learn and exploit style correlations.",
    "data_type": "text (user reviews and album titles with multi-label style annotations)",
    "domains": [
      "Natural Language Processing",
      "Music Information Retrieval"
    ],
    "languages": [
      "Chinese"
    ],
    "similar_benchmarks": [],
    "resources": [
      "https://music.douban.com",
      "https://arxiv.org/abs/1808.07604"
    ]
  },
  "purpose_and_intended_users": {
    "goal": "To create a dataset and evaluate methods for review-driven multi-label music style classification: identifying multiple music styles from user reviews and exploiting style correlations.",
    "audience": [],
    "tasks": [
      "Multi-Label Classification",
      "Text Classification",
      "Music Style Classification"
    ],
    "limitations": "The method performs worse on styles with low frequency in the training set due to a highly imbalanced label distribution; difficulty distinguishing music styles that share many common elements and only subtly differ from each other; limited availability of audio data due to copyright.",
    "out_of_scope_uses": []
  },
  "data": {
    "source": "Collected from a popular Chinese music review website (https://music.douban.com). Each sample is defined as an album and includes the title, a set of human annotated styles, and associated user reviews sorted by time.",
    "size": "7,172 samples total; 5,020 training, 646 validation, 1,506 testing; over 287,000 reviews; over 3.6 million words; 22 style labels; each sample labeled with 2 to 5 styles; average 2.2 styles per sample; average 40 reviews per sample; average 12.4 words per review.",
    "format": "N/A",
    "annotation": "Human annotated styles (manual annotation)"
  },
  "methodology": {
    "methods": [
      "Automated metrics",
      "Model-based evaluation (comparison to baseline models)",
      "Visualization analysis"
    ],
    "metrics": [
      "Micro F1",
      "Macro F1",
      "One-Error",
      "Hamming Loss"
    ],
    "calculation": "Macro F1 computes the metric independently for each label and then takes the average; Micro F1 aggregates the contributions of all labels to compute the average. One-Error evaluates the fraction of examples whose top-ranked label is not in the gold label set. Hamming Loss counts the fraction of wrong labels to the total number of labels.",
    "interpretation": "Higher Micro F1 and Macro F1 indicate better performance. Lower One-Error and lower Hamming Loss indicate better performance. (Table legend: \"+\" indicates higher is better; \"-\" indicates lower is better.)",
    "baseline_results": "Baselines and key results (from Table 2): LSTM: One-Error 30.5, Hamming Loss 0.089, Macro F1 33.0, Micro F1 53.9. Proposed method (HAN + LCM): One-Error 22.6, Hamming Loss 0.074, Macro F1 54.4, Micro F1 64.5.",
    "validation": "A validation set of 646 samples was used to tune hyper-parameters and select the best parameters for test evaluation."
  },
  "targeted_risks": {
    "risk_categories": [
      "Bias",
      "Data acquisition"
    ],
    "atlas_risks": {
      "risks": [
        {
          "category": "Data Laws",
          "subcategory": [
            "Data acquisition restrictions"
          ]
        },
        {
          "category": "Fairness",
          "subcategory": [
            "Data bias"
          ]
        }
      ]
    },
    "demographic_analysis": null,
    "harm": null
  },
  "ethical_and_legal_considerations": {
    "privacy_and_anonymity": "N/A",
    "data_licensing": "N/A",
    "consent_procedures": "N/A",
    "compliance_with_regulations": "N/A"
  }
}