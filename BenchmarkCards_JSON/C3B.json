{
  "benchmark_details": {
    "is_benchmark": true,
    "name": "C3B (Comics Cross-Cultural Benchmark)",
    "abbreviation": "C3B",
    "overview": "C3B is a novel multicultural, multitask and multilingual cultural awareness capabilities benchmark composed of over 2,220 images and over 18,789 QA pairs, designed to evaluate MLLMs' cultural awareness through three progressively difficult tasks.",
    "data_type": "image-question-answering pairs",
    "domains": [
      "Natural Language Processing",
      "Computer Vision"
    ],
    "languages": [
      "Japanese",
      "Russian",
      "Thai",
      "English",
      "Spanish"
    ],
    "similar_benchmarks": [
      "CVQA",
      "CulturalVQA",
      "GIMMICK"
    ],
    "resources": [
      "https://c3b-benchmark.github.io/"
    ]
  },
  "purpose_and_intended_users": {
    "goal": "To advance the cultural awareness capabilities of Multimodal Large Language Models (MLLMs) through comprehensive evaluation.",
    "audience": [
      "ML Researchers",
      "Model Developers"
    ],
    "tasks": [
      "Visual Recognition",
      "Cultural Conflict Understanding",
      "Cultural Content Generation"
    ],
    "limitations": "N/A",
    "out_of_scope_uses": []
  },
  "data": {
    "source": "C3B consists of 2,220 images generated using comic generation technologies and manually selected from culture-representative comics.",
    "size": "2,220 images and 18,789 QA pairs",
    "format": "image and question-answer pair format",
    "annotation": "Manual QA pair creation and automated cultural conflict detection."
  },
  "methodology": {
    "methods": [
      "Automated metrics",
      "Human evaluation"
    ],
    "metrics": [
      "Accuracy",
      "BLEU Score",
      "COMET",
      "BLEURT"
    ],
    "calculation": "Specific weighting for metrics considering contributions of various tasks.",
    "interpretation": "Scores indicate the understanding of cultural contexts and conflicts by MLLMs, with results compared to human performance.",
    "baseline_results": "C3B evaluations show a significant performance gap between MLLMs and human evaluators.",
    "validation": "Evaluations were conducted across multiple open-source MLLMs."
  },
  "targeted_risks": {
    "risk_categories": [
      "Bias",
      "Fairness",
      "Accuracy"
    ],
    "atlas_risks": {
      "risks": []
    },
    "demographic_analysis": "N/A",
    "harm": []
  },
  "ethical_and_legal_considerations": {
    "privacy_and_anonymity": "N/A",
    "data_licensing": "N/A",
    "consent_procedures": "N/A",
    "compliance_with_regulations": "N/A"
  }
}