{
  "benchmark_details": {
    "is_benchmark": true,
    "name": "SuperCLUE: A Comprehensive Chinese Large Language Model Benchmark",
    "abbreviation": "SuperCLUE",
    "overview": "SuperCLUE encompasses three sub-tasks: actual users’ queries and ratings derived from an LLM battle platform (CArena), open-ended questions with single and multiple-turn dialogues (OPEN), and closed-ended questions with the same stems as open-ended single-turn ones (CLOSE). It is constructed to predict LLMs' performances on a diverse set of abilities in real Chinese scenarios and to study the relationship between closed-ended accuracy and human preferences.",
    "data_type": "question-answering pairs (open-ended single- and multi-turn dialogues), closed-ended multiple-choice questions, and user interaction/rating data",
    "domains": [
      "Natural Language Processing"
    ],
    "languages": [
      "Chinese"
    ],
    "similar_benchmarks": [
      "CLUE",
      "MMLU",
      "Big-Bench",
      "HELM",
      "MMCU",
      "AGIEval",
      "C-Eval",
      "MT-bench",
      "Chatbot Arena"
    ],
    "resources": [
      "https://www.CLUEbenchmarks.com"
    ]
  },
  "purpose_and_intended_users": {
    "goal": "Construct a benchmark that predicts LLMs’ performances on a diverse set of abilities in real Chinese scenarios and to analyze the relationship between closed-ended accuracy and human preferences.",
    "audience": [
      "Model Developers",
      "Community Users"
    ],
    "tasks": [
      "Question Answering",
      "Dialogue (Multi-turn)",
      "Instruction Following",
      "Multiple-choice Question Answering",
      "Human Preference Evaluation"
    ],
    "limitations": "Human evaluation is time-consuming and cost-intensive and thus hard to scale. Closed-ended multi-choice questions alone are insufficient to reflect human preferences in open interactive scenarios. Limited access prevented inclusion of some Chinese LLMs (e.g., Wenxin Yiyan, 360 Brain, SparkDesk) on the platform.",
    "out_of_scope_uses": []
  },
  "data": {
    "source": "CArena (LangYa Leaderboard model battle platform) providing user-model interactions with user-reported ratings; OPEN dataset of open-ended questions (OPEN SINGLE and OPEN MULTIPLE); CLOSE dataset derived from OPEN SINGLE by GPT-3.5 generation and human proofreading.",
    "size": "CArena: 9.9k queries with ratings; OPEN: 600 questions total (300 single-turn, 300 multi-turn); CLOSE: 300 closed-ended questions (one per OPEN single-turn question).",
    "format": "N/A",
    "annotation": "Initial manual annotation of 300 entries (30 per capability category); trained a BERT classifier on these annotations to label remaining data; final labels reviewed and corrected by four human evaluators; CLOSE items generated by GPT-3.5 and proofread/verified by humans in a three-stage review process (each stage with three human reviewers)."
  },
  "methodology": {
    "methods": [
      "Zero-shot evaluation",
      "Human evaluation (model battle platform CArena, user self-reported ratings)",
      "Automatic evaluation using GPT-4 as a judge (pairwise comparisons)",
      "Closed-ended accuracy evaluation"
    ],
    "metrics": [
      "Accuracy",
      "Average win rate (win and tie rate)",
      "Pearson correlation",
      "Spearman correlation"
    ],
    "calculation": "Closed-ended multi-choice: classification accuracy. Open-ended and CArena: average win rate (average of win and tie rates) against other models. OPEN automatic evaluation: pairwise comparisons judged by GPT-4. CArena uses user self-reported ratings from LangYa Leaderboard (Elo-based pairing).",
    "interpretation": "Higher classification accuracy or higher average win rate indicates better model performance. The paper finds closed-ended accuracy is concentrated and less discriminative for Chinese LLMs and may not reflect human preferences; combining CLOSE and OPEN (especially OPEN MULTIPLE) yields higher correlation with user preferences in CArena. Agreement between GPT-4 judgments and human raters (Pearson correlation) is reported as 0.80.",
    "baseline_results": "GPT-4: CLOSE 70.67% accuracy; OPEN SINGLE win&tie 94.52%; OPEN MULTI 94.87%; OPEN ALL 94.64%. MiniMax: CLOSE 60.67%; OPEN SINGLE 65.32%; OPEN MULTI 47.34%; OPEN ALL 57.94%; CArena 86.69%. ChatGLM2-6B: CLOSE 57.67%; OPEN SINGLE 42.33%; OPEN MULTI 30.67%; OPEN ALL 36.50%; CArena 85.63%.",
    "validation": "Validated against user self-reported ratings from CArena treated as gold standard. Agreement between GPT-4 automatic judgments and human reviewers measured with Pearson correlation = 0.80. CLOSE conversion underwent a three-stage human review process (each stage with three human reviewers)."
  },
  "targeted_risks": {
    "risk_categories": [
      "Safety",
      "Accuracy"
    ],
    "atlas_risks": {
      "risks": null
    },
    "demographic_analysis": "N/A",
    "harm": null
  },
  "ethical_and_legal_considerations": {
    "privacy_and_anonymity": "Interactions on the LangYa Leaderboard are anonymous and user-reported ratings are collected from anonymized interactions.",
    "data_licensing": "N/A",
    "consent_procedures": "N/A",
    "compliance_with_regulations": "N/A"
  }
}