{
  "benchmark_details": {
    "name": "GraphEval",
    "overview": "A hallucination evaluation framework based on representing information in Knowledge Graph structures. It identifies specific triples in the KG that are prone to hallucinations, providing more insight into inconsistencies in LLM outputs.",
    "data_type": "Text",
    "domains": [
      "Summarization",
      "Question Answering",
      "Common Sense Reasoning"
    ],
    "languages": [
      "English"
    ],
    "similar_benchmarks": [
      "FactScore",
      "G-Eval",
      "GPTScore"
    ],
    "resources": [
      "arXiv:2407.10793v1",
      "Amazon Bedrock API"
    ]
  },
  "purpose_and_intended_users": {
    "goal": "To improve the detection and correction of hallucinations in LLM outputs using a Knowledge Graph framework.",
    "audience": [
      "Researchers",
      "AI Developers",
      "Practitioners in Natural Language Processing"
    ],
    "tasks": [
      "Detect hallucinations in LLM outputs",
      "Provide explainable decisions on inconsistencies",
      "Correct identified hallucinations"
    ],
    "limitations": "Focuses exclusively on closed-domain hallucination detection.",
    "out_of_scope_uses": [
      "Open-domain hallucination detection",
      "Real-time applications without prior context"
    ]
  },
  "data": {
    "source": "SummEval, QAGS-C, QAGS-X datasets",
    "size": "N/A",
    "format": "Text",
    "annotation": "Human-annotated for factual consistency"
  },
  "methodology": {
    "methods": [
      "Knowledge Graph construction",
      "Natural Language Inference models"
    ],
    "metrics": [
      "Balanced accuracy",
      "ROUGE-1",
      "ROUGE-2",
      "ROUGE-L"
    ],
    "calculation": "Balanced accuracy calculated based on the number of factually consistent vs inconsistent examples.",
    "interpretation": "Higher accuracy indicates better consistency between LLM output and provided context.",
    "baseline_results": "Adding the GraphEval preprocessing step improved balanced accuracy by an average of 6.2 (SE=1.3).",
    "validation": "Evaluated on popular hallucination detection benchmarks."
  },
  "targeted_risks": {
    "risk_categories": [
      "Accuracy",
      "Societal Impact",
      "Privacy"
    ],
    "atlas_risks": {
      "risks": [
        {
          "category": "Accuracy",
          "subcategory": [
            "Unrepresentative data"
          ]
        },
        {
          "category": "Societal Impact",
          "subcategory": [
            "Impact on education: plagiarism",
            "Impact on Jobs"
          ]
        },
        {
          "category": "Privacy",
          "subcategory": [
            "Reidentification"
          ]
        }
      ]
    },
    "demographic_analysis": "N/A",
    "harm": [
      "Potential job displacement due to automation in content generation",
      "Educational integrity risks from reliance on LLMs"
    ]
  },
  "ethical_and_legal_considerations": {
    "privacy_and_anonymity": "No personal data is used in the evaluation datasets employed.",
    "data_licensing": "Data is sourced from licensed datasets like CNN/DailyMail.",
    "consent_procedures": "N/A",
    "compliance_with_regulations": "Follows ethical guidelines for handling generated data."
  }
}