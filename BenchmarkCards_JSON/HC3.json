{
  "benchmark_details": {
    "is_benchmark": true,
    "name": "Human ChatGPT Comparison Corpus (HC3)",
    "abbreviation": "HC3",
    "overview": "We collected tens of thousands of comparison responses from both human experts and ChatGPT, with questions ranging from open-domain, financial, medical, legal, and psychological areas. We call the collected dataset the Human ChatGPT Comparison Corpus (HC3). The dataset is intended to facilitate LLM-related research, analyze linguistic and stylistic differences between humans and ChatGPT, and support development/evaluation of ChatGPT content detection systems.",
    "data_type": "question-answering pairs (text)",
    "domains": [
      "Open-domain",
      "Computer Science",
      "Finance",
      "Medicine",
      "Legal",
      "Psychology"
    ],
    "languages": [
      "English",
      "Chinese"
    ],
    "similar_benchmarks": [
      "ELI5",
      "WikiQA",
      "FiQA",
      "Medical Dialog dataset",
      "NLPCC-DBQA",
      "WebTextQA",
      "BaikeQA",
      "LegalQA"
    ],
    "resources": [
      "https://github.com/Hello-SimpleAI/chatgpt-comparison-detection",
      "https://arxiv.org/abs/2301.07597"
    ]
  },
  "purpose_and_intended_users": {
    "goal": "To facilitate LLM-related research, especially the study on the comparison between humans and LLMs, by providing a comparison corpus of nearly 40K questions and their corresponding human and ChatGPT answers to analyze linguistic/stylistic characteristics and to support development of ChatGPT content detectors.",
    "audience": [
      "ML Researchers",
      "Academic researchers",
      "Developers of AIGC-detection tools",
      "Online platform regulators"
    ],
    "tasks": [
      "Question Answering",
      "Machine-generated Text Detection",
      "Linguistic Analysis",
      "Human Evaluation",
      "Sentiment Analysis"
    ],
    "limitations": "1. The amount and range of collected data are still not enough and the data from different sources are unbalanced. 2. All collected ChatGPT answers are generated without special prompts; analyses are based on ChatGPT's general style/state. 3. ChatGPT may be mainly trained on English corpus while less on Chinese; conclusions on HC3-Chinese may not always be precise.",
    "out_of_scope_uses": []
  },
  "data": {
    "source": "Publicly available question-answering datasets and crawled wiki sources. Specific sources include ELI5, WikiQA, FiQA, Medical Dialog dataset, NLPCC-DBQA, WebTextQA, BaikeQA, LegalQA, and crawled content from Wikipedia and BaiduBaike.",
    "size": "HC3-English: 24,322 questions, 58,546 human answers, 26,903 ChatGPT answers; HC3-Chinese: 12,853 questions, 22,259 human answers, 17,522 ChatGPT answers. (Nearly 40K questions in total.)",
    "format": "JSON (each record: { \"question\": \"Q1\", \"human_answers\": [\"A1\", \"A2\"], \"chatgpt_answers\": [\"B1\"] })",
    "annotation": "Human answers are taken from existing QA datasets (expert-authored answers or high-voted web answers) or wiki explanations; ChatGPT answers were generated by manually inputting questions into ChatGPT (refreshing thread per question)."
  },
  "methodology": {
    "methods": [
      "Human evaluation (Turing tests and Helpfulness Test)",
      "Linguistic analyses (vocabulary, POS, dependency parsing, sentiment, perplexity)",
      "Automated detection experiments (GLTR Test-2 features + logistic regression; RoBERTa-based single-text classifier; RoBERTa-based QA-style classifier)"
    ],
    "metrics": [
      "Proportion (human evaluation: proportion that ChatGPT-generated answer is correctly detected)",
      "Proportion (helpfulness: proportion that ChatGPT-generated answer is considered more helpful)",
      "F1 Score (detection models)",
      "Perplexity (PPL) at text and sentence levels",
      "Average answer length",
      "Vocabulary size",
      "Density (100 * V / (L * N))",
      "Sentiment distribution (neutral/positive/negative)"
    ],
    "calculation": "Human Turing tests report the proportion that testers correctly detect ChatGPT-generated answers. Helpfulness test reports the proportion that testers consider the ChatGPT answer more helpful. Detection models are evaluated using F1 Score on held-out test sets. Perplexity computed using GPT-2 small for English and Wenzhong-GPT2-110M for Chinese at text and sentence levels. Vocabulary/density statistics computed as described in the paper (average length L, vocab size V, density D = 100 * V / (L * N)).",
    "interpretation": "For human evaluations, higher proportion indicates easier detection of ChatGPT outputs or higher perceived helpfulness. Lower perplexity indicates the text is more aligned with patterns learned by the language model (model more confident). Higher F1 indicates better detector performance. Density, vocab size, and average length are used to compare lexical diversity and verbosity between human and ChatGPT answers.",
    "baseline_results": "Representative results reported in the paper include: RoBERTa-based detectors achieve very high F1 on in-distribution tests (e.g., RoBERTa trained on raw-full: 99.82% F1 on raw-full English test). GLTR Test-2 based logistic regression shows lower and less robust performance (e.g., GLTR raw-full: 98.26% F1 on raw-full English test, with larger drops under OOD and sentence-level settings). Detailed per-split and per-setting F1 scores are provided in Tables 4-6 of the paper.",
    "validation": "Held-out test sets and multiple train/test versions (raw-full, filtered-full, raw-sent, filtered-sent, raw-mix, filtered-mix) were used. Out-of-distribution (OOD) generalization evaluations conducted by training on one version and testing on another to assess robustness."
  },
  "targeted_risks": {
    "risk_categories": [
      "Misuse",
      "Societal Impact",
      "Value Alignment",
      "Robustness",
      "Accuracy",
      "Transparency"
    ],
    "atlas_risks": {
      "risks": [
        {
          "category": "Misuse",
          "subcategory": [
            "Spreading disinformation"
          ]
        },
        {
          "category": "Societal Impact",
          "subcategory": [
            "Impact on education: plagiarism",
            "Impact on education: bypassing learning"
          ]
        },
        {
          "category": "Value Alignment",
          "subcategory": [
            "Toxic output",
            "Harmful output"
          ]
        },
        {
          "category": "Robustness",
          "subcategory": [
            "Hallucination"
          ]
        },
        {
          "category": "Accuracy",
          "subcategory": [
            "Poor model accuracy"
          ]
        },
        {
          "category": "Transparency",
          "subcategory": [
            "Lack of training data transparency"
          ]
        }
      ]
    },
    "demographic_analysis": "N/A",
    "harm": [
      "Fake news / spreading disinformation",
      "Plagiarism",
      "Threats to social security (mentioned as social security issues)",
      "Undermining online exam integrity",
      "Potentially harmful or incorrect medical advice"
    ]
  },
  "ethical_and_legal_considerations": {
    "privacy_and_anonymity": "N/A",
    "data_licensing": "N/A",
    "consent_procedures": "N/A",
    "compliance_with_regulations": "N/A"
  }
}