{
  "benchmark_details": {
    "is_benchmark": true,
    "name": "DiLM (Distilling Dataset into Language Model)",
    "abbreviation": "DiLM",
    "overview": "DiLM is the first text-level dataset distillation approach that trains a language model to generate informative synthetic samples as text data, enabling the training of models independent of word embedding weights and improving interpretability.",
    "data_type": "text",
    "domains": [
      "Natural Language Processing"
    ],
    "languages": [
      "English"
    ],
    "similar_benchmarks": [],
    "resources": [
      "https://github.com/arumaekawa/DiLM"
    ]
  },
  "purpose_and_intended_users": {
    "goal": "To distill a training dataset into a smaller number of informative synthetic samples that enable effective training of neural networks.",
    "audience": [
      "ML Researchers",
      "Model Developers"
    ],
    "tasks": [
      "Text Classification"
    ],
    "limitations": "While successful as a text-level distillation method, there is still a performance gap from full datasets.",
    "out_of_scope_uses": []
  },
  "data": {
    "source": "GLUE benchmark datasets (SST-2, QQP, MNLI-m)",
    "size": "Approximately 67,000 examples for SST-2, 364,000 examples for QQP, 393,000 examples for MNLI-m",
    "format": "JSON",
    "annotation": "Synthetic samples generated by training a language model."
  },
  "methodology": {
    "methods": [
      "Gradient matching",
      "K-centers sampling",
      "Diverse mini-batch sampling"
    ],
    "metrics": [
      "Accuracy",
      "F1 Score"
    ],
    "calculation": "Gradient matching loss calculated using cosine similarity between real and generated sample gradients.",
    "interpretation": "Higher accuracy and F1 score indicate better model performance when trained on distilled datasets.",
    "baseline_results": "Outperformed Random and K-centers methods on various benchmarks.",
    "validation": "Performance validated across different models and settings using 100 runs for reliability."
  },
  "targeted_risks": {
    "risk_categories": [
      "Accuracy",
      "Fairness"
    ],
    "atlas_risks": {
      "risks": []
    },
    "demographic_analysis": "Further demographic analysis needed for fairness assessment.",
    "harm": []
  },
  "ethical_and_legal_considerations": {
    "privacy_and_anonymity": "N/A",
    "data_licensing": "N/A",
    "consent_procedures": "N/A",
    "compliance_with_regulations": "N/A"
  }
}