{
  "benchmark_details": {
    "is_benchmark": true,
    "name": "TransportationGames",
    "abbreviation": "N/A",
    "overview": "TransportationGames is a carefully designed and thorough evaluation benchmark for assessing (M)LLMs in the transportation domain, focusing on their capabilities in memorizing, understanding, and applying transportation knowledge through various tasks.",
    "data_type": "multimodal",
    "domains": [
      "Transportation"
    ],
    "languages": [
      "Chinese"
    ],
    "similar_benchmarks": [
      "MMLU",
      "C-Eval",
      "CMMLU",
      "BIG-bench",
      "MMBench",
      "LegalBench",
      "LawBench",
      "MIR-based benchmark",
      "ChemLLM-Bench"
    ],
    "resources": [
      "http://transportation.games"
    ]
  },
  "purpose_and_intended_users": {
    "goal": "To accurately evaluate the capabilities of (M)LLMs in executing transportation-related tasks.",
    "audience": [
      "ML Researchers",
      "Industry Practitioners",
      "Model Developers",
      "Domain Experts"
    ],
    "tasks": [
      "Traffic Concepts Question Answering",
      "Traffic Regulations Question Answering",
      "Traffic Signs Question Answering",
      "Traffic Accidents Analysis",
      "Traffic Public Sentiment Analysis",
      "Traffic Safety Recommendation",
      "Traffic Sign Error Detection",
      "Traffic Road Occupation Detection",
      "Traffic Emergency Plan Generation",
      "Traffic Safety Education Copy Generation"
    ],
    "limitations": "Data leakage due to the use of data from the internet. Evaluation of long text generation is challenging owing to non-uniqueness of answers.",
    "out_of_scope_uses": []
  },
  "data": {
    "source": "Data for TransportationGames is primarily sourced from internet examination papers, accident reports, public sentiment from news websites, and institutional articles.",
    "size": "N/A",
    "format": "N/A",
    "annotation": "Data has been processed with manual verification to ensure accuracy and coherence."
  },
  "methodology": {
    "methods": [
      "Automated metrics",
      "Model-based evaluation"
    ],
    "metrics": [
      "Accuracy",
      "ROUGE"
    ],
    "calculation": "Metrics are calculated based on golden answers, with accuracy for multiple-choice and True/False tasks, and ROUGE for generation tasks.",
    "interpretation": "The higher the metric score, the better the performance of the model.",
    "baseline_results": "N/A",
    "validation": "The benchmark was validated through extensive testing on various (M)LLMs."
  },
  "targeted_risks": {
    "risk_categories": [
      "Accuracy",
      "Fairness"
    ],
    "atlas_risks": {
      "risks": [
        {
          "category": "Accuracy",
          "subcategory": [
            "Unrepresentative data"
          ]
        },
        {
          "category": "Fairness",
          "subcategory": [
            "Data bias"
          ]
        }
      ]
    },
    "demographic_analysis": "N/A",
    "harm": []
  },
  "ethical_and_legal_considerations": {
    "privacy_and_anonymity": "Sensitive information was removed during data processing to safeguard privacy.",
    "data_licensing": "N/A",
    "consent_procedures": "N/A",
    "compliance_with_regulations": "N/A"
  }
}