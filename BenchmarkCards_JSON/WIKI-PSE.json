{
  "benchmark_details": {
    "is_benchmark": true,
    "name": "WIKI-PSE (WIKIpedia-driven resource for Probing Semantics in word Embeddings)",
    "abbreviation": "WIKI-PSE",
    "overview": "A large dataset based on Wikipedia annotations and word senses where word senses from different words are related by semantic classes (S-classes). It is the basis for diagnostic tests that probe word embeddings for semantic classes and analyze embedding space by classifying embeddings into semantic classes.",
    "data_type": "text (Wikipedia sentences; word/S-class pairs; token-level S-class annotations)",
    "domains": [
      "Natural Language Processing"
    ],
    "languages": [
      "English"
    ],
    "similar_benchmarks": [
      "SEMCAT",
      "HyperLex",
      "SemCor",
      "Supersense annotated Wikipedia (UKP)",
      "FIGER"
    ],
    "resources": [
      "https://github.com/yyaghoobzadeh/WIKI-PSE",
      "https://arxiv.org/abs/1906.03608",
      "https://github.com/xiaoling/figer"
    ]
  },
  "purpose_and_intended_users": {
    "goal": "Enable probing of word embeddings for semantic classes (S-classes) and analyze how multiple senses are represented; provide a corpus and dataset for supervised diagnostic classification tests of embedding content.",
    "audience": [],
    "tasks": [
      "Text Classification (S-class prediction at word/type level)",
      "Ambiguity Detection (predict whether a word is single-sense or multi-sense)",
      "Word Sense Representation Evaluation (probing)"
    ],
    "limitations": "Annotation contains noise due to distant supervision from Wikipedia anchor links; granularity is reduced by using 34 parent S-classes to reduce noise; resource focuses on common and proper nouns and is based on an English Wikipedia snapshot (2014-07-07).",
    "out_of_scope_uses": []
  },
  "data": {
    "source": "English Wikipedia (2014-07-07); annotations derived from Wikipedia anchor links mapped to FIGER types via Freebase mapping (https://github.com/xiaoling/figer); uses distant supervision.",
    "size": "550 million tokens; vocabulary ≈500,000; ≈276,000 annotated words in the vocabulary; ≈343,000 word/S-class pairs; train set: 44,250 examples; test set: 44,250 examples.",
    "format": "Plain text (lowercased sentences) with linked mentions marked as '@mention@' and word/S-class tokens; corpus-based dataset of word/S-class pairs.",
    "annotation": "Distant supervision from Wikipedia anchor links mapped to FIGER types; token-level S-class annotations; reduced to 34 parent S-classes to reduce noise."
  },
  "methodology": {
    "methods": [
      "Supervised probing via binary classification per S-class",
      "Logistic Regression",
      "Multi-layer Perceptron (one hidden layer with ReLU)",
      "K-Nearest Neighbors",
      "Nearest-neighbor cosine similarity",
      "Word embedding learning using SkipGram (SKIP) and Structured SkipGram (SSKIP) trained with Wang2Vec"
    ],
    "metrics": [
      "Micro F1",
      "Accuracy",
      "Spearman correlation"
    ],
    "calculation": "S-class prediction: global metric of micro F1 over all test examples and over all S-class predictions. Ambiguity prediction: Accuracy on held-out test set. Similarity evaluation: Spearman correlation between model similarities and human judgments.",
    "interpretation": "Micro F1 quantifies how well S-classes are encoded in embeddings; Accuracy quantifies ability to distinguish ambiguous vs. unambiguous words; higher Spearman correlation indicates better alignment with human similarity judgments.",
    "baseline_results": "Random baseline micro F1 = 0.273 (Table 2). Ambiguity FREQUENCY baseline accuracy = 64.8% (Table 4). GloVe (6B) MLP F1 = 0.685 on the shared-vocabulary subset (Table 3). FastText (Wiki) MLP F1 = 0.697 on the shared-vocabulary subset (Table 3).",
    "validation": "Dataset split into equal-sized train and test sets of 44,250 each with equal numbers of single- and multiclass words; evaluation performed on held-out test sets. Additional subset for public embeddings comparison: 13,000 train and 13,000 test."
  },
  "targeted_risks": {
    "risk_categories": [
      "Accuracy",
      "Transparency"
    ],
    "atlas_risks": {
      "risks": [
        {
          "category": "Accuracy",
          "subcategory": [
            "Data contamination"
          ]
        },
        {
          "category": "Transparency",
          "subcategory": [
            "Uncertain data provenance"
          ]
        }
      ]
    },
    "demographic_analysis": null,
    "harm": null
  },
  "ethical_and_legal_considerations": {
    "privacy_and_anonymity": "N/A",
    "data_licensing": "N/A",
    "consent_procedures": "N/A",
    "compliance_with_regulations": "N/A"
  }
}