{
  "benchmark_details": {
    "is_benchmark": true,
    "name": "AIR-Bench (Audio InstRuction Bench mark)",
    "abbreviation": "AIR-Bench",
    "overview": "AIR-Bench is the first benchmark designed to evaluate the ability of LALMs to understand various types of audio signals (including human speech, natural sounds, and music), and interact with humans in text format. It consists of two dimensions: foundation and chat benchmarks.",
    "data_type": "question-answering pairs",
    "domains": [
      "Natural Language Processing"
    ],
    "languages": [
      "English"
    ],
    "similar_benchmarks": [],
    "resources": [
      "https://github.com/OFA-Sys/AIR-Bench"
    ]
  },
  "purpose_and_intended_users": {
    "goal": "To evaluate the ability of large audio-language models to comprehend various audio signals and to follow human instructions.",
    "audience": [
      "ML Researchers",
      "Model Developers"
    ],
    "tasks": [
      "Question Answering"
    ],
    "limitations": "The benchmark does not incorporate tasks involving multiple audio comparisons and does not encompass the evaluation of multi-turn dialogues.",
    "out_of_scope_uses": []
  },
  "data": {
    "source": "Various established audio datasets for speech, sound, and music, curated for performance assessments.",
    "size": "approximately 19,000 single-choice questions and 2,000 open-ended audio questions",
    "format": "N/A",
    "annotation": "Manually reviewed and generated with the assistance of GPT-4."
  },
  "methodology": {
    "methods": [
      "Human evaluation",
      "Automated metrics"
    ],
    "metrics": [
      "Accuracy",
      "GPT-4 alignment score"
    ],
    "calculation": "Scores are averaged across different evaluations, considering reference answers generated by GPT-4.",
    "interpretation": "Scores are interpreted based on the relevance, usefulness, and accuracy of generated hypotheses.",
    "baseline_results": "N/A",
    "validation": "Evaluated through consistency checks between GPT-4 assessments and human judgments."
  },
  "targeted_risks": {
    "risk_categories": [
      "Bias",
      "Accuracy"
    ],
    "atlas_risks": {
      "risks": [
        {
          "category": "Fairness",
          "subcategory": [
            "Data bias"
          ]
        },
        {
          "category": "Accuracy",
          "subcategory": [
            "Unrepresentative data"
          ]
        },
        {
          "category": "Transparency",
          "subcategory": [
            "Uncertain data provenance"
          ]
        }
      ]
    },
    "demographic_analysis": "N/A",
    "harm": []
  },
  "ethical_and_legal_considerations": {
    "privacy_and_anonymity": "Data privacy measures adhere to licensing and guideline requirements of original open-source materials.",
    "data_licensing": "N/A",
    "consent_procedures": "N/A",
    "compliance_with_regulations": "N/A"
  }
}