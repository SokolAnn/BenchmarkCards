{
  "benchmark_details": {
    "is_benchmark": true,
    "name": "StereoBias-Stories (SBS)",
    "abbreviation": "SBS",
    "overview": "The StereoBias-Stories (SBS) dataset is designed to analyze gender bias in narratives generated by large language models (LLMs) using psychological stereotypes. It contains nearly 150,000 generated short stories that explore how gender representation shifts in response to different stereotypical attributes during story generation.",
    "data_type": "narrative stories",
    "domains": [
      "Natural Language Processing"
    ],
    "languages": [
      "English"
    ],
    "similar_benchmarks": [],
    "resources": [
      "https://arxiv.org/abs/2508.03292v1"
    ]
  },
  "purpose_and_intended_users": {
    "goal": "To provide a detailed examination of how gender bias manifests within narratives generated by large language models and how it is influenced by psychological stereotypes.",
    "audience": [
      "ML Researchers",
      "Model Developers",
      "Fairness Researchers"
    ],
    "tasks": [
      "Story Generation",
      "Bias Analysis"
    ],
    "limitations": "The analysis focuses on binary gender representations and may not encapsulate the complexity of gender identities. It also primarily uses English, limiting its generalizability across languages.",
    "out_of_scope_uses": []
  },
  "data": {
    "source": "Generated dataset from fine-tuning large language models across different prompting settings.",
    "size": "148,082 stories",
    "format": "JSON",
    "annotation": "Stories generated based on psychological stereotypes, evaluated using human judgments and automated metrics."
  },
  "methodology": {
    "methods": [
      "Lexical evaluation",
      "User study",
      "Automated metrics"
    ],
    "metrics": [
      "Perplexity",
      "User ratings (1-5 scale)",
      "Attribute expression rating"
    ],
    "calculation": "Metrics are calculated to evaluate the quality and bias of generated narratives.",
    "interpretation": "A higher rating denotes better overall quality and stronger attribute expression in the narratives.",
    "baseline_results": null,
    "validation": "The dataset was evaluated using a user study with 58 participants and multiple automated metrics."
  },
  "targeted_risks": {
    "risk_categories": [
      "Bias",
      "Fairness"
    ],
    "atlas_risks": {
      "risks": [
        {
          "category": "Fairness",
          "subcategory": [
            "Output bias"
          ]
        },
        {
          "category": "Accuracy",
          "subcategory": [
            "Unrepresentative data"
          ]
        }
      ]
    },
    "demographic_analysis": "No demographic analysis includes non-binary and diverse gender identities.",
    "harm": [
      "Potential reinforcement of harmful gender stereotypes through generated narratives"
    ]
  },
  "ethical_and_legal_considerations": {
    "privacy_and_anonymity": "The content generation process included anonymization of participant data in the user study.",
    "data_licensing": "N/A",
    "consent_procedures": "Participants in the user study were informed about the content nature and allowed to opt out.",
    "compliance_with_regulations": "N/A"
  }
}