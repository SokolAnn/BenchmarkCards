{
  "benchmark_details": {
    "is_benchmark": true,
    "name": "KorQuAD1.0 : Korean QA Dataset for Machine Reading Comprehension",
    "abbreviation": "KorQuAD",
    "overview": "We present Korean Question Answering Dataset( KorQuAD ), a large-scale Korean dataset for extractive machine reading comprehension task. It consists of 70,000+ human generated question-answer pairs on Korean Wikipedia articles. We release KorQuAD1.0 and launch a challenge at https:// KorQuAD .github.io to encourage the development of multilingual natural language processing research.",
    "data_type": "question-answering pairs (extractive)",
    "domains": [
      "Natural Language Processing"
    ],
    "languages": [
      "Korean"
    ],
    "similar_benchmarks": [
      "SQuAD (Stanford Question Answering Dataset)",
      "SQuAD2.0",
      "Hotpot QA",
      "MS Marco (Microsoft Machine Reading Comprehension Dataset)",
      "K-QuAD"
    ],
    "resources": [
      "N/A"
    ]
  },
  "purpose_and_intended_users": {
    "goal": "To introduce KorQuAD1.0, a large-scale standard question-answering dataset for Korean extractive machine reading comprehension and to contribute to researchers in multilingual natural language processing by providing training data and an official evaluation challenge.",
    "audience": [
      "Researchers in multilingual natural language processing"
    ],
    "tasks": [
      "Question Answering",
      "Machine Reading Comprehension (Extractive)"
    ],
    "limitations": "2.9% of the questions are due to errors of the worker (errors in questioning). F1 is computed at character-level because morpheme analysis is imperfect for the Korean language.",
    "out_of_scope_uses": []
  },
  "data": {
    "source": "Korean Wikipedia articles (1,647 articles collected; passages extracted from these articles; selected from 'Alchan-geul' and 'Joeun-geul' lists plus random sampling).",
    "size": "70,079 question-answer pairs; 1,647 articles collected. Train: 1,420 articles, 9,681 paragraphs, 60,407 questions; Dev: 140 articles, 964 paragraphs, 5,774 questions; Test: 77 articles, 623 paragraphs, 3,898 questions.",
    "format": "N/A",
    "annotation": "Crowdsourced by crowdworkers: one passage allocated to 3 workers, each worker created up to 3 questions per passage; workers typed questions in natural language and were asked to select a minimal answer span in the morpheme unit. Additional workers generated secondary (shortest-span) answers on the test dataset."
  },
  "methodology": {
    "methods": [
      "Automated metrics",
      "Human evaluation",
      "Baseline model evaluation"
    ],
    "metrics": [
      "Exact Match (EM)",
      "Character-level F1"
    ],
    "calculation": "EM: This metric measures the percentage of predictions that exactly match the ground truth answer. F1: This metric measures the overlap between the prediction and the ground truth answer. We adopt character-level F1 that compute the percentage of overlapping characters.",
    "interpretation": "EM measures the percentage of exact matches; character-level F1 measures overlap of characters between prediction and ground truth; higher values indicate better performance. The paper does not provide explicit numeric thresholds for good vs. poor performance.",
    "baseline_results": "Validation set: S³-Net EM 71.68%, F1 82.87%; BERT EM 70.89%, F1 90.24%. Test set: S³-Net EM 71.52%, F1 82.99%; BERT EM 71.68%, F1 89.76%. Human (test set) F1 91.20%. Total collected question-answer pairs: 70,079.",
    "validation": "Data split into Train/Dev/Test (Train: 1,420 articles; Dev: 140; Test: 77). Additional workers generated secondary answers on the test dataset to compare human performance. A sample of 280 development questions (two per 140 paragraphs) was used for qualitative analysis of reasoning types."
  },
  "targeted_risks": {
    "risk_categories": [
      "Accuracy"
    ],
    "atlas_risks": {
      "risks": [
        {
          "category": "Accuracy",
          "subcategory": [
            "Data contamination",
            "Poor model accuracy"
          ]
        }
      ]
    },
    "demographic_analysis": null,
    "harm": null
  },
  "ethical_and_legal_considerations": {
    "privacy_and_anonymity": "N/A",
    "data_licensing": "N/A",
    "consent_procedures": "N/A",
    "compliance_with_regulations": "N/A"
  }
}