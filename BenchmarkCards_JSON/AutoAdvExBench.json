{
  "benchmark_details": {
    "is_benchmark": true,
    "name": "AutoAdvExBench",
    "abbreviation": "N/A",
    "overview": "AutoAdvExBench is a benchmark to evaluate if large language models (LLMs) can autonomously exploit defenses to adversarial examples. It evaluates the ability of LLMs to automatically implement adversarial attack algorithms that break defenses designed to be robust to adversarial examples.",
    "data_type": "code implementations",
    "domains": [
      "Natural Language Processing",
      "Machine Learning Security"
    ],
    "languages": [
      "English"
    ],
    "similar_benchmarks": [
      "MMLU",
      "GLUE"
    ],
    "resources": [
      "https://github.com/ethz-spylab/AutoAdvExBench"
    ]
  },
  "purpose_and_intended_users": {
    "goal": "To serve as a challenging benchmark for evaluating LLM capabilities in breaking adversarial example defenses.",
    "audience": [
      "AI Researchers",
      "Machine Learning Security Practitioners",
      "Security Experts"
    ],
    "tasks": [
      "Adversarial Attack Generation",
      "Evaluation of Defense Mechanisms"
    ],
    "limitations": "The dataset may contain benchmark contamination as some defenses have known published breaks.",
    "out_of_scope_uses": []
  },
  "data": {
    "source": "Crawled from arXiv papers relating to adversarial machine learning, filtered to include only defenses with code implementations.",
    "size": "51 implementations",
    "format": "N/A",
    "annotation": "N/A"
  },
  "methodology": {
    "methods": [
      "Automated Evaluation",
      "Human Evaluation"
    ],
    "metrics": [
      "Attack Success Rate"
    ],
    "calculation": "Success rate is calculated as the percentage of defenses whose robust accuracy falls below a specific threshold.",
    "interpretation": "A lower robust accuracy indicates a higher attack success rate.",
    "baseline_results": null,
    "validation": "N/A"
  },
  "targeted_risks": {
    "risk_categories": [
      "Bias",
      "Safety",
      "Privacy",
      "Robustness",
      "Fairness",
      "Accuracy"
    ],
    "atlas_risks": {
      "risks": []
    },
    "demographic_analysis": "N/A",
    "harm": "N/A"
  },
  "ethical_and_legal_considerations": {
    "privacy_and_anonymity": "N/A",
    "data_licensing": "N/A",
    "consent_procedures": "N/A",
    "compliance_with_regulations": "N/A"
  }
}