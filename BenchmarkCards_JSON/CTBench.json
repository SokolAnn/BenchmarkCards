{
  "benchmark_details": {
    "is_benchmark": true,
    "name": "CTBench (Comprehensive Benchmark for Evaluating Language Model Capabilities in Clinical Trial Design)",
    "abbreviation": "CTBench",
    "overview": "CTBench is a benchmark to assess language models (LMs) in aiding clinical study design, examining how well AI models can determine baseline features of clinical trials.",
    "data_type": "clinical trial metadata and baseline features",
    "domains": [
      "Healthcare"
    ],
    "languages": [
      "English"
    ],
    "similar_benchmarks": [],
    "resources": [
      "https://github.com/nafis-neehal/CTBench_LLM"
    ]
  },
  "purpose_and_intended_users": {
    "goal": "The primary objective of the benchmark is to assess the role of language models in aiding clinical study design by predicting baseline characteristics based on metadata.",
    "audience": [
      "Clinical researchers",
      "AI researchers",
      "Healthcare professionals"
    ],
    "tasks": [
      "Baseline feature prediction",
      "Evaluation of language models"
    ],
    "limitations": "CTBench consists of only RCTs for 5 chronic diseases with a subset annotated with additional ",
    "out_of_scope_uses": []
  },
  "data": {
    "source": "ClinicalTrials.gov API and relevant publications",
    "size": "1,690 clinical trials",
    "format": "structured textual data",
    "annotation": "Manually collected baseline features by human annotators from clinical publications"
  },
  "methodology": {
    "methods": [
      "Human evaluation",
      "Automated metrics using ListMatch-LM and ListMatch-BERT"
    ],
    "metrics": [
      "Precision",
      "Recall",
      "F1 Score"
    ],
    "calculation": "Precision and recall are calculated based on the matched pairs and the remaining candidate/reference features.",
    "interpretation": "High recall ensures that all relevant baseline features are identified for accurate characterization of study cohorts.",
    "baseline_results": null,
    "validation": "Validated through human-in-the-loop evaluations and expert feedback."
  },
  "targeted_risks": {
    "risk_categories": [
      "Bias",
      "Accuracy"
    ],
    "atlas_risks": {
      "risks": []
    },
    "demographic_analysis": null,
    "harm": null
  },
  "ethical_and_legal_considerations": {
    "privacy_and_anonymity": "N/A",
    "data_licensing": "N/A",
    "consent_procedures": "N/A",
    "compliance_with_regulations": "N/A"
  }
}