{
  "benchmark_details": {
    "is_benchmark": true,
    "name": "Don’t Patronize Me! An Annotated Dataset with Patronizing and Condescending Language towards Vulnerable Communities",
    "abbreviation": "N/A",
    "overview": "This paper introduces the Don’t Patronize Me! dataset aimed at supporting the development of NLP models to identify and categorize language that is patronizing or condescending towards vulnerable communities like refugees, homeless people, and poor families.",
    "data_type": "text",
    "domains": [
      "Natural Language Processing"
    ],
    "languages": [
      "English"
    ],
    "similar_benchmarks": [
      "Talkdown"
    ],
    "resources": [
      "https://github.com/Perez-AlmendrosC/dontpatronizeme"
    ]
  },
  "purpose_and_intended_users": {
    "goal": "To encourage more research on detecting patronizing and condescending language (PCL) towards vulnerable communities.",
    "audience": [
      "ML Researchers",
      "NLP Practitioners",
      "Social Scientists"
    ],
    "tasks": [
      "Text Classification"
    ],
    "limitations": "N/A",
    "out_of_scope_uses": []
  },
  "data": {
    "source": "Paragraphs extracted from news stories in the News on Web (NoW) corpus.",
    "size": "10,637 paragraphs",
    "format": "N/A",
    "annotation": "Annotated by three expert annotators, using a two-step annotation process."
  },
  "methodology": {
    "methods": [
      "SVM",
      "BiLSTM",
      "BERT",
      "RoBERTa",
      "DistilBERT"
    ],
    "metrics": [
      "Precision",
      "Recall",
      "F1 Score"
    ],
    "calculation": "Metrics calculated using standard evaluation methods for binary and multi-label classification tasks.",
    "interpretation": "Higher scores indicate better detection and categorization of PCL.",
    "baseline_results": "BERT-based approaches outperformed simpler methods, achieving significant improvements over random guessing.",
    "validation": "10-fold cross-validation used to validate methods."
  },
  "targeted_risks": {
    "risk_categories": [
      "Bias",
      "Fairness",
      "Accuracy"
    ],
    "atlas_risks": {
      "risks": []
    },
    "demographic_analysis": "N/A",
    "harm": "N/A"
  },
  "ethical_and_legal_considerations": {
    "privacy_and_anonymity": "N/A",
    "data_licensing": "Creative Commons Attribution 4.0 International Licence",
    "consent_procedures": "N/A",
    "compliance_with_regulations": "N/A"
  }
}