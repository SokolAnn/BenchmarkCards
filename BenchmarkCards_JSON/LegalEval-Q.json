{
  "benchmark_details": {
    "is_benchmark": true,
    "name": "LegalEval-Q: A New Benchmark for The Quality Evaluation of LLM-Generated Legal Text",
    "abbreviation": "LegalEval-Q",
    "overview": "LegalEval-Q introduces a multidimensional assessment model that measures critical quality attributes of legal texts, including clarity, coherence, and terminological precision. This benchmark is designed to systematically evaluate the quality of responses generated by language models in various legal contexts.",
    "data_type": "legal question-answering pairs",
    "domains": [
      "Natural Language Processing",
      "Legal"
    ],
    "languages": [
      "Chinese"
    ],
    "similar_benchmarks": [
      "MMLU (Massive Multitask Language Understanding)",
      "BIG-Bench"
    ],
    "resources": [
      "https://github.com/lyxx3rd/LegalEval-Q"
    ]
  },
  "purpose_and_intended_users": {
    "goal": "To establish standardized evaluation protocols for assessing the quality of LLM-generated legal texts and identify the optimal models for specific legal applications.",
    "audience": [
      "Legal Practitioners",
      "ML Researchers",
      "AI Developers"
    ],
    "tasks": [
      "Text Quality Evaluation",
      "Legal Question Answering"
    ],
    "limitations": "The current benchmark is specialized in the legal domain, which may bias its application to other fields.",
    "out_of_scope_uses": []
  },
  "data": {
    "source": "The data includes legal questions sourced from the DISC-Law-SFT-Pair dataset, Criminal-Law-Dataset, and a proprietary dataset focused on the Civil Code of China.",
    "size": "10,000 queries",
    "format": "JSON",
    "annotation": "Data generated by models during testing without predefined answers."
  },
  "methodology": {
    "methods": [
      "Regression Model Assessment",
      "Qualitative Evaluation"
    ],
    "metrics": [
      "Score (0-100 scale)"
    ],
    "calculation": "Scores are derived from a combination of content quality, comments, and conclusions assessing the generated responses.",
    "interpretation": "Higher scores indicate better quality of textual output, reflecting clarity, coherence, and appropriate terminology use.",
    "baseline_results": null,
    "validation": "Evaluation conducted on a validation set with strictly non-overlapping content from the training set."
  },
  "targeted_risks": {
    "risk_categories": [
      "Fairness",
      "Accuracy",
      "Privacy"
    ],
    "atlas_risks": {
      "risks": []
    },
    "demographic_analysis": "Demographic factors are not specifically analyzed but could influence text quality assessment.",
    "harm": [
      "Bias in legal text generation affecting fairness and outcome."
    ]
  },
  "ethical_and_legal_considerations": {
    "privacy_and_anonymity": "The study does not specifically discuss privacy measures.",
    "data_licensing": "N/A",
    "consent_procedures": "N/A",
    "compliance_with_regulations": "N/A"
  }
}