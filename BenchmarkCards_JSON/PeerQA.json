{
  "benchmark_details": {
    "is_benchmark": true,
    "name": "PeerQA",
    "abbreviation": "N/A",
    "overview": "PeerQA is a document-level Question Answering dataset that source questions from peer reviews of scientific articles. The dataset contains 579 QA pairs from 208 academic papers, focusing on evidence retrieval, unanswerable question classification, and answer generation.",
    "data_type": "question-answering pairs",
    "domains": [
      "Natural Language Processing",
      "Computer Science",
      "Geoscience",
      "Public Health"
    ],
    "languages": [
      "English"
    ],
    "similar_benchmarks": [
      "BioASQ",
      "QASPER",
      "QASA"
    ],
    "resources": [
      "https://github.com/UKPLab/peerqa"
    ]
  },
  "purpose_and_intended_users": {
    "goal": "To advance and study question answering on scientific documents using peer review sourced questions.",
    "audience": [
      "ML Researchers",
      "AI Practitioners",
      "NLP Experts"
    ],
    "tasks": [
      "Evidence Retrieval",
      "Question Answering",
      "Answer Generation"
    ],
    "limitations": "The dataset is limited in size compared to general domain QA datasets, and it primarily focuses on scientific articles written in English.",
    "out_of_scope_uses": []
  },
  "data": {
    "source": "Collected from peer reviews of scientific articles and annotated by the original authors.",
    "size": "579 QA pairs from 208 articles",
    "format": "JSON",
    "annotation": "Expert annotations by the authors of the respective papers."
  },
  "methodology": {
    "methods": [
      "Human evaluation",
      "Automated metrics"
    ],
    "metrics": [
      "F1 Score",
      "Recall"
    ],
    "calculation": "Metrics are calculated based on retrieved evidence and generated answers.",
    "interpretation": "The evaluation determines how accurately and effectively the QA system answers questions based on the retrieved evidence.",
    "baseline_results": "Baseline results are established for all three tasks in PeerQA, detailing model performances.",
    "validation": "Validated through expert annotations and comparison with previously established benchmarks."
  },
  "targeted_risks": {
    "risk_categories": [
      "Bias",
      "Safety",
      "Accuracy"
    ],
    "atlas_risks": {
      "risks": [
        {
          "category": "Fairness",
          "subcategory": [
            "Data bias"
          ]
        },
        {
          "category": "Accuracy",
          "subcategory": [
            "Unrepresentative data"
          ]
        }
      ]
    },
    "demographic_analysis": "Analysis includes examination across different scientific domains but is limited by the predominance of ML and NLP.",
    "harm": []
  },
  "ethical_and_legal_considerations": {
    "privacy_and_anonymity": "Authors participated voluntarily, and no personal information was collected beyond their public email addresses.",
    "data_licensing": "PeerQA's questions and answers are published under CC-BY-NC-SA 4.0 licenses.",
    "consent_procedures": "Authors were contacted via email to participate in the dataset creation.",
    "compliance_with_regulations": "N/A"
  }
}