{
  "benchmark_details": {
    "is_benchmark": true,
    "name": "Dr.Academy",
    "abbreviation": "N/A",
    "overview": "Dr.Academy is a benchmark developed to evaluate the questioning capability in education for Large Language Models (LLMs) through assessing the quality of generated educational questions based on Anderson and Krathwohl's taxonomy across various domains.",
    "data_type": "question-answering pairs",
    "domains": [
      "Natural Language Processing",
      "Education"
    ],
    "languages": [
      "English"
    ],
    "similar_benchmarks": [
      "MMLU (Massive Multitask Language Understanding)",
      "SQuAD"
    ],
    "resources": [
      "N/A"
    ]
  },
  "purpose_and_intended_users": {
    "goal": "To evaluate the questioning capability in education as a teacher for Large Language Models (LLMs) through assessing generated educational questions.",
    "audience": [
      "ML Researchers",
      "Educational Technologists"
    ],
    "tasks": [
      "Question Generation"
    ],
    "limitations": "The benchmark primarily focuses on the questioning ability and does not encompass the full range of teaching interactions.",
    "out_of_scope_uses": []
  },
  "data": {
    "source": "Generated contexts from the SQuAD dataset and the MMLU dataset.",
    "size": "20,000 contexts",
    "format": "text",
    "annotation": "Manual evaluation conducted by experts."
  },
  "methodology": {
    "methods": [
      "Automated metrics",
      "Human evaluation"
    ],
    "metrics": [
      "Relevance",
      "Coverage",
      "Representativeness",
      "Consistency"
    ],
    "calculation": "Scores are calculated based on the established metrics through evaluations performed by both automated systems and human assessors.",
    "interpretation": "Higher scores indicate better educational quality and alignment with the educational taxonomy.",
    "baseline_results": "N/A",
    "validation": "Validated through comparative assessments with expert evaluations."
  },
  "targeted_risks": {
    "risk_categories": [
      "Fairness",
      "Accuracy"
    ],
    "atlas_risks": {
      "risks": [
        {
          "category": "Fairness",
          "subcategory": [
            "Data bias"
          ]
        },
        {
          "category": "Accuracy",
          "subcategory": [
            "Poor model accuracy"
          ]
        }
      ]
    },
    "demographic_analysis": "N/A",
    "harm": []
  },
  "ethical_and_legal_considerations": {
    "privacy_and_anonymity": "N/A",
    "data_licensing": "N/A",
    "consent_procedures": "N/A",
    "compliance_with_regulations": "N/A"
  }
}