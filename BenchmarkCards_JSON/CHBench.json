{
  "benchmark_details": {
    "is_benchmark": true,
    "name": "CHBench (Chinese Health Benchmark)",
    "abbreviation": "CHBench",
    "overview": "CHBench is the first comprehensive safety-oriented benchmark designed to evaluate large language models' capabilities in understanding and addressing physical and mental health issues, consisting of 6,493 entries on mental health and 2,999 entries on physical health.",
    "data_type": "question-answering pairs",
    "domains": [
      "Healthcare"
    ],
    "languages": [
      "Chinese"
    ],
    "similar_benchmarks": [],
    "resources": [
      "https://github.com/TracyGuo2001/CHBench"
    ]
  },
  "purpose_and_intended_users": {
    "goal": "To evaluate the proficiency of Chinese large language models in understanding health-related inquiries with a focus on safety.",
    "audience": [
      "ML Researchers",
      "Healthcare Practitioners"
    ],
    "tasks": [
      "Question Answering"
    ],
    "limitations": "N/A",
    "out_of_scope_uses": []
  },
  "data": {
    "source": "Data is sourced from web posts, exams, and existing datasets, such as content from Zhihu and relevant health education materials.",
    "size": "9,492 entries",
    "format": "N/A",
    "annotation": "Gold-standard responses were generated by the ERNIE Bot after manual evaluations of existing user responses."
  },
  "methodology": {
    "methods": [
      "Human evaluation",
      "Automated metrics"
    ],
    "metrics": [
      "Cosine Similarity",
      "Jaccard Similarity Coefficient"
    ],
    "calculation": "Metrics calculated based on the similarity between generated responses and the gold-standard responses.",
    "interpretation": "Higher similarity scores indicate better alignment with desired responses, while lower scores reflect discrepancies.",
    "baseline_results": "Responses from ERNIE Bot considered as the gold-standard for evaluation.",
    "validation": "Evaluation involved manual review and comparison of model-generated responses."
  },
  "targeted_risks": {
    "risk_categories": [
      "Safety",
      "Accuracy",
      "Fairness"
    ],
    "atlas_risks": {
      "risks": []
    },
    "demographic_analysis": "N/A",
    "harm": "N/A"
  },
  "ethical_and_legal_considerations": {
    "privacy_and_anonymity": "N/A",
    "data_licensing": "N/A",
    "consent_procedures": "N/A",
    "compliance_with_regulations": "N/A"
  }
}