{
  "benchmark_details": {
    "is_benchmark": true,
    "name": "OAEI-LLM-T",
    "abbreviation": "N/A",
    "overview": "OAEI-LLM-T is a TBox benchmark dataset that captures hallucinations of large language models (LLMs) performing ontology matching (OM) tasks. It consists of seven TBox datasets from the Ontology Alignment Evaluation Initiative (OAEI) and classifies the hallucinations into two primary categories and six sub-categories.",
    "data_type": "text",
    "domains": [
      "Natural Language Processing",
      "Computer Science"
    ],
    "languages": [
      "English"
    ],
    "similar_benchmarks": [],
    "resources": [
      "https://github.com/qzc438-research/ontology-hallucination",
      "https://github.com/qzc438/ontology-llm",
      "https://dwslab.github.io/melt/track-repository",
      "https://oaei.ontologymatching.org/doc/oaei-deontology.2.html"
    ]
  },
  "purpose_and_intended_users": {
    "goal": "To understand LLM hallucinations in ontology matching tasks and improve performance of LLMs used in these tasks.",
    "audience": [
      "ML Researchers",
      "Domain Experts"
    ],
    "tasks": [
      "Ontology Matching"
    ],
    "limitations": "The dataset primarily captures significant hallucinations generated by the LLM-based ontology matching system but may not encompass the complete set of potential hallucinations.",
    "out_of_scope_uses": []
  },
  "data": {
    "source": "Constructed from seven TBox datasets in the Ontology Alignment Evaluation Initiative (OAEI).",
    "size": "N/A",
    "format": "N/A",
    "annotation": "Classified by an LLM arbiter to determine categories of hallucinations."
  },
  "methodology": {
    "methods": [
      "Automated metrics",
      "Leaderboard Construction"
    ],
    "metrics": [
      "Precision",
      "Recall",
      "F1 Score"
    ],
    "calculation": "Precision, Recall, and F1 score are calculated based on the alignment produced and the OAEI reference.",
    "interpretation": "Higher precision indicates fewer incorrect mappings, while higher recall indicates a lower number of missing mappings.",
    "baseline_results": "N/A",
    "validation": "Validation procedures were standardized across different LLMs using consistent hyperparameter settings."
  },
  "targeted_risks": {
    "risk_categories": [
      "Accuracy",
      "Fairness",
      "Robustness"
    ],
    "atlas_risks": {
      "risks": [
        {
          "category": "Accuracy",
          "subcategory": [
            "Poor model accuracy"
          ]
        }
      ]
    },
    "demographic_analysis": "N/A",
    "harm": "N/A"
  },
  "ethical_and_legal_considerations": {
    "privacy_and_anonymity": "N/A",
    "data_licensing": "N/A",
    "consent_procedures": "N/A",
    "compliance_with_regulations": "N/A"
  }
}