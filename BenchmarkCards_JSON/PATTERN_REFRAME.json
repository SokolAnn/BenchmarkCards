{
  "benchmark_details": {
    "is_benchmark": true,
    "name": "PATTERN REFRAME",
    "abbreviation": "N/A",
    "overview": "A novel dataset, PATTERN REFRAME, consisting in ~10k crowdsourced examples of thoughts containing ten classical types of unhelpful thought patterns (Burns, 1980), conditioned on personas, matched with crowdsourced proposals of reframing that do not exhibit the patterns. The paper introduces two controllable text-to-text generation tasks (generating and reframing unhelpful thoughts conditioned on persona and pattern) and a classification task to identify the unhelpful thought pattern given a persona and a thought.",
    "data_type": "text (paired examples: unhelpful thought → reframed thought; persona-conditioned statements)",
    "domains": [
      "Natural Language Processing",
      "Mental Health"
    ],
    "languages": [
      "English"
    ],
    "similar_benchmarks": [
      "PERSONA-CHAT"
    ],
    "resources": [
      "https://github.com/facebookresearch/ParlAI/tree/main/projects/reframe_thoughts",
      "https://arxiv.org/abs/2307.02768"
    ]
  },
  "purpose_and_intended_users": {
    "goal": "Provide a dataset and evaluation tasks to (1) generate persona-conditioned examples of unhelpful thought patterns, (2) reframe those unhelpful thoughts into more helpful versions, and (3) classify unhelpful thought patterns; enable training and evaluation of models for producing, recognizing, and reframing unhelpful thoughts.",
    "audience": [
      "ML Researchers",
      "Model Developers",
      "Practitioners of Cognitive Behavioral Therapy"
    ],
    "tasks": [
      "Text Generation",
      "Text Rewriting (Reframing)",
      "Text Classification"
    ],
    "limitations": "Dataset uses English-language responses written by crowdworkers located in the United States and may not reflect thought patterns and personas across cultures and diverse populations. Data is generated by paid crowdworkers (lay people), not necessarily trained clinical psychologists; entire material requires further validation. Models evaluated are resource-intensive and best-performing model (GPT3.5) is available only via a paid API.",
    "out_of_scope_uses": [
      "Using the material as is, without the supervision of a trained professional, which could be harmful (e.g., increased anxiety or warped understanding of unhelpful thoughts and useful reframings)."
    ]
  },
  "data": {
    "source": "Crowdsourced via Mephisto and Amazon Mechanical Turk; personas sourced from the PERSONA-CHAT dataset (Zhang et al., 2018). Data collection comprised four tasks: (1) writing unhelpful thoughts conditioned on a persona and a pattern, (2) labeling which patterns appear (5 annotators per thought), (3) collecting reframes (3 rewrites per thought), and (4) evaluating rewrites (5 annotators per set); majority-vote filtering applied.",
    "size": "9,688 thoughts and 26,507 reframed versions. Splits: thoughts (train/validation/test = 1,920 / 961 / 6,807), reframed thoughts (train/validation/test = 5,249 / 2,623 / 18,635). Average 2.74 rewrites per thought after filtering. Average word lengths: thoughts 19.1, rewrites 23.9.",
    "format": "N/A",
    "annotation": "Crowdsourced annotation: five annotations for pattern labeling (task 2) and five annotations for evaluation of rewrites (task 4). Collected three rewrites per thought in task 3. Used majority voting to keep items marked valid by majority. Worker qualification pipeline with onboarding tasks and selection of high-quality workers (524 qualified workers). Reported inter-annotator agreement (Krippendorf’s Alpha) of 0.355 for task 2 and 0.454 for task 4."
  },
  "methodology": {
    "methods": [
      "Fine-tuning pretrained models (BART-large, T5-large, R2C2-3B, RoBERTa-large)",
      "Few-shot prompting with GPT3.5 (text-davinci-002)",
      "Human evaluation (binary Yes/No ratings with majority voting)"
    ],
    "metrics": [
      "BLEU",
      "ROUGE",
      "BERTScore",
      "Distinct-1",
      "Distinct-2",
      "Weighted F1",
      "Human evaluation: Fluency (binary), Meaning preservation (binary), Quality (binary)"
    ],
    "calculation": "BLEU, ROUGE, and BERTScore capture semantic similarity to human references. Distinct-n measures lexical diversity as ratio of unique n-grams to total n-grams. For reframing (where up to 3 ground-truth reframes exist), take the maximum score over the three references and report the mean of these maxima. Classification chosen label distributions use soft-label distributions normalized from multiple annotator votes. Human evaluation uses 9 annotations per output with majority voting; an alternative more stringent threshold (7/9) is reported in Appendix F.",
    "interpretation": "Positive model performance is indicated by higher BLEU/ROUGE/BERTScore and higher Distinct-n for lexical diversity; human-eval positive percentages reflect fluency, meaning preservation, and successful removal of the unhelpful pattern. The paper interprets outputs rated positively by majority of annotators as successful. GPT3.5 generally achieves highest fluency and quality in human evaluation; fine-tuned models perform reasonably but vary by metric.",
    "baseline_results": "Automatic evaluation on PATTERN REFRAME test set (Table 2): Generating Unhelpful Thoughts — BART: BLEU 25.3, ROUGE 23.9, BERTScore 89.0, Dist-1 0.021, Dist-2 0.087; T5: BLEU 24.5, ROUGE 24.3, BERTScore 89.1, Dist-1 0.019, Dist-2 0.080; R2C2: BLEU 25.5, ROUGE 24.1, BERTScore 89.2, Dist-1 0.023, Dist-2 0.100; GPT3.5 (100 samples): BLEU 24.9, ROUGE 19.2, BERTScore 88.1, Dist-1 0.196, Dist-2 0.586. Reframing Unhelpful Thoughts — BART: BLEU 69.7, ROUGE 53.1, BERTScore 93.5, Dist-1 0.034, Dist-2 0.223; T5: BLEU 69.9, ROUGE 55.5, BERTScore 93.6, Dist-1 0.039, Dist-2 0.261; R2C2: BLEU 70.0, ROUGE 55.0, BERTScore 93.7, Dist-1 0.036, Dist-2 0.235; GPT3.5 (100 samples): BLEU 51.5, ROUGE 41.2, BERTScore 91.7, Dist-1 0.204, Dist-2 0.633. (Human evaluation percentages and classification results are reported in the paper; RoBERTa achieves >72% on most classification categories except Mislabeling.)",
    "validation": "Validation procedures include held-out validation and test splits (specified sizes), selection of qualified crowdworkers via onboarding and iterative seeding, human evaluation with 9 annotators per generated output using majority voting (also reported results for a 7/9 threshold), and reporting of inter-annotator agreement (Krippendorf's Alpha 0.355 and 0.454 for labeling tasks)."
  },
  "targeted_risks": {
    "risk_categories": [
      "Bias",
      "Safety",
      "Misuse",
      "Fairness",
      "Accuracy",
      "Value Alignment",
      "Governance",
      "Legal Compliance",
      "Societal Impact"
    ],
    "atlas_risks": {
      "risks": [
        {
          "category": "Fairness",
          "subcategory": [
            "Data bias"
          ]
        },
        {
          "category": "Accuracy",
          "subcategory": [
            "Unrepresentative data"
          ]
        },
        {
          "category": "Value Alignment",
          "subcategory": [
            "Incomplete advice",
            "Toxic output"
          ]
        },
        {
          "category": "Misuse",
          "subcategory": [
            "Spreading toxicity",
            "Improper usage"
          ]
        },
        {
          "category": "Governance",
          "subcategory": [
            "Lack of testing diversity"
          ]
        },
        {
          "category": "Legal Compliance",
          "subcategory": [
            "Model usage rights restrictions"
          ]
        },
        {
          "category": "Societal Impact",
          "subcategory": [
            "Impact on affected communities"
          ]
        }
      ]
    },
    "demographic_analysis": "The paper states the dataset uses English-language responses written by workers located in the United States and notes that the examples may not reflect thought patterns and personas across cultures and diverse populations.",
    "harm": [
      "Generation of negative/toxic content from positive/neutral texts (dataset could be used to train systems that produce negative texts).",
      "Potential harm if materials are used without supervision (e.g., increased anxiety or warped understanding of unhelpful thoughts and reframings)."
    ]
  },
  "ethical_and_legal_considerations": {
    "privacy_and_anonymity": "N/A",
    "data_licensing": "N/A",
    "consent_procedures": "N/A",
    "compliance_with_regulations": "N/A"
  }
}