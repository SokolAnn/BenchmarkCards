{
  "benchmark_details": {
    "is_benchmark": true,
    "name": "Spam-T5: Benchmarking Large Language Models for Few-Shot Email Spam Detection",
    "abbreviation": "N/A",
    "overview": "This paper develops a benchmark to evaluate traditional machine learning algorithms and large language models (LLMs) on four widely used spam detection datasets, comparing performance in both full-training and few-shot settings. The paper also introduces Spam-T5, a Flan-T5 model fine-tuned for email spam detection, and provides code and data publicly.",
    "data_type": "text (email and SMS messages)",
    "domains": [
      "Natural Language Processing",
      "Cybersecurity"
    ],
    "languages": [
      "English"
    ],
    "similar_benchmarks": [],
    "resources": [
      "https://github.com/jpmorganchase/llm-email-spam-detection",
      "https://github.com/jpmorganchase/emailspamdetection",
      "https://archive.ics.uci.edu/ml/datasets/sms+spam+collection",
      "https://spamassassin.apache.org/old/publiccorpus/",
      "https://www.cs.cmu.edu/~enron/",
      "https://github.com/huggingface/setfit",
      "http://arxiv.org/abs/1907.11692"
    ]
  },
  "purpose_and_intended_users": {
    "goal": "Develop a benchmark for evaluating traditional machine learning algorithms and LLMs on four popular spam detection datasets and to evaluate models in both supervised (full training set) and few-shot settings; introduce Spam-T5, a Flan-T5 fine-tuned for spam detection.",
    "audience": [
      "Researchers",
      "Industry Practitioners"
    ],
    "tasks": [
      "Text Classification",
      "Binary Classification (Spam vs Ham)"
    ],
    "limitations": "High computational requirements for LLMs and need for specialized hardware; the small Flan-T5 model demonstrated limited generalization and was excluded; SetFit did not achieve a meaningful result on the full Enron training set after 104 hours of training.",
    "out_of_scope_uses": []
  },
  "data": {
    "source": "Four public spam detection datasets: Ling-Spam dataset; SMS Spam Collection; SpamAssassin Public Corpus; Enron Email dataset.",
    "size": "Ling-Spam: 2,893 messages; SMS Spam Collection: 5,574 messages; SpamAssassin Public Corpus: 6,047 messages; Enron Email dataset (used subset): 33,716 emails",
    "format": "N/A",
    "annotation": "Existing dataset labels as provided by the original datasets (each message labeled as 'spam' or 'ham')."
  },
  "methodology": {
    "methods": [
      "Automated metrics",
      "Model-based evaluation",
      "Stratified 5-fold cross-validation (for tf-idf feature selection)",
      "Few-shot evaluation with varying numbers of training samples"
    ],
    "metrics": [
      "F1 Score",
      "Precision",
      "Recall"
    ],
    "calculation": "Metrics (F1, precision, recall) computed on test sets. Full-training experiments used an 80% train / 20% test split. Few-shot experiments report macro-averages across the four datasets for k in {4, 8, 16, 32, 64, 128, 256, Full}.",
    "interpretation": "Higher F1 score indicates better spam detection performance. The paper treats the model with the highest average F1 as best-performing (Spam-T5 reported as best overall). Few-shot robustness and higher sample efficiency are interpreted as advantages of LLMs.",
    "baseline_results": "Full training sets: Spam-T5 average F1 = 0.9742; RoBERTa average F1 = 0.9670; SetFit average F1 = 0.9670; SVM average F1 = 0.9560; XGBoost average F1 = 0.8842. Few-shot mean F1 across shot sizes (macro average): Spam-T5 = 0.7498; RoBERTa = 0.6253; SetFit = 0.7112. (See paper Tables 3, 4, 5 for per-dataset and per-shot details.)",
    "validation": "Used 80%/20% train/test splits for experiments. Stratified 5-fold cross-validation was used to tune number of tf-idf features for baseline models. Few-shot evaluation performed by varying training sample counts and reporting macro-averages across datasets."
  },
  "targeted_risks": {
    "risk_categories": [
      "Robustness",
      "Misuse",
      "Societal Impact",
      "Accuracy",
      "Fairness"
    ],
    "atlas_risks": {
      "risks": [
        {
          "category": "Robustness",
          "subcategory": [
            "Evasion attack"
          ]
        },
        {
          "category": "Misuse",
          "subcategory": [
            "Improper usage"
          ]
        },
        {
          "category": "Societal Impact",
          "subcategory": [
            "Impact on the environment"
          ]
        },
        {
          "category": "Accuracy",
          "subcategory": [
            "Unrepresentative data"
          ]
        },
        {
          "category": "Fairness",
          "subcategory": [
            "Data bias"
          ]
        }
      ]
    },
    "demographic_analysis": null,
    "harm": [
      "Potential misuse for censorship",
      "Environmental impact (energy consumption and carbon emissions)"
    ]
  },
  "ethical_and_legal_considerations": {
    "privacy_and_anonymity": "N/A",
    "data_licensing": "N/A",
    "consent_procedures": "N/A",
    "compliance_with_regulations": "N/A"
  }
}