{
  "benchmark_details": {
    "is_benchmark": true,
    "name": "An Annotated Dataset for Explainable Interpersonal Risk Factors of Mental Disturbance in Social Media Posts",
    "abbreviation": "N/A",
    "overview": "We construct and release a new annotated dataset with human-labelled explanations and classification of Interpersonal Risk Factors (IRF) affecting mental disturbance on social media: (i) Thwarted Belongingness (TBE), and (ii) Perceived Burdensomeness (PBU).",
    "data_type": "text (Reddit social media posts) with extractive explanations (text-span annotations) and binary labels",
    "domains": [
      "Natural Language Processing",
      "Healthcare",
      "Mental Health"
    ],
    "languages": [
      "English"
    ],
    "similar_benchmarks": [
      "SDCNL dataset",
      "Suicide Notes"
    ],
    "resources": [
      "https://github.com/drmuskangarg/Irf",
      "https://arxiv.org/abs/2305.18727"
    ]
  },
  "purpose_and_intended_users": {
    "goal": "To construct and release an annotated Reddit dataset for discovering interpersonal risk factors through human-annotated extractive explanations (text-spans) and binary labels for Thwarted Belongingness (TBE) and Perceived Burdensomeness (PBU) to support research and applications in mental health screening and explainable NLP.",
    "audience": [
      "Social NLP research community",
      "Clinical psychologists and mental health practitioners",
      "Machine learning researchers and model developers"
    ],
    "tasks": [
      "Text Classification",
      "Binary Classification",
      "Extractive Explanation (text-span extraction)"
    ],
    "limitations": "There might be linguistic discrepancies between Reddit users and Twitter users; social media users may intentionally post such thoughts to gain attention but for simplicity the authors assume posts are credible; annotation is subjective and interpretation about wellness dimensions may vary between annotators.",
    "out_of_scope_uses": [
      "Clinical diagnosis"
    ]
  },
  "data": {
    "source": "Collected Reddit posts from r/depression and r/SuicideWatch via PRAW API from 02 December 2021 to 04 January 2022 combined with 1,896 posts from the SDCNL dataset; final corpus filtered and limited to posts up to 300 words resulting in 3,522 posts.",
    "size": "3,522 posts",
    "format": "N/A",
    "annotation": "Manual annotation by three trained postgraduate annotators guided and validated by a team of three experts (clinical psychologist, rehabilitation counselor, social NLP expert); majority voting for binary labels <TBE,PBU>; extractive text-span explanations provided for positive instances; inter-annotator agreement reported via Fliess' Kappa (78.83% for TBE, 82.39% for PBU) and group annotation agreement metrics."
  },
  "methodology": {
    "methods": [
      "Automated model evaluation (RNNs: LSTM, GRU; Transformer-based models: BERT, RoBERTa, DistilBERT, MentalBERT; OpenAI embeddings + classifiers: Logistic Regression, Random Forest, SVM, MLP, XGBoost)",
      "Explainability evaluation using LIME and SHAP compared against ground-truth explanations",
      "Human annotation with majority-vote aggregation"
    ],
    "metrics": [
      "Precision",
      "Recall",
      "F1-score",
      "Accuracy",
      "ROUGE Precision/Recall/F1 (for comparing extracted explanations to ground truth)",
      "Pearson Correlation Coefficient (PCC)",
      "Fliess' Kappa (inter-annotator agreement)"
    ],
    "calculation": "All classification metrics (Precision/Recall/F1/Accuracy) computed on the test set (30% of dataset). ROUGE scores used to compare top keywords/text-spans from LIME/SHAP to ground-truth explanations. Fliess' Kappa used to compute inter-annotator agreement. Pearson Correlation Coefficient computed between TBE and PBU labels.",
    "interpretation": "Higher Precision/Recall/F1/Accuracy indicate better classification performance; ROUGE P/R/F measure overlap between generated explanations and ground-truth text-spans; authors note moderate model performance and emphasize the need to reduce false negatives for practical mental health triaging; high recall in explainability indicates ability to identify relevant text-spans but with scope to reduce superfluous spans.",
    "baseline_results": "MentalBERT achieved F1-scores of 76.73% (TBE) and 62.77% (PBU). OpenAI embeddings + Logistic Regression achieved F1-score 81.23% (TBE). OpenAI embeddings + SVM achieved F1-score 76.90% (PBU). (See Table 3 in paper for full precision, recall, F1, and accuracy per model.)",
    "validation": "Dataset split into train/validation/test with test set = 30% of data. Grid search used for hyperparameter optimization. Inter-annotator agreement validated using Fliess' Kappa and group annotation agreement metrics for extractive explanations."
  },
  "targeted_risks": {
    "risk_categories": [
      "Privacy",
      "Fairness",
      "Accuracy",
      "Misuse"
    ],
    "atlas_risks": {
      "risks": [
        {
          "category": "Privacy",
          "subcategory": [
            "Personal information in data"
          ]
        },
        {
          "category": "Fairness",
          "subcategory": [
            "Data bias"
          ]
        },
        {
          "category": "Accuracy",
          "subcategory": [
            "Poor model accuracy"
          ]
        },
        {
          "category": "Misuse",
          "subcategory": [
            "Improper usage"
          ]
        }
      ]
    },
    "demographic_analysis": "N/A",
    "harm": "We recognize the huge impact of false negatives in practical use of applications such as mental health triaging."
  },
  "ethical_and_legal_considerations": {
    "privacy_and_anonymity": "Users' IDs are anonymized. All sample posts shown are anonymized, obfuscated, and paraphrased for user privacy and to prevent misuse. The authors state that this study does not require ethical approval.",
    "data_licensing": "N/A",
    "consent_procedures": "N/A",
    "compliance_with_regulations": "N/A"
  }
}