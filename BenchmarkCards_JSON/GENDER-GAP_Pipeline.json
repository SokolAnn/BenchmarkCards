{
  "benchmark_details": {
    "name": "GENDER-GAP Pipeline",
    "overview": "An automatic pipeline to characterize gender representation in large-scale datasets for 55 languages, using a multilingual lexicon of gendered person-nouns.",
    "data_type": "N/A",
    "domains": null,
    "languages": [
      "English",
      "Finnish",
      "Zulu",
      "Vietnamese",
      "Ganda",
      "Japanese",
      "Lithuanian",
      "Arab Modern Standard Arabic",
      "Assamese",
      "Belarusian",
      "Bengali",
      "Bulgarian",
      "Catalan",
      "Czech",
      "Central Kurdish",
      "Mandarin Chinese",
      "Welsh",
      "Danish",
      "German",
      "Greek",
      "Irish",
      "Hindi",
      "Hungarian",
      "Indonesian",
      "Italian",
      "Japanese",
      "Georgian",
      "Halh Mongolian",
      "Kyrgyz",
      "Lithuanian",
      "Ganda",
      "Standard Latvian",
      "Marathi",
      "Maltese",
      "Dutch",
      "Eastern Panjabi",
      "Western Persian",
      "Polish",
      "Portuguese",
      "Romanian",
      "Russian",
      "Slovak",
      "Slovenian",
      "Spanish",
      "Swedish",
      "Swahili",
      "Tamil",
      "Thai",
      "Turkish",
      "Ukrainian",
      "Urdu",
      "Northern Uzbek",
      "Vietnamese",
      "Yue Chinese"
    ],
    "similar_benchmarks": null,
    "resources": [
      "https://github.com/facebookresearch/ResponsibleNLP/tree/main/gender_gap_pipeline"
    ]
  },
  "purpose_and_intended_users": {
    "goal": "To quantify gender representation bias of multilingual texts using lexical matching as a proxy.",
    "audience": [
      "NLP practitioners",
      "Researchers in gender bias in AI"
    ],
    "tasks": [
      "Characterizing gender representation in datasets",
      "Quantitative reporting of gender biases"
    ],
    "limitations": null,
    "out_of_scope_uses": null
  },
  "data": {
    "source": "Collected from various datasets including Common Crawl, FLORES-200, and NTREX-128.",
    "size": "N/A",
    "format": "N/A",
    "annotation": "N/A"
  },
  "methodology": {
    "methods": [
      "Lexical matching",
      "Word segmentation using Stanza tokenizer"
    ],
    "metrics": [
      "Gender-class scores",
      "Distribution of gendered nouns"
    ],
    "calculation": "Scores are defined by counting occurrences of gendered nouns and dividing by the total number of words.",
    "interpretation": "Provides a depiction of gender representation in datasets.",
    "baseline_results": null,
    "validation": "N/A"
  },
  "targeted_risks": {
    "risk_categories": [
      "Data bias",
      "Underrepresentation of genders in datasets"
    ],
    "atlas_risks": {
      "risks": [
        {
          "category": "Fairness",
          "subcategory": [
            "Data bias"
          ]
        },
        {
          "category": "Societal Impact",
          "subcategory": [
            "Impact on affected communities"
          ]
        }
      ]
    },
    "demographic_analysis": "Analysis covers 55 languages with attention to gender representation.",
    "harm": "Potential misrepresentation or bias in NLP systems due to conducted analysis."
  },
  "ethical_and_legal_considerations": {
    "privacy_and_anonymity": "N/A",
    "data_licensing": "N/A",
    "consent_procedures": "N/A",
    "compliance_with_regulations": "N/A"
  }
}