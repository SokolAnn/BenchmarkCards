{
  "benchmark_details": {
    "is_benchmark": true,
    "name": "MGTBench: Benchmarking Machine-Generated Text Detection",
    "abbreviation": "MGTBench",
    "overview": "The paper proposes the first benchmark framework for machine-generated text (MGT) detection against powerful large language models (LLMs), named MGTBench. MGTBench is a modular framework (input, detection, evaluation) that integrates multiple metric-based and model-based detection methods, generates LLM-produced texts on existing human-written datasets using representative LLMs, and provides standardized evaluation (including robustness/adversarial testing) to compare detection and attribution methods.",
    "data_type": "text (human-written texts and machine-generated texts)",
    "domains": [
      "Natural Language Processing"
    ],
    "languages": [],
    "similar_benchmarks": [
      "TURINGBENCH",
      "Ghostbuster"
    ],
    "resources": [
      "https://github.com/xinleihe/MGTBench",
      "https://arxiv.org/abs/2303.14822"
    ]
  },
  "purpose_and_intended_users": {
    "goal": "Provide a benchmarking framework (MGTBench) to evaluate machine-generated text detection and text attribution methods against powerful LLMs, and to assess robustness under adversarial attacks.",
    "audience": [
      "Researchers",
      "Users"
    ],
    "tasks": [
      "Text Classification",
      "Text Attribution"
    ],
    "limitations": "Choice of LLM/Methods/Datasets. The study concentrates on 6 representative LLMs and 13 detection methods on 3 benchmarking datasets.",
    "out_of_scope_uses": []
  },
  "data": {
    "source": "Datasets: Essay, WP, and Reuters provided by Verma et al. [36]. For each dataset entry, human-written text (HWT) and machine-generated texts (MGTs) generated by six LLMs (ChatGLM, Dolly, ChatGPT-turbo, GPT4All, StableLM, and Claude) are included.",
    "size": "Essay: 1,000 examples; WP: 1,000 examples; Reuters: 1,000 examples",
    "format": "N/A",
    "annotation": "Texts are labeled as HWT (human-written text) and MGT (machine-generated text); MGT labels are assigned via generation from the six LLMs."
  },
  "methodology": {
    "methods": [
      "Automated metrics (metric-based detection)",
      "Model-based evaluation (classification models)",
      "Ablation studies",
      "Adversarial robustness testing"
    ],
    "metrics": [
      "Accuracy",
      "Precision",
      "Recall",
      "F1-score",
      "Area Under ROC Curve (AUC)"
    ],
    "calculation": "Standard classification metrics (Accuracy, Precision, Recall, F1-score, AUC) computed at sample level; F1-score is used as the main evaluation metric unless otherwise noted.",
    "interpretation": "Higher F1-score indicates better detection/attribution. The paper reports that LM Detector outperforms other methods; larger text length (around 200 words) generally yields better detection performance; metric-based methods show better transferability across LLMs while model-based methods adapt better across datasets; robustness is measured by F1-score degradation under adversarial attacks.",
    "baseline_results": "Example results reported: LM Detector achieves 0.993 F1-score while Log-Likelihood achieves 0.968 F1-score to differentiate human vs ChatGPT-turbo on Essay. Log-Likelihood reaches 0.970, 0.980, and 0.972 F1-score on Essay, WP, and Reuters respectively for ChatGLM-generated texts vs HWTs (as reported in Table 2).",
    "validation": "Entries are randomly split with 80% for training and 20% for testing. Evaluations are performed across three datasets and six LLMs; ablation studies (text length, training sample size), transfer experiments (train/test dataset and train/test LLM shifts), and adversarial attack evaluations (paraphrasing, random spacing, adversarial perturbation) are conducted for validation."
  },
  "targeted_risks": {
    "risk_categories": [
      "Fairness",
      "Accuracy",
      "Robustness",
      "Misuse",
      "Legal Compliance",
      "Explainability",
      "Societal Impact"
    ],
    "atlas_risks": {
      "risks": [
        {
          "category": "Fairness",
          "subcategory": [
            "Data bias"
          ]
        },
        {
          "category": "Accuracy",
          "subcategory": [
            "Poor model accuracy"
          ]
        },
        {
          "category": "Robustness",
          "subcategory": [
            "Evasion attack"
          ]
        },
        {
          "category": "Misuse",
          "subcategory": [
            "Spreading disinformation",
            "Dangerous use"
          ]
        },
        {
          "category": "Legal Compliance",
          "subcategory": [
            "Legal accountability"
          ]
        },
        {
          "category": "Explainability",
          "subcategory": [
            "Untraceable attribution"
          ]
        },
        {
          "category": "Transparency",
          "subcategory": [
            "Uncertain data provenance"
          ]
        },
        {
          "category": "Societal Impact",
          "subcategory": [
            "Impact on education: plagiarism"
          ]
        }
      ]
    },
    "demographic_analysis": null,
    "harm": [
      "Spreading misinformation / disinformation",
      "Academic misuse in education (making fair judgment difficult / plagiarism)",
      "Phishing and scam facilitation"
    ]
  },
  "ethical_and_legal_considerations": {
    "privacy_and_anonymity": "N/A",
    "data_licensing": "N/A",
    "consent_procedures": "N/A",
    "compliance_with_regulations": "N/A"
  }
}