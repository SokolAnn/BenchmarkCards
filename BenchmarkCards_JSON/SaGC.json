{
  "benchmark_details": {
    "is_benchmark": true,
    "name": "Situational awareness for Goal Classification in robotic tasks (SaGC)",
    "abbreviation": "SaGC",
    "overview": "A dataset to evaluate situational-awareness uncertainty in robotic tasks, consisting of pairs of high-level user commands, scene descriptions (including objects, floorplan, people, and robot capability), and labels of command type (clear, ambiguous, or infeasible).",
    "data_type": "high-level command and scene description pairs (text)",
    "domains": [
      "Robotics",
      "Natural Language Processing"
    ],
    "languages": [
      "English"
    ],
    "similar_benchmarks": [],
    "resources": [
      "https://clararobot.github.io"
    ]
  },
  "purpose_and_intended_users": {
    "goal": "To evaluate whether language models can distinguish user commands as clear, ambiguous, or infeasible in the context of interactive robotic agents by providing situational context (scene and robot capabilities) and to enable disambiguation via question generation.",
    "audience": [
      "Robotics Researchers",
      "ML Researchers"
    ],
    "tasks": [
      "Text Classification",
      "Question Generation",
      "Instruction Understanding"
    ],
    "limitations": "Relies on few-shot/zero-shot capabilities of LLMs (no fine-tuning); sampling-based approach has computational cost and speed limitations; calibration requires a subset of clear samples; weakness under partially observable environments; SaGC may be biased because it was formulated via a large language model (gpt-3.5-turbo).",
    "out_of_scope_uses": []
  },
  "data": {
    "source": "Generated using gpt-3.5-turbo from crafted prompts (105 crafted examples expanded by LLM) and validated by four human validators who could accept, discard, or change samples; includes 15 scenes across three robot categories (cooking, cleaning, massage).",
    "size": "5,222 examples (1,749 certain, 1,560 ambiguous, 1,917 infeasible)",
    "format": "JSON (records with fields 'scene', 'goal', 'label', 'task')",
    "annotation": "Automatically generated by gpt-3.5-turbo and then validated by four human validators who could discard samples, change labels, or accept samples (approximately 10% of labels changed after validation)."
  },
  "methodology": {
    "methods": [
      "Automated metrics (AUROC, Accuracy, F1 Score)",
      "Human validation (four validators for dataset labeling)"
    ],
    "metrics": [
      "Area Under ROC Curve (AUROC)",
      "Accuracy",
      "F1 Score"
    ],
    "calculation": "AUROC measured between 'certain' and 'uncertain' (ambiguous+infeasible) goals for uncertainty quantification; Accuracy measured for three-way classification (clear/ambiguous/infeasible); F1 Score measured for question-generation detection where ambiguous = positive and unambiguous = negative (see Appendix confusion matrix).",
    "interpretation": "Higher AUROC indicates better separation of certain vs uncertain goals; higher Accuracy indicates better three-way classification of command types; higher F1 Score indicates better precision/recall balance for generating questions only on ambiguous commands.",
    "baseline_results": "On the SaGC dataset (reported in Table I): Uncertainty quantification (AUROC) - Ours: LLaMA 0.725, GPT3.5 0.710, InstructGPT 0.870. Baselines (Quan.Entropy / NE / SE / LS) reported as: Entropy 0.714 / - / 0.861; NE 0.736 / - / 0.867; SE 0.700 / - / 0.862; LS 0.690 / 0.628 / 0.852 (per LLaMA / GPT3.5 / InstructGPT columns where available). Classification accuracy (three-way) - IM [4]: 0.368 / 0.480 / 0.513; CLAMâ€  [22]: 0.362 / 0.376 / 0.532; Ours: 0.447 / 0.556 / 0.710 (per LLaMA / GPT3.5 / InstructGPT columns).",
    "validation": "Dataset samples were reviewed by four validators who could discard, change the label, or accept samples; approximately 10% of labels changed after validation. Experimental validation includes simulated pick-and-place and real-world demonstrations evaluating AUROC, Accuracy, and F1 against baselines."
  },
  "targeted_risks": {
    "risk_categories": [
      "Bias",
      "Robustness"
    ],
    "atlas_risks": {
      "risks": [
        {
          "category": "Fairness",
          "subcategory": [
            "Data bias"
          ]
        },
        {
          "category": "Robustness",
          "subcategory": [
            "Hallucination"
          ]
        }
      ]
    },
    "demographic_analysis": null,
    "harm": [
      "Reduce robot malfunctions",
      "Prevent undesired robotic actions"
    ]
  },
  "ethical_and_legal_considerations": {
    "privacy_and_anonymity": "N/A",
    "data_licensing": "N/A",
    "consent_procedures": "N/A",
    "compliance_with_regulations": "N/A"
  }
}