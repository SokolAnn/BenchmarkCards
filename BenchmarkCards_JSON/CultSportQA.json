{
  "benchmark_details": {
    "is_benchmark": true,
    "name": "CultSportQA",
    "abbreviation": "N/A",
    "overview": "CultSportQA is a comprehensive benchmark designed to evaluate language modelsâ€™ understanding of traditional sports across 60 countries and 6 continents, encompassing four distinct cultural categories. It contains 33,000 multiple-choice questions (MCQs) across text and image modalities, categorized into history-based, rule-based, and scenario-based types.",
    "data_type": "question-answering pairs",
    "domains": [
      "Natural Language Processing"
    ],
    "languages": [
      "English",
      "Chinese",
      "Hindi",
      "Bengali",
      "Arabic",
      "Amharic",
      "Thai",
      "Indonesian",
      "Urdu",
      "French",
      "German",
      "Italian"
    ],
    "similar_benchmarks": [
      "SportQA",
      "SportU"
    ],
    "resources": [
      "https://github.com/M-Groot7/CultSportQA"
    ]
  },
  "purpose_and_intended_users": {
    "goal": "The primary objective of the CultSportQA benchmark is to assess AI models' understanding of culturally significant traditional sports.",
    "audience": [
      "ML Researchers",
      "Industry Practitioners",
      "Model Developers",
      "Domain Experts"
    ],
    "tasks": [
      "Question Answering"
    ],
    "limitations": "While this study represents one of the most comprehensive evaluations of language models in the context of traditional sports, it acknowledges limitations such as the limited geographic scope and representation of traditional sports.",
    "out_of_scope_uses": []
  },
  "data": {
    "source": "Data sources include Wikipedia, National Heritage and Sports Boards, Local Sports Blogs, Cultural Journals, News Outlets, and Academic Publications.",
    "size": "33,000 questions",
    "format": "JSON",
    "annotation": "Questions were manually created and verified by native language experts from the targeted countries."
  },
  "methodology": {
    "methods": [
      "Automated metrics",
      "Human evaluation"
    ],
    "metrics": [
      "Accuracy"
    ],
    "calculation": "Metrics were calculated based on model performance on the dataset across different modalities and question types.",
    "interpretation": "High performance indicates a language model's strong understanding of traditional sports and cultural contexts.",
    "baseline_results": null,
    "validation": "The benchmark was validated through a rigorous multi-step process involving domain experts and cultural knowledge."
  },
  "targeted_risks": {
    "risk_categories": [
      "Bias",
      "Cultural Sensitivity",
      "Inclusivity"
    ],
    "atlas_risks": {
      "risks": []
    },
    "demographic_analysis": "The dataset includes a wide range of languages and cultural backgrounds to ensure diversity.",
    "harm": "N/A"
  },
  "ethical_and_legal_considerations": {
    "privacy_and_anonymity": "N/A",
    "data_licensing": "N/A",
    "consent_procedures": "N/A",
    "compliance_with_regulations": "N/A"
  }
}