{
  "benchmark_details": {
    "is_benchmark": true,
    "name": "SpokenWOZ: A Large-Scale Speech-Text Benchmark for Spoken Task-Oriented Dialogue Agents",
    "abbreviation": "SpokenWOZ",
    "overview": "SpokenWOZ is a large-scale speech-text dataset for spoken task-oriented dialogue (TOD), containing 8 domains, 203k turns, 5.7k dialogues and 249 hours of audios from human-to-human spoken conversations. It incorporates spoken characteristics such as word-by-word processing, ASR noise, and reasoning in spoken language, and introduces cross-turn slot detection and reasoning slot detection as new challenges. The dataset, code, and leaderboard are available at https://spokenwoz.github.io/.",
    "data_type": "Speech and text (audio files and ASR transcriptions; annotated dialogue text)",
    "domains": [
      "Natural Language Processing",
      "Human-Computer Interaction"
    ],
    "languages": [
      "English"
    ],
    "similar_benchmarks": [
      "MultiWOZ",
      "DSTC2",
      "DSTC10",
      "ATIS",
      "EVI",
      "RADDLE",
      "CGoDial",
      "SSTOD",
      "M2M",
      "KVRET",
      "ABCD",
      "SGD"
    ],
    "resources": [
      "https://spokenwoz.github.io/",
      "https://arxiv.org/abs/2305.13040",
      "https://creativecommons.org/licenses/by-nc/4.0/legalcode",
      "https://www.alibabacloud.com/help/en/intelligent-speech-interaction/latest/recording-file-recognition"
    ]
  },
  "purpose_and_intended_users": {
    "goal": "To provide a large-scale, human-to-human, speech-text task-oriented dialogue dataset that reflects realistic spoken conversation characteristics (word-by-word processing, ASR noise, reasoning) and to introduce new challenges (cross-turn slot detection and reasoning slot detection) for developing and evaluating spoken TOD systems.",
    "audience": [
      "ML Researchers",
      "Model Developers"
    ],
    "tasks": [
      "Dialogue State Tracking",
      "Policy Optimization",
      "End-to-end Modeling",
      "Response Generation"
    ],
    "limitations": "Audio data from real conversations was not used due to privacy concerns; for the \"profile\" domain personal information is generated by script and may not be realistic; the dataset ontology is limited and the benchmark is intended primarily for research rather than deployment in real applications.",
    "out_of_scope_uses": [
      "Deployment in realistic applications"
    ]
  },
  "data": {
    "source": "Collected from 250 participants via phone calls (human-to-human) using an online database; annotations inherit and expand the MultiWOZ ontology. Audio was recorded two-track (user and agent). ASR transcriptions were produced by an ASR tool and annotators transcribed agent utterances while keeping ASR noise in user utterances.",
    "size": "5,700 dialogues; 203,074 turns; 249 hours of audio. Dataset split: 4,200 train / 500 dev / 1,000 test dialogues. Audio hours by split: 183 hours (train), 22 hours (dev), 44 hours (test). Turns by split: 149,126 (train), 18,384 (dev), 35,564 (test). Tokens by split: 1,672,984 (train), 204,644 (dev), 396,933 (test).",
    "format": "Text and annotations: JSON (MultiWOZ 2.2-like). Audio: WAV files, two-track, 8000 Hz. ASR transcription word-level timestamps provided in JSON 'words' field.",
    "annotation": "Annotation performed by 15 trained annotators (~3 weeks training). Annotators used ASR transcriptions and audio; agent utterances were manually transcribed, user utterances kept ASR noise. Annotation QC: three-step process (script checking, full inspection by annotators, random inspection on 10% of dialogues); annotator qualification tests required; achieved final turn-level annotation accuracy over 97%."
  },
  "methodology": {
    "methods": [
      "Automated metrics",
      "Model-based evaluation (text-modal baselines, dual-modal baselines)",
      "Zero-shot evaluation of large language models (e.g., ChatGPT, InstructGPT)"
    ],
    "metrics": [
      "Joint Goal Accuracy (JGA)",
      "INFORM",
      "SUCCESS",
      "BLEU Score",
      "Combined Score (Comb)",
      "Macro Average Mentioned Slot Accuracy (MAMS Acc)"
    ],
    "calculation": "JGA measures the ratio of turns for which the value of each slot is correct. Combined Score (Comb) is computed as (INFORM + SUCCESS) Ã— 0.5 + BLEU. MAMS Acc is calculated by (i) determining accuracy of each slot excluding instances with 'none' in final turn; (ii) computing macro average accuracy for slots in each category to obtain MAMS Acc.",
    "interpretation": "Higher JGA indicates more accurate dialogue state tracking at the turn level. Combined Score is an overall quality measure combining INFORM, SUCCESS, and BLEU. MAMS Acc reflects difficulty of different slot categories (reasoning, cross-turn, ASR-sensitive, normal).",
    "baseline_results": "DST best Joint Goal Accuracy: 25.65% (SPACE+WavLM align). End-to-end Modeling best SUCCESS: 52.10% (SPACE+WavLM align). Example LLM results: ChatGPT JGA 13.75%. (Results reported in paper tables.)",
    "validation": "Quality control during collection and annotation: qualification tests for participants and annotators; three-step annotation QC including script checking, full inspection, and random inspection on 10% of dialogues with requirement >=97% turn-level accuracy. Dataset split provided and test set (1,000 dialogues) kept for hidden leaderboard evaluation."
  },
  "targeted_risks": {
    "risk_categories": [
      "Robustness",
      "Accuracy",
      "Privacy",
      "Legal Compliance"
    ],
    "atlas_risks": {
      "risks": [
        {
          "category": "Privacy",
          "subcategory": [
            "Personal information in data",
            "Data privacy rights alignment"
          ]
        },
        {
          "category": "Data Laws",
          "subcategory": [
            "Data acquisition restrictions",
            "Data usage restrictions"
          ]
        },
        {
          "category": "Legal Compliance",
          "subcategory": [
            "Generated content ownership and IP"
          ]
        },
        {
          "category": "Accuracy",
          "subcategory": [
            "Poor model accuracy"
          ]
        },
        {
          "category": "Robustness",
          "subcategory": [
            "Hallucination"
          ]
        }
      ]
    },
    "demographic_analysis": "Speaker origins include participants from Canada, Singapore, China, and South Africa; distribution by dialogues and participant percentages provided in Appendix A.1.3 and Table 11.",
    "harm": null
  },
  "ethical_and_legal_considerations": {
    "privacy_and_anonymity": "Participants were informed the collected audio would be used as a public research dataset and signed contracts; ownership and use rights of participant data belong to the authors; authors state they will not disclose which specific participant an audio came from. Annotators' personal information was not collected; annotators signed non-disclosure agreements. Profile personal information in dialogues is synthetically generated by scripts to avoid privacy leakage.",
    "data_licensing": "CC BY-NC 4.0 (Creative Commons Attribution-NonCommercial 4.0 International).",
    "consent_procedures": "Participants who agreed to participate signed a contract; annotators signed non-disclosure agreements. Qualification tests were conducted for participants and annotators prior to collection/annotation.",
    "compliance_with_regulations": "The authors reviewed the legal regulations of four countries/regions (Canada, Singapore, China, South Africa) and chose data collection locations to enable legal open-source release; they state the dataset will be released in a legal manner and in compliance with the MultiWOZ terms where applicable."
  }
}