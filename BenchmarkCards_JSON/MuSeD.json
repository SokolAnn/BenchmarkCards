{
  "benchmark_details": {
    "is_benchmark": true,
    "name": "MuSeD (Multimodal Spanish Dataset for Sexism Detection)",
    "abbreviation": "MuSeD",
    "overview": "MuSeD is a new Multimodal Spanish dataset for Sexism Detection consisting of approximately 11 hours of videos extracted from TikTok and BitChute, with an innovative annotation framework that examines the contributions of textual, vocal, and visual modalities in identifying sexist content.",
    "data_type": "multimodal (video, audio, text)",
    "domains": [
      "Natural Language Processing",
      "Computer Vision"
    ],
    "languages": [
      "Spanish"
    ],
    "similar_benchmarks": [],
    "resources": [
      "https://github.com/lauradegrazia/MuSeD"
    ]
  },
  "purpose_and_intended_users": {
    "goal": "The primary objective is to provide a dataset for detecting sexism in social media videos by combining text, audio, and video modalities.",
    "audience": [
      "ML Researchers",
      "Industry Practitioners",
      "Model Developers",
      "Domain Experts"
    ],
    "tasks": [
      "Sexism Detection"
    ],
    "limitations": "N/A",
    "out_of_scope_uses": []
  },
  "data": {
    "source": "Videos collected from TikTok and BitChute using curated hashtags related to sexism and gender identity.",
    "size": "400 videos (â‰ˆ11 hours)",
    "format": "Video, audio, text",
    "annotation": "Annotated at multiple levels including text, audio, and visual content by a team of trained linguists and an expert moderator."
  },
  "methodology": {
    "methods": [
      "Manual annotation",
      "Automated metrics"
    ],
    "metrics": [
      "Accuracy"
    ],
    "calculation": "Model performance is calculated based on accuracy against labels assigned by human annotators.",
    "interpretation": "A model is considered effective in detecting sexism if it shows high accuracy in correctly identifying sexist vs. non-sexist content.",
    "baseline_results": "N/A",
    "validation": "Inter-annotator agreement was evaluated to ensure the reliability of annotations."
  },
  "targeted_risks": {
    "risk_categories": [
      "Fairness",
      "Privacy",
      "Accuracy"
    ],
    "atlas_risks": {
      "risks": []
    },
    "demographic_analysis": "The dataset includes diverse demographic backgrounds in annotator expertise to minimize bias.",
    "harm": "The dataset aims to identify and prevent the spread of sexism and discrimination in social media content."
  },
  "ethical_and_legal_considerations": {
    "privacy_and_anonymity": "All collected videos were publicly available and shared by their creators.",
    "data_licensing": "N/A",
    "consent_procedures": "Annotators provided informed consent to engage with potentially offensive material.",
    "compliance_with_regulations": "N/A"
  }
}