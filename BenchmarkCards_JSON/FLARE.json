{
  "benchmark_details": {
    "is_benchmark": true,
    "name": "FLARE (Financial Language Understanding And PRediction Evaluation Benchmark)",
    "abbreviation": "FLARE",
    "overview": "A standardized evaluation benchmark for financial LLMs covering financial natural language understanding and prediction tasks. FLARE covers 4 financial NLP tasks with 6 datasets and 1 financial prediction task with 3 datasets (test and validation splits per dataset are provided in Table 4).",
    "data_type": "multimodal (text, tables, time-series)",
    "domains": [
      "Natural Language Processing",
      "Finance"
    ],
    "languages": [
      "English"
    ],
    "similar_benchmarks": [
      "FLUE",
      "BBT-CFLEB"
    ],
    "resources": [
      "https://github.com/chancefocus/PIXIU",
      "https://arxiv.org/abs/2306.05443"
    ]
  },
  "purpose_and_intended_users": {
    "goal": "Provide a standardized evaluation benchmark for financial LLMs covering financial natural language understanding and prediction tasks.",
    "audience": [
      "ML Researchers",
      "Model Developers",
      "Financial Researchers",
      "Industry Practitioners"
    ],
    "tasks": [
      "Financial Sentiment Analysis",
      "News Headline Classification",
      "Named Entity Recognition",
      "Question Answering",
      "Stock Movement Prediction"
    ],
    "limitations": "1. Model and Training Constraints: only present FinMA models up to 30B; FinMA-30B has not been fine-tuned on the full dataset. 2. Complex Task Performance: FinMA struggles with tasks requiring quantitative reasoning (e.g., financial question answering) due to limitations of the LLaMA backbone. 3. Resource Constraints and Generalizability: development is influenced by available resources and handcrafted instructions, potentially affecting model diversity and generalizability. 4. Potential Negative Impacts: possible spread of financial misinformation or unethical market influence.",
    "out_of_scope_uses": []
  },
  "data": {
    "source": "Assembled from open-sourced datasets: Financial Phrase Bank (FPB), FiQA-SA, Headline (Gold news headline), FIN (NER), FinQA, ConvFinQA, BigData22, ACL18, CIKM18. Instruction tuning data (FIT) is created by converting these datasets into instruction-response samples using domain-expert written instructions.",
    "size": "136,609 examples (FIT instruction tuning data)",
    "format": "N/A",
    "annotation": "Original datasets annotated by domain experts (where applicable); instruction templates and instructions written by domain experts to convert raw dataset samples into instruction-response pairs."
  },
  "methodology": {
    "methods": [
      "Automated metrics",
      "Zero-shot evaluation",
      "Few-shot evaluation (5-shot or 20-shot for some models)",
      "Human evaluation (for some baseline results)"
    ],
    "metrics": [
      "Accuracy",
      "F1 Score",
      "Average F1 Score (weighted over categories)",
      "Entity F1 Score",
      "Exact Match Accuracy",
      "Matthews correlation coefficient (MCC)"
    ],
    "calculation": "Sentiment classification (FPB, FiQA-SA): Accuracy and weighted F1 Score. News headline classification: weighted average F1 over nine categories (Avg F1). NER: entity-level F1 score (Entity F1). Question Answering (FinQA, ConvFinQA): Exact Match Accuracy (EM Acc). Stock movement prediction (BigData22, ACL18, CIKM18): Accuracy and Matthews correlation coefficient (MCC).",
    "interpretation": "Higher metric values indicate better performance on the corresponding task/metric. No absolute thresholds for 'good' vs 'poor' performance are provided in the paper; relative comparisons across models are used.",
    "baseline_results": "Table 5 reports baseline results for multiple models (GPT-NeoX, OPT-66B, BLOOM, ChatGPT, GPT-4, BloombergGPT, FinMA variants). Examples: FPB - FinMA-30B Accuracy 0.87, F1 0.88; FiQA-SA - FinMA-30B F1 0.87; Headline - FinMA variants Avg F1 ~0.97; FinQA - GPT-4 EM Acc 0.63 vs FinMA-30B EM Acc 0.11; ConvFinQA - GPT-4 EM Acc 0.76, FinMA-30B EM Acc 0.40. (See Table 5 in paper for full per-model, per-dataset numbers.)",
    "validation": "Validation sets are randomly selected from FIT and used to select the best model checkpoint (validation splits per dataset are provided in Table 4)."
  },
  "targeted_risks": {
    "risk_categories": [
      "Misuse",
      "Societal Impact",
      "Governance",
      "Accuracy"
    ],
    "atlas_risks": {
      "risks": [
        {
          "category": "Misuse",
          "subcategory": [
            "Spreading disinformation"
          ]
        },
        {
          "category": "Societal Impact",
          "subcategory": [
            "Impact on affected communities"
          ]
        },
        {
          "category": "Governance",
          "subcategory": [
            "Lack of testing diversity"
          ]
        },
        {
          "category": "Accuracy",
          "subcategory": [
            "Poor model accuracy"
          ]
        }
      ]
    },
    "demographic_analysis": null,
    "harm": [
      "Spread of financial misinformation",
      "Unethical market influence"
    ]
  },
  "ethical_and_legal_considerations": {
    "privacy_and_anonymity": "N/A",
    "data_licensing": "Per Table 2: FPB: CC BY-SA 3.0; FiQA-SA: Public; Headline: CC BY-SA 3.0; NER: CC BY-SA 3.0; FinQA: MIT License; ConvFinQA: MIT License; BigData22: Public; ACL18: MIT License; CIKM18: Public.",
    "consent_procedures": "N/A",
    "compliance_with_regulations": "N/A"
  }
}