{
  "benchmark_details": {
    "is_benchmark": true,
    "name": "MACRONYM: A Large-Scale Dataset for Multilingual and Multi-Domain Acronym Extraction",
    "abbreviation": "MACRONYM",
    "overview": "We propose a new dataset for multilingual multi-domain Acronym Extraction (AE). Specifically, 27,200 sentences in 6 typologically different languages and 2 domains, i.e., Legal and Scientific, is manually annotated for AE.",
    "data_type": "text (sentence-level annotated sentences with acronym and long-form spans)",
    "domains": [
      "Legal",
      "Scientific"
    ],
    "languages": [
      "English",
      "Danish",
      "Spanish",
      "French",
      "Persian",
      "Vietnamese"
    ],
    "similar_benchmarks": [],
    "resources": [
      "N/A"
    ]
  },
  "purpose_and_intended_users": {
    "goal": "Introduce the first multilingual and multi-domain manually labeled dataset for Acronym Extraction to support research and advanced model development.",
    "audience": [
      "ML Researchers",
      "Model Developers",
      "NLP Researchers"
    ],
    "tasks": [
      "Acronym Extraction"
    ],
    "limitations": "Dataset currently covers 6 languages and 2 domains only; Persian and Vietnamese portions are relatively small; sentence filtering for candidate selection was applied only to English, French, Spanish, and Danish.",
    "out_of_scope_uses": []
  },
  "data": {
    "source": "United Nations Parallel Corpus (UNPC); Europarl corpus; ACL anthology papers (English scientific papers); public computer science M.S./Ph.D. theses in Persian and Vietnamese.",
    "size": "27,200 annotated sentences total; Legal-English: 4,000 sentences; Legal-Spanish: 6,400 sentences; Legal-French: 8,000 sentences; Legal-Danish: 3,000 sentences; Scientific-English: 4,000 sentences; Scientific-Persian: 1,000 sentences; Scientific-Vietnamese: 800 sentences.",
    "format": "N/A",
    "annotation": "Manual annotation by native speakers recruited via Upwork; two annotators per language independently annotated sampled sentences then resolved disagreements to produce the final annotations. Annotators were trained with guidelines and examples; acronyms are required to be single words and long forms annotated must be in the same language as the sentence."
  },
  "methodology": {
    "methods": [
      "Automated evaluation using a rule-based AE system (Veyseh et al., 2021)",
      "Sequence labeling with BIO tagging using fine-tuned transformer-based models (mBERT, XLM-R)",
      "Evaluation in four learning settings: Mono-Lingual Mono-Domain, Mono-Lingual Cross-Domain, Cross-Lingual Mono-Domain, Cross-Lingual Cross-Domain"
    ],
    "metrics": [
      "F1 Score (model performance)",
      "Krippendorff's alpha (inter-annotator agreement) with MASI distance"
    ],
    "calculation": "Inter-annotator agreement computed using Krippendorff's alpha with MASI distance for initial independent annotations. Model performance measured and reported using F1 Score (as shown in Table 2).",
    "interpretation": "Lower F1 scores indicate poorer model performance; the paper interprets substantially lower performance in cross-lingual and cross-domain settings compared to mono-lingual mono-domain as evidence of challenges due to language differences and domain shift. High Krippendorff's alpha indicates annotation quality.",
    "baseline_results": "Table 2 (F1 scores). Mono-Lingual Mono-Domain (Rule-Based / mBERT / XLMR): Legal-English: 16.55 / 61.66 / 62.07; Legal-Spanish: 10.82 / 51.43 / 55.41; Legal-French: 10.05 / 58.77 / 61.14; Legal-Danish: 8.78 / 50.05 / 48.38; Scientific-English: 20.72 / 60.51 / 59.00; Scientific-Persian: 60.59 / 62.41 / 63.10; Scientific-Vietnamese: 53.44 / 58.71 / 59.13. (Full table includes additional cross-domain and cross-lingual settings as reported in the paper.)",
    "validation": "For each language-domain pair, annotated sentences were randomly split into training/development/test with ratios 80/10/10. Hyper-parameters were tuned on development data. Inter-annotator agreement (Krippendorff's alpha with MASI) was computed to assess annotation quality."
  },
  "targeted_risks": {
    "risk_categories": [
      "Accuracy"
    ],
    "atlas_risks": {
      "risks": [
        {
          "category": "Accuracy",
          "subcategory": [
            "Poor model accuracy"
          ]
        }
      ]
    },
    "demographic_analysis": null,
    "harm": null
  },
  "ethical_and_legal_considerations": {
    "privacy_and_anonymity": "N/A",
    "data_licensing": "N/A",
    "consent_procedures": "N/A",
    "compliance_with_regulations": "N/A"
  }
}