{
  "benchmark_details": {
    "is_benchmark": true,
    "name": "CYBER SECEVAL (Cybersecurity Evaluation Suite for Language Models)",
    "abbreviation": "CYBER SECEVAL",
    "overview": "CYBER SECEVAL is a comprehensive benchmark developed to help bolster the cybersecurity of Large Language Models (LLMs) employed as coding assistants, evaluating their propensity to generate insecure code and their compliance when asked to assist in cyberattacks.",
    "data_type": "text",
    "domains": [
      "Cybersecurity",
      "Natural Language Processing"
    ],
    "languages": [
      "English"
    ],
    "similar_benchmarks": [],
    "resources": [
      "https://github.com/facebookresearch/PurpleLlama/tree/main/CybersecurityBenchmarks"
    ]
  },
  "purpose_and_intended_users": {
    "goal": "To provide a robust framework for assessing the cybersecurity risks of LLMs.",
    "audience": [
      "ML Researchers",
      "Cybersecurity Professionals",
      "Model Developers"
    ],
    "tasks": [
      "Insecure Code Detection",
      "Cyberattack Compliance Evaluation"
    ],
    "limitations": "N/A",
    "out_of_scope_uses": []
  },
  "data": {
    "source": "Open source codebases and static analysis rules.",
    "size": "N/A",
    "format": "N/A",
    "annotation": "Automated test case generation and evaluation pipeline."
  },
  "methodology": {
    "methods": [
      "Automated metrics",
      "Static analysis",
      "LLM-based evaluation"
    ],
    "metrics": [
      "Precision",
      "Recall"
    ],
    "calculation": "Metrics calculated using an Insecure Code Detector for insecure coding practices and compliance evaluation against cyberattack prompts.",
    "interpretation": "Higher precision and recall indicate better detection of insecure code and compliance with malicious requests.",
    "baseline_results": "N/A",
    "validation": "Manual inspection of random test case responses for accuracy."
  },
  "targeted_risks": {
    "risk_categories": [
      "Safety",
      "Accuracy"
    ],
    "atlas_risks": {
      "risks": [
        {
          "category": "Fairness",
          "subcategory": [
            "Output bias"
          ]
        },
        {
          "category": "Accuracy",
          "subcategory": [
            "Poor model accuracy"
          ]
        },
        {
          "category": "Misuse",
          "subcategory": [
            "Nonconsensual use",
            "Improper usage"
          ]
        }
      ]
    },
    "demographic_analysis": "N/A",
    "harm": "Evaluation of models that may generate insecure or harmful code."
  },
  "ethical_and_legal_considerations": {
    "privacy_and_anonymity": "N/A",
    "data_licensing": "MIT License",
    "consent_procedures": "N/A",
    "compliance_with_regulations": "N/A"
  }
}