{
  "benchmark_details": {
    "is_benchmark": true,
    "name": "SpatialScore (Spatial Understanding Benchmark)",
    "abbreviation": "N/A",
    "overview": "SpatialScore is the most comprehensive multimodal spatial understanding benchmark, integrating VGBench with data from 11 existing datasets to assess MLLMs' abilities in 3D spatial reasoning through 28K samples across various tasks and modalities.",
    "data_type": "question-answering pairs",
    "domains": [
      "Natural Language Processing",
      "Computer Vision"
    ],
    "languages": [
      "English"
    ],
    "similar_benchmarks": [
      "VGBench",
      "MMVP",
      "RealWorldQA",
      "VSR",
      "SpatialSense",
      "SpatialBench"
    ],
    "resources": [
      "https://haoningwu3639.github.io/SpatialScore"
    ]
  },
  "purpose_and_intended_users": {
    "goal": "To investigate and evaluate the spatial understanding capabilities of existing multimodal large language models (MLLMs) and to provide a rigorous testing framework for future research.",
    "audience": [
      "ML Researchers",
      "Industry Practitioners",
      "Model Developers"
    ],
    "tasks": [
      "3D Spatial Reasoning",
      "Distance Estimation",
      "Object Localization",
      "Positional Relations"
    ],
    "limitations": "N/A",
    "out_of_scope_uses": []
  },
  "data": {
    "source": "Integrated data from 12 spatial understanding datasets including VGBench and various existing benchmarks.",
    "size": "28,000 samples",
    "format": "question-answering pairs",
    "annotation": "Data is extracted and curated from multiple existing datasets, ensuring diversity in spatial understanding tasks."
  },
  "methodology": {
    "methods": [
      "Automated metrics",
      "Model-based evaluation"
    ],
    "metrics": [
      "Accuracy"
    ],
    "calculation": "Accuracy calculated based on correct answers provided by models against the ground truth.",
    "interpretation": "Higher accuracy indicates better spatial understanding abilities in MLLMs.",
    "baseline_results": "Performance of current MLLMs on SpatialScore evaluated across various model parameters.",
    "validation": "All results are validated through extensive evaluations and comparisons with existing MLLMs."
  },
  "targeted_risks": {
    "risk_categories": [
      "Transparency",
      "Fairness",
      "Accuracy"
    ],
    "atlas_risks": {
      "risks": [
        {
          "category": "Accuracy",
          "subcategory": [
            "Unrepresentative data"
          ]
        },
        {
          "category": "Fairness",
          "subcategory": [
            "Decision bias"
          ]
        },
        {
          "category": "Transparency",
          "subcategory": [
            "Lack of training data transparency"
          ]
        }
      ]
    },
    "demographic_analysis": "N/A",
    "harm": "N/A"
  },
  "ethical_and_legal_considerations": {
    "privacy_and_anonymity": "N/A",
    "data_licensing": "N/A",
    "consent_procedures": "N/A",
    "compliance_with_regulations": "N/A"
  }
}