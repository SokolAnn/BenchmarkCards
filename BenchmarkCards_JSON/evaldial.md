# EvalDial

## üìä Benchmark Details

**Name**: EvalDial

**Overview**: EvalDial is an evaluation benchmark designed to measure dialogue hallucination in large vision language models (LVLMs) by prepending hallucinatory dialogues to original test examples.

**Data Type**: Multimodal

**Domains**:
- Visual Question Answering (VQA)
- Image Captioning

**Languages**:
- English

**Similar Benchmarks**:
- ScienceQA
- OKVQA
- GQA
- IconQA
- NoCaps
- Flickr-30K
- WHOOPS

**Resources**:
- [GitHub Repository](https://github.com/dongmean/LVLM_DialHalu)

## üéØ Purpose and Intended Users

**Goal**: To precisely measure the dialogue hallucination of LVLMs and evaluate the robustness of these models in multi-modal tasks.

**Target Audience**:
- Researchers
- Data Scientists
- AI Developers

**Tasks**:
- Measuring LVLM performance under adversarial conditions
- Evaluating hallucination vulnerabilities in LVLMs

**Limitations**: EvalDial does not include responses to the prepended dialogues; answers are generated by LVLMs only.

**Out of Scope Uses**:
- Non-multimodal tasks
- General NLP tasks not involving visual data

## üíæ Data

**Source**: EvalDial benchmark datasets including ScienceQA, OKVQA, GQA, and others.

**Size**: N/A

**Format**: N/A

**Annotation**: Dialogues were crafted based on the relation to image content and context.

## üî¨ Methodology

**Methods**:
- Adversarial Question Generator (AQG)
- Adversarial Instruction Tuning (AIT)

**Metrics**:
- Top-1 accuracy for VQA
- CIDEr score for Image Captioning

**Calculation**: Performance is evaluated based on accuracy drops and CIDEr scores before and after applying dialogues.

**Interpretation**: Higher performance drop indicates greater susceptibility to dialogue hallucination.

**Baseline Results**: Zero-shot performance drops of 37.7% for VQA and 59.6% for Captioning tasks were measured.

**Validation**: Extensive experiments conducted on six vision-language datasets.

## ‚ö†Ô∏è Targeted Risks

**Risk Categories**:
- Dialogue hallucination
- Prediction bias
- Model robustness

**Atlas Risks**:
- **Accuracy**: Poor model accuracy
- **Robustness**: Prompt injection attack
- **Fairness**: Output bias

**Demographic Analysis**: N/A

**Potential Harm**: Potential for generating misleading outputs during user interactions.

## üîí Ethical and Legal Considerations

**Privacy And Anonymity**: Not Applicable

**Data Licensing**: Not Applicable

**Consent Procedures**: Not Applicable

**Compliance With Regulations**: Not Applicable
