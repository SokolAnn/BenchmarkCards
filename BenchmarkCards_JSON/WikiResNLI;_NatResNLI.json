{
  "benchmark_details": {
    "is_benchmark": true,
    "name": "WikiResNLI and NatResNLI",
    "abbreviation": "WikiResNLI; NatResNLI",
    "overview": "The paper proposes two English NLI datasets to probe language models' reasoning with respective readings: WikiResNLI (a controlled synthetic dataset based on analogies/Wikidata) and NatResNLI (a naturally occurring dataset). The benchmarks are designed to encompass various explicit and implicit realizations of \"respectively\" and to evaluate zero-shot and few-shot generalization of LMs.",
    "data_type": "premise-hypothesis pairs (text)",
    "domains": [
      "Natural Language Processing"
    ],
    "languages": [
      "English"
    ],
    "similar_benchmarks": [
      "SNLI",
      "MNLI",
      "ANLI",
      "FEVER-NLI",
      "LingNLI",
      "WANLI",
      "GLUE"
    ],
    "resources": [
      "https://github.com/ruixiangcui/WikiResNLI_NatResNLI",
      "https://sentence.yourdictionary.com/respectively",
      "https://www.dictionary.com/browse/respectively",
      "https://crosstalk.cell.com/blog/how-to-use-respectively-respectfully"
    ]
  },
  "purpose_and_intended_users": {
    "goal": "To investigate whether NLI models can reason with respective readings (explicit and implicit realizations of \"respectively\") in zero-shot and few-shot settings, and to analyze what cues models leverage and how they generalize from synthetic to natural examples.",
    "audience": [
      "Machine Learning Researchers",
      "Natural Language Processing Researchers",
      "Model Developers"
    ],
    "tasks": [
      "Natural Language Inference"
    ],
    "limitations": "Both WikiResNLI and NatResNLI have only one sentence in the premise and do not exhaust all possible and complicated realizations of respective readings. Experiments are English-specific and limited to LMs that can be run with an academic budget.",
    "out_of_scope_uses": []
  },
  "data": {
    "source": "WikiResNLI: synthetic data generated from an analogy dataset (WiQueen) extracted from Wikidata (Garneau et al., 2021). NatResNLI: naturally occurring sentences collected from online resources (sentence.yourdictionary.com, dictionary.com) and a writing-advice blog; premises curated and filtered by the authors.",
    "size": "WikiResNLI EXPLICIT: 2,317 premises (18,536 premise-hypothesis pairs total); development set: 1,312 premise-hypothesis pairs; training set: 1,577 premises (12,616 premise-hypothesis pairs). WikiResNLI IMPLICIT (test set derived by removing \"respectively\"): 451 premises (3,608 premise-hypothesis pairs). NatResNLI: 76 premises and 608 premise-hypothesis pairs. Additional statistics: WikiResNLI EXPLICIT contains 139 different predicates; NatResNLI average premise length 20.1 tokens, hypothesis length 10.1 tokens, average 2.32 conjuncts per sentence (max 4).",
    "format": "N/A",
    "annotation": "WikiResNLI: hypotheses are automatically generated following rule-based templates; for WikiResNLI IMPLICIT, two authors manually annotated 139 predicates for whether they allow a single subject predicating conjunction of two objects to exclude ambiguous predicates. NatResNLI: two authors wrote hypotheses; annotations collected via Amazon Mechanical Turk with five workers per premise-hypothesis pair; majority vote adopted as final labels; inter-annotator agreement (Fleiss' kappa) = 0.65."
  },
  "methodology": {
    "methods": [
      "Zero-shot evaluation",
      "Few-shot fine-tuning",
      "Fine-tuning pretrained encoder models (RoBERTa, ALBERT, DeBERTa) on NLI corpora",
      "In-context learning (LLaMA, FLAN-T5, GPT-JT)",
      "Human annotation (Amazon Mechanical Turk) for NatResNLI"
    ],
    "metrics": [
      "Accuracy",
      "Fleiss' kappa",
      "Label distribution percentages"
    ],
    "calculation": "Accuracy reported as percentage of correct predictions. Inter-annotator agreement for NatResNLI measured by Fleiss' kappa (0.65). For NatResNLI, majority vote of five annotators per pair is adopted as final label.",
    "interpretation": "Higher accuracy indicates better model reasoning with respective readings. The paper notes random-guessing baselines depend on label setup (three-way baseline 33.3%; for tasks restricted to entailment/contradiction baseline becomes 50%). Fleiss' kappa of 0.65 indicates moderate agreement among annotators for NatResNLI.",
    "baseline_results": "Zero-shot on WikiResNLI EXPLICIT: best model (DeBERTa) achieves 35% accuracy when fine-tuned with MNLI and can reach 68.5% when fine-tuned with nearly all NLI training datasets. Zero-shot on WikiResNLI IMPLICIT: DeBERTa achieves 43.7% when fine-tuned with all NLI corpora (about 10% above chance). NatResNLI performance is generally lower due to domain drift; models can surpass 95% after 32-shot training in some settings while pre-assigned labels match humans at ~90% (see paper for detailed per-condition numbers).",
    "validation": "NatResNLI labels were validated via human annotation on Amazon Mechanical Turk (five annotators per pair); majority vote used as final label; Fleiss' kappa reported as 0.65. For WikiResNLI IMPLICIT, two authors manually annotated predicates to exclude ambiguous cases when constructing the test set."
  },
  "targeted_risks": {
    "risk_categories": [],
    "atlas_risks": {
      "risks": null
    },
    "demographic_analysis": "N/A",
    "harm": "N/A"
  },
  "ethical_and_legal_considerations": {
    "privacy_and_anonymity": "N/A",
    "data_licensing": "N/A",
    "consent_procedures": "N/A",
    "compliance_with_regulations": "N/A"
  }
}