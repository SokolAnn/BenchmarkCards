{
  "benchmark_details": {
    "is_benchmark": true,
    "name": "QASC (Question Answering via Sentence Composition)",
    "abbreviation": "QASC",
    "overview": "A multi-hop reasoning dataset that requires retrieving facts from a large corpus and composing them to answer multiple-choice questions. QASC provides annotated supporting facts (two facts per question) and their composition, and is designed to emphasize retrieval and sentence-level composition for 2-hop science questions.",
    "data_type": "question-answering pairs (text) with annotated supporting facts and a large sentence corpus",
    "domains": [
      "Natural Language Processing",
      "Science Education"
    ],
    "languages": [
      "English"
    ],
    "similar_benchmarks": [
      "OpenBookQA",
      "MultiRC",
      "WikiHop",
      "ComplexWebQuestions",
      "HotPotQA",
      "DROP",
      "WorldTree"
    ],
    "resources": [
      "https://github.com/allenai/qasc",
      "https://arxiv.org/abs/1910.11473",
      "https://www.ck12.org",
      "https://spacy.io/",
      "https://pypi.org/project/spacy-langdetect/",
      "https://github.com/LuminosoInsight/python-ftfy",
      "https://www.elastic.co"
    ]
  },
  "purpose_and_intended_users": {
    "goal": "To present a dataset focused on sentence-level composition for 2-hop multi-hop question answering: provide 8-way multiple-choice science questions with annotated pairs of supporting facts and composed facts to enable development of retrieval and reasoning models.",
    "audience": [
      "Machine Learning Researchers",
      "Natural Language Processing Researchers",
      "Model Developers"
    ],
    "tasks": [
      "Question Answering",
      "Multi-hop Reasoning",
      "Information Retrieval",
      "Entailment (multi-sentence entailment)"
    ],
    "limitations": "Limited to exactly 2-hop reasoning questions; domain limited to elementary and middle-school level science.",
    "out_of_scope_uses": []
  },
  "data": {
    "source": "Questions were crowd-sourced using seed facts FS (selected from WorldTree, a middle-school extension, and CK-12) and a large web-derived corpus FL. Seed facts FS: 928 facts (356 from WorldTree, 123 middle-school extension, 449 from CK-12). Large corpus FL: 17M cleaned sentences produced from processing 73M web documents (281GB) from Clark et al. (2016). Each question is annotated with two supporting facts fS and fL and a composed fact fC.",
    "size": "9,980 questions; 928 seed facts; 17,000,000 sentences (1GB) corpus derived from processing 73,000,000 web documents (281GB). Train/dev/test splits: 8,134 train, 926 dev, 920 test.",
    "format": "N/A",
    "annotation": "Crowdsourced annotation: supporting facts fS and fL and composed fact fC were created by crowdworkers via a multi-step process with server-side quality checks. Questions were validated by 5 crowdworkers (questions answered incorrectly or deemed unanswerable by >=2 workers were dropped). Adversarial distractor validation: 3 annotators filtered candidate distractors for dev/test sets."
  },
  "methodology": {
    "methods": [
      "Two-step information retrieval (IR) over the 17M-sentence corpus",
      "BERT-based multiple-choice QA fine-tuning (BERT-MCQ)",
      "Multi-adversary adversarial distractor selection (model-guided)",
      "Crowdsourcing with automated quality checks"
    ],
    "metrics": [
      "Accuracy",
      "Recall (retrieval recall of gold supporting facts)"
    ],
    "calculation": "Accuracy: proportion of questions answered correctly on dev/test splits. Retrieval recall: percentage of questions for which both annotated supporting facts (fS and fL) appear in the top-M retrieved sentences (reported for top-10 sentences).",
    "interpretation": "Higher Accuracy indicates better QA performance; authors report a human baseline of 93.0% Accuracy. Two-step retrieval substantially increases retrieval Recall (e.g., 44.4% vs 2.9% for single-step when measuring both facts in top-10) and leads to higher QA Accuracy. A gap of ~20 percentage points to human performance indicates remaining challenge.",
    "baseline_results": "Human baseline Accuracy: 93.0%. BERT-MCQ (bert-large-cased) fine-tuned on RACE+SCIquestions with two-step retrieval on FQASC (17M) achieved Dev Accuracy up to 78.0% and Test Accuracy 73.2% (best reported configuration: BERT-LC[WM] two-step, additional fine-tuning on RACE+SCI gave Dev 78.0 / Test 73.2). Simpler non-BERT baselines (GloVe/ESIM variants) performed near random.",
    "validation": "Questions were validated by 5 crowdworkers; those answered incorrectly or marked unanswerable by >=2 workers were dropped (reducing collection to 7,660 before adversarial expansion). For adversarial distractors, 3 annotators marked valid distractors; distractors marked valid by at least one annotator were removed. Automated server-side quality checks enforced constraints during crowdsourcing."
  },
  "targeted_risks": {
    "risk_categories": [],
    "atlas_risks": {
      "risks": null
    },
    "demographic_analysis": "N/A",
    "harm": "N/A"
  },
  "ethical_and_legal_considerations": {
    "privacy_and_anonymity": "N/A",
    "data_licensing": "N/A",
    "consent_procedures": "N/A",
    "compliance_with_regulations": "N/A"
  }
}