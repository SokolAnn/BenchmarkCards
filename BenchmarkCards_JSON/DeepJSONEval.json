{
  "benchmark_details": {
    "is_benchmark": true,
    "name": "DeepJSONEval",
    "abbreviation": "N/A",
    "overview": "DeepJSONEval is a pioneering multilingual deep-nested JSON evaluation benchmark designed to assess LLMs' ability to convert raw text into structured JSON formats with complex nested calculations. It features 2100 instances categorized by difficulty, allowing for robust evaluation in web data mining contexts.",
    "data_type": "JSON instances",
    "domains": [
      "Natural Language Processing"
    ],
    "languages": [
      "English"
    ],
    "similar_benchmarks": [],
    "resources": [
      "https://github.com/GTS-AI-Intra-Lab-SotaS/DeepJSONEval"
    ]
  },
  "purpose_and_intended_users": {
    "goal": "To provide a comprehensive benchmark for assessing the structured output capabilities of large language models in deep nested JSON generation and information extraction tasks.",
    "audience": [
      "ML Researchers",
      "Model Developers",
      "Industry Practitioners"
    ],
    "tasks": [
      "Structured Data Extraction"
    ],
    "limitations": "N/A",
    "out_of_scope_uses": []
  },
  "data": {
    "source": "Diverse web sources collected through systematic text aggregation and schema generation algorithms.",
    "size": "2,100 instances",
    "format": "JSON",
    "annotation": "Ground truth established through expert validation and a human-in-the-loop quality control process."
  },
  "methodology": {
    "methods": [
      "Human evaluation",
      "Automated metrics"
    ],
    "metrics": [
      "Syntax Score",
      "Hierarchical key matching Score",
      "Strict Score"
    ],
    "calculation": "Metrics are calculated based on a multi-dimensional evaluation framework assessing JSON structural correctness, format adherence, and content accuracy.",
    "interpretation": "Higher scores indicate better performance in generating accurate and structured JSON outputs.",
    "baseline_results": null,
    "validation": "Systematic dual review process involving independent evaluation and consensus checks to ensure quality and fidelity."
  },
  "targeted_risks": {
    "risk_categories": [
      "Accuracy",
      "Fairness"
    ],
    "atlas_risks": {
      "risks": [
        {
          "category": "Accuracy",
          "subcategory": [
            "Unrepresentative data"
          ]
        },
        {
          "category": "Fairness",
          "subcategory": [
            "Output bias"
          ]
        }
      ]
    },
    "demographic_analysis": "N/A",
    "harm": []
  },
  "ethical_and_legal_considerations": {
    "privacy_and_anonymity": "N/A",
    "data_licensing": "N/A",
    "consent_procedures": "N/A",
    "compliance_with_regulations": "N/A"
  }
}