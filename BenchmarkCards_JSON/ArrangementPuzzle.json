{
  "benchmark_details": {
    "is_benchmark": true,
    "name": "ArrangementPuzzle",
    "abbreviation": "N/A",
    "overview": "ArrangementPuzzle is a novel puzzle dataset designed to analyze the internal representations of reasoning in large language models (LLMs). It allows for automated stepwise correctness verification and is used for training a classifier model on LLM activations to assess reasoning correctness.",
    "data_type": "structured puzzle statements",
    "domains": [
      "Natural Language Processing"
    ],
    "languages": [
      "English"
    ],
    "similar_benchmarks": [],
    "resources": [
      "https://github.com/Solid-Energy-Systems/arrangement_puzzle",
      "https://github.com/Solid-Energy-Systems/activation_classifier"
    ]
  },
  "purpose_and_intended_users": {
    "goal": "To investigate LLM internal representations of reasoning and provide insights into their reasoning mechanisms.",
    "audience": [
      "ML Researchers",
      "AI Developers"
    ],
    "tasks": [
      "Reasoning Analysis"
    ],
    "limitations": "While our study provides insights into the internal representations of reasoning in large language models (LLMs), it has several limitations, including the restricted nature of the dataset and its applicability to real-world reasoning tasks.",
    "out_of_scope_uses": []
  },
  "data": {
    "source": "Custom constructed dataset of puzzles designed for testing LLM reasoning.",
    "size": "10,000 prompts",
    "format": "N/A",
    "annotation": "Manually annotated based on LLM outputs compared to ground-truth solutions."
  },
  "methodology": {
    "methods": [
      "Classifier training",
      "Automated correctness verification"
    ],
    "metrics": [
      "Accuracy"
    ],
    "calculation": "The classifier predicts whether a reasoning step is accurate based on activations from LLMs.",
    "interpretation": "An accuracy above 80% indicates effective differentiation between correct and incorrect reasoning steps.",
    "baseline_results": "N/A",
    "validation": "Evaluation was conducted on 1141 withheld test puzzles."
  },
  "targeted_risks": {
    "risk_categories": [
      "Accuracy"
    ],
    "atlas_risks": {
      "risks": []
    },
    "demographic_analysis": "N/A",
    "harm": []
  },
  "ethical_and_legal_considerations": {
    "privacy_and_anonymity": "N/A",
    "data_licensing": "N/A",
    "consent_procedures": "N/A",
    "compliance_with_regulations": "N/A"
  }
}