{
  "benchmark_details": {
    "is_benchmark": true,
    "name": "3MDBench (Medical Multimodal Multi-agent Dialogue Benchmark)",
    "abbreviation": "3MDBench",
    "overview": "3MDBench is an open-source framework for simulating and evaluating LVLM-driven telemedical consultations, focusing on diagnostic accuracy and dialogue quality between patient and doctor agents. It includes 3013 cases across 34 diagnoses drawn from real-world telemedicine interactions, combining textual and image-based data.",
    "data_type": "dialogue, medical image and text data",
    "domains": [
      "Healthcare"
    ],
    "languages": [
      "English"
    ],
    "similar_benchmarks": [
      "Medical-Diff-VQA",
      "PathVQA",
      "Cholec80-VQA",
      "RadBench"
    ],
    "resources": [
      "https://anonymous.4open.science/r/3mdbench_acl-0511"
    ]
  },
  "purpose_and_intended_users": {
    "goal": "To evaluate LVLMs in telemedicine through simulated and interactive patient-doctor consultations incorporating personality-driven dialogue and diagnostic processes.",
    "audience": [
      "ML Researchers",
      "Medical Practitioners",
      "AI Researchers"
    ],
    "tasks": [
      "Medical Diagnosis",
      "Dialogue Generation",
      "Question Answering"
    ],
    "limitations": "The benchmark is limited to 34 diagnoses which may restrict diagnostic coverage.",
    "out_of_scope_uses": []
  },
  "data": {
    "source": "Constructed from 6 open-source datasets, real-world telemedicine interactions and enriched with text descriptions generated by GPT-4o-mini.",
    "size": "3,013 cases with corresponding images and dialogues",
    "format": "N/A",
    "annotation": "Manual annotation by medical experts and automated quality checks."
  },
  "methodology": {
    "methods": [
      "Human evaluation",
      "Automated metrics",
      "Agent-based evaluation"
    ],
    "metrics": [
      "F1 Score",
      "Accuracy"
    ],
    "calculation": "Calculated using the performance of Agent models compared against human annotations.",
    "interpretation": "Higher F1 scores indicate better diagnostic accuracy and effective dialogue management.",
    "baseline_results": "Results demonstrate improvements in F1 Scores when utilizing dialogue and image modalities.",
    "validation": "Used Wilcoxon signed-rank test for statistical significance of differences in evaluated metrics."
  },
  "targeted_risks": {
    "risk_categories": [
      "Bias",
      "Accuracy"
    ],
    "atlas_risks": {
      "risks": [
        {
          "category": "Accuracy",
          "subcategory": [
            "Unrepresentative data"
          ]
        },
        {
          "category": "Fairness",
          "subcategory": [
            "Data bias"
          ]
        }
      ]
    },
    "demographic_analysis": "The benchmark includes personality-driven analyses which may correlate with demographic factors in patient interactions.",
    "harm": "N/A"
  },
  "ethical_and_legal_considerations": {
    "privacy_and_anonymity": "Data used complies with ethical considerations and is anonymized.",
    "data_licensing": "N/A",
    "consent_procedures": "N/A",
    "compliance_with_regulations": "N/A"
  }
}