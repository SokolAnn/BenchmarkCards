{
  "benchmark_details": {
    "is_benchmark": true,
    "name": "AutoBencher (Declarative Benchmark Construction)",
    "abbreviation": "N/A",
    "overview": "AutoBencher is a declarative framework for automatic benchmark construction, producing datasets to evaluate language models' capabilities and safety. It operationalizes benchmark desiderata like salience, difficulty, separability, and novelty as optimization problems and constructs datasets accordingly.",
    "data_type": "question-answering pairs",
    "domains": [
      "Natural Language Processing"
    ],
    "languages": [
      "English"
    ],
    "similar_benchmarks": [
      "MMLU (Massive Multitask Language Understanding)",
      "XSTest",
      "HarmBench"
    ],
    "resources": [
      "https://github.com/XiangLi1999/AutoBencher.git"
    ]
  },
  "purpose_and_intended_users": {
    "goal": "To efficiently generate datasets that reveal novel insights and model vulnerabilities through automatic benchmark construction.",
    "audience": [
      "ML Researchers",
      "Model Developers",
      "Safety Analysts"
    ],
    "tasks": [
      "Text Classification",
      "Safety Evaluation"
    ],
    "limitations": "N/A",
    "out_of_scope_uses": []
  },
  "data": {
    "source": "Generated by AutoBencher using language models and datasets descriptions.",
    "size": "4,000 examples",
    "format": "JSON",
    "annotation": "Automatically generated with human oversight for correctness and salience."
  },
  "methodology": {
    "methods": [
      "Automated metrics",
      "Human evaluation"
    ],
    "metrics": [
      "Novelty",
      "Difficulty",
      "Separability",
      "Attack Success Rate (ASR)"
    ],
    "calculation": "Metrics are calculated based on model performance on constructed datasets compared to existing benchmarks.",
    "interpretation": "Higher scores in novelty indicate datasets that provide new insights into model performance. Difficulty is assessed based on the error rates of models, while separability measures how well models differ in performance.",
    "baseline_results": "AutoBencher datasets achieve 27% lower rank correlation and 22% more difficulty than existing human-constructed benchmarks like MMLU.",
    "validation": "Human validation is performed to ensure the quality and relevance of generated datasets."
  },
  "targeted_risks": {
    "risk_categories": [
      "Bias",
      "Safety"
    ],
    "atlas_risks": {
      "risks": [
        {
          "category": "Fairness",
          "subcategory": [
            "Data bias"
          ]
        },
        {
          "category": "Robustness",
          "subcategory": [
            "Evasion attack"
          ]
        },
        {
          "category": "Societal Impact",
          "subcategory": [
            "Impact on education: bypassing learning"
          ]
        }
      ]
    },
    "demographic_analysis": "N/A",
    "harm": [
      "Potential for generating harmful prompts that models fail to reject."
    ]
  },
  "ethical_and_legal_considerations": {
    "privacy_and_anonymity": "N/A",
    "data_licensing": "N/A",
    "consent_procedures": "N/A",
    "compliance_with_regulations": "N/A"
  }
}