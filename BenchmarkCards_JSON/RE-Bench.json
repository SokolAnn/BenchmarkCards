{
  "benchmark_details": {
    "is_benchmark": true,
    "name": "RE-Bench (Research Engineering Benchmark, V1)",
    "abbreviation": "RE-Bench",
    "overview": "RE-Bench aims to evaluate whether AI agents can fully automate the work of expert AI R&D researchers, using direct performance comparisons between AI agents and human experts under equivalent conditions and resources.",
    "data_type": "text",
    "domains": [
      "Natural Language Processing"
    ],
    "languages": [
      "English"
    ],
    "similar_benchmarks": [
      "MLE-bench",
      "SWE-bench",
      "GAIA",
      "Sci-code",
      "GPQA"
    ],
    "resources": [
      "https://github.com/METR/ai-rd-tasks",
      "https://transcripts.metr.org"
    ]
  },
  "purpose_and_intended_users": {
    "goal": "To evaluate the AI R&D capabilities of AI agents compared to human experts under equivalent conditions.",
    "audience": [
      "ML Researchers",
      "AI Developers",
      "Policy Makers"
    ],
    "tasks": [
      "Machine Learning Optimization"
    ],
    "limitations": "N/A",
    "out_of_scope_uses": []
  },
  "data": {
    "source": "71 attempts by 61 ML experts completing the tasks under equivalent conditions.",
    "size": "71 experiments",
    "format": "N/A",
    "annotation": "N/A"
  },
  "methodology": {
    "methods": [
      "Direct performance comparison",
      "Expert baseline evaluation"
    ],
    "metrics": [
      "Score@k",
      "Average normalized score"
    ],
    "calculation": "Normalized scores are calculated based on the performance comparison between human experts and AI agents.",
    "interpretation": "Higher scores indicate better performance of the AI agents compared to human experts under equivalent conditions.",
    "baseline_results": "Expert average normalized scores varied, showing significant differences based on experience and environment.",
    "validation": "Cross-validation with multiple iterations and expert evaluations."
  },
  "targeted_risks": {
    "risk_categories": [
      "Bias",
      "Safety",
      "Robustness"
    ],
    "atlas_risks": {
      "risks": [
        {
          "category": "Fairness",
          "subcategory": [
            "Data bias"
          ]
        },
        {
          "category": "Privacy",
          "subcategory": [
            "Personal information in data"
          ]
        }
      ]
    },
    "demographic_analysis": "N/A",
    "harm": [
      "Risk of underestimating AI capabilities in real-world R&D."
    ]
  },
  "ethical_and_legal_considerations": {
    "privacy_and_anonymity": "N/A",
    "data_licensing": "N/A",
    "consent_procedures": "N/A",
    "compliance_with_regulations": "N/A"
  }
}