{
  "benchmark_details": {
    "is_benchmark": true,
    "name": "MIB (Mechanistic Interpretability Benchmark)",
    "abbreviation": "MIB",
    "overview": "MIB is a Mechanistic Interpretability Benchmark that facilitates the evaluation of mechanistic interpretability methods by comparing their ability to locate and validate task mechanisms or specific concepts in neural language models. It features two tracks: circuit localization and causal variable localization, with evaluations spanning four tasks across five models.",
    "data_type": "task-based evaluation datasets",
    "domains": [
      "Natural Language Processing"
    ],
    "languages": [
      "English"
    ],
    "similar_benchmarks": [
      "RA VEL",
      "CausalGym"
    ],
    "resources": [
      "https://huggingface.co/datasets/mib",
      "https://github.com/user/repo"
    ]
  },
  "purpose_and_intended_users": {
    "goal": "To establish stable standards for comparing mechanistic interpretability methods in a principled manner.",
    "audience": [
      "Researcher in AI Safety",
      "ML Researchers",
      "Interpretability Researchers"
    ],
    "tasks": [
      "Circuit Localization",
      "Causal Variable Localization"
    ],
    "limitations": "N/A",
    "out_of_scope_uses": []
  },
  "data": {
    "source": "Tasks involve synthetic datasets composed of well-defined questions",
    "size": "10000 examples for each task",
    "format": "N/A",
    "annotation": "N/A"
  },
  "methodology": {
    "methods": [
      "Automated metrics",
      "Intervention evaluations"
    ],
    "metrics": [
      "Accuracy",
      "Faithfulness"
    ],
    "calculation": "Metrics are calculated using a range of evaluation tasks and results from model outputs.",
    "interpretation": "Higher accuracy and faithfulness scores indicate a better understanding of model mechanisms.",
    "baseline_results": "N/A",
    "validation": "Performance is validated against established models and tasks with set benchmarks."
  },
  "targeted_risks": {
    "risk_categories": [
      "Accuracy",
      "Fairness"
    ],
    "atlas_risks": {
      "risks": [
        {
          "category": "Accuracy",
          "subcategory": [
            "Poor model accuracy"
          ]
        },
        {
          "category": "Fairness",
          "subcategory": [
            "Data bias"
          ]
        }
      ]
    },
    "demographic_analysis": "N/A",
    "harm": []
  },
  "ethical_and_legal_considerations": {
    "privacy_and_anonymity": "N/A",
    "data_licensing": "N/A",
    "consent_procedures": "N/A",
    "compliance_with_regulations": "N/A"
  }
}