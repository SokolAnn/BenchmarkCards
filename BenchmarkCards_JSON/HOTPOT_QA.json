{
  "benchmark_details": {
    "is_benchmark": true,
    "name": "HOTPOT QA",
    "abbreviation": "HOTPOT QA",
    "overview": "HOTPOT QA is a new dataset with 113k Wikipedia-based question-answer pairs with four key features: (1) the questions require finding and reasoning over multiple supporting documents to answer; (2) the questions are diverse and not constrained to any pre-existing knowledge bases or knowledge schemas; (3) sentence-level supporting facts required for reasoning are provided, allowing QA systems to reason with strong supervision and explain predictions; (4) includes a new type of factoid comparison questions to test ability to extract relevant facts and perform necessary comparison.",
    "data_type": "text (question-answering pairs with sentence-level supporting facts)",
    "domains": [
      "Natural Language Processing"
    ],
    "languages": [
      "English"
    ],
    "similar_benchmarks": [
      "SQuAD",
      "TriviaQA",
      "SearchQA",
      "QAngaroo",
      "COMPLEX WEBQUESTIONS",
      "MS MARCO"
    ],
    "resources": [
      "https://HotpotQA.github.io",
      "https://arxiv.org/abs/1809.09600"
    ]
  },
  "purpose_and_intended_users": {
    "goal": "Facilitating the development of QA systems capable of performing explainable, multi-hop reasoning over diverse natural language; providing sentence-level supporting facts and comparison questions to enable explainable predictions and test multi-hop reasoning.",
    "audience": [],
    "tasks": [
      "Question Answering",
      "Supporting Fact Prediction",
      "Information Retrieval"
    ],
    "limitations": "Authors note that approximately 6% of sampled questions can be answered with one paragraph (single-hop) and 2% unanswerable; supporting facts have lower annotator agreement and can be subjective. The authors also note that train-medium contains examples that models could answer with sufficient training and that incorporating supporting-fact supervision in their model is likely suboptimal.",
    "out_of_scope_uses": []
  },
  "data": {
    "source": "Collected by crowdsourcing (Amazon Mechanical Turk) based on English Wikipedia articles (Wikipedia dump of October 1, 2017) using a Wikipedia hyperlink graph and curated candidate paragraph pairs.",
    "size": "112,779 examples",
    "format": "Extracted text and hyperlinks (WikiExtractor output) with sentence boundaries from Stanford CoreNLP",
    "annotation": "Crowdsourced via Amazon Mechanical Turk; workers provided questions, answer spans, and sentence-level supporting facts (supporting fact sentences)"
  },
  "methodology": {
    "methods": [
      "Model-based evaluation (baseline neural QA model with multi-task supporting-fact supervision)",
      "Automated retrieval evaluation (bigram tf-idf retrieval and inverted-index filtering for full wiki setting)",
      "Human evaluation (additional Turkers to establish human performance and supporting facts)"
    ],
    "metrics": [
      "Exact Match (EM)",
      "F1",
      "Supporting facts Exact Match (EM)",
      "Supporting facts F1",
      "Joint Exact Match (Joint EM)",
      "Joint F1",
      "Mean Average Precision (MAP)",
      "Mean Rank",
      "Hits@k"
    ],
    "calculation": "Answer EM and F1 follow SQuAD-style evaluation. Supporting-fact EM and F1 evaluate predicted supporting fact sentences against gold set. Joint metrics computed per example as: P(joint)=P(ans) * P(sup); R(joint)=R(ans) * R(sup); Joint F1 = 2 * P(joint) * R(joint) / (P(joint) + R(joint)). Joint EM is 1 only if both answer and supporting facts achieve exact match; otherwise 0. All metrics are evaluated example-by-example and averaged over examples.",
    "interpretation": "Joint metrics penalize systems that perform poorly on either answer prediction or supporting-fact prediction, emphasizing both correctness and explainability. Lower performance in the full wiki setting indicates greater retrieval difficulty; human performance is substantially higher than baseline models, indicating room for improvement.",
    "baseline_results": "Distractor test (model with strong supervision): Answer EM 45.46, Answer F1 58.99; Supporting-fact EM 22.24, Supporting-fact F1 66.62; Joint EM 12.04, Joint F1 41.37. Full wiki test (model): Answer EM 25.23, Answer F1 34.40; Supporting-fact EM 5.07, Supporting-fact F1 40.69; Joint EM 2.63, Joint F1 17.85. Human (sample of 1,000): Human EM 83.60, Human F1 91.40; Supporting-fact EM 61.50, Supporting-fact F1 90.04; Joint EM 52.30, Joint F1 82.55. (Numbers taken from Tables 4 and 8 in the paper.)",
    "validation": "Data splits created using sampling and three-fold cross validation on multi-hop examples; train-easy identified by sampling top-contributing Turkers and labeling examples that overwhelmingly required single-paragraph reasoning; train-medium identified by model correctly answering 60% of multi-hop examples in cross-validation; remaining hard examples divided into train-hard, dev, test-distractor, and test-fullwiki. Human validation: for 1,000 sampled examples, at least three additional Turkers provided answers and supporting facts to establish human performance and inter-annotator agreement."
  },
  "targeted_risks": {
    "risk_categories": [],
    "atlas_risks": {
      "risks": null
    },
    "demographic_analysis": null,
    "harm": null
  },
  "ethical_and_legal_considerations": {
    "privacy_and_anonymity": "N/A",
    "data_licensing": "N/A",
    "consent_procedures": "N/A",
    "compliance_with_regulations": "N/A"
  }
}