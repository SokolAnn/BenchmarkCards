{
  "benchmark_details": {
    "is_benchmark": true,
    "name": "HumanEvo (An Evolution-aware Benchmark for More Realistic Evaluation of Repository-level Code Generation)",
    "abbreviation": "HumanEvo",
    "overview": "HumanEvo is a novel evolution-aware repository-level code generation benchmark that simulates real-world software development processes by reflecting the dynamic nature of software projects over time. It aims to accurately evaluate the performance of Large Language Models (LLMs) in generating code that aligns with historical project states.",
    "data_type": "programming tasks with dependency levels",
    "domains": [
      "Software Development"
    ],
    "languages": [
      "Python",
      "Java"
    ],
    "similar_benchmarks": [
      "CoderEval",
      "RepoBench",
      "EvoCodeBench",
      "RepoEval"
    ],
    "resources": [
      "https://github.com/DeepSoftwareAnalytics/HumanEvo",
      "https://anonymous.4open.science/r/HumanEvo/"
    ]
  },
  "purpose_and_intended_users": {
    "goal": "To provide a comprehensive evaluation of LLMs' performance in repository-level code generation that incorporates the temporal evolution of software projects.",
    "audience": [
      "ML Researchers",
      "Software Developers",
      "Benchmark Developers"
    ],
    "tasks": [
      "Code Generation"
    ],
    "limitations": "N/A",
    "out_of_scope_uses": []
  },
  "data": {
    "source": "High-quality repositories from GitHub having over 50 common development domains.",
    "size": "400 task instances (200 for Python and 200 for Java)",
    "format": "N/A",
    "annotation": "Manual annotation by experienced programmers, including category labeling and docstring rewriting."
  },
  "methodology": {
    "methods": [
      "Automated execution-based evaluation",
      "Human evaluation"
    ],
    "metrics": [
      "Accuracy",
      "Pass@1"
    ],
    "calculation": "Metrics computed based on the execution correctness of generated code.",
    "interpretation": "Higher accuracy indicates better performance of LLMs in generating contextually relevant code.",
    "baseline_results": "Performance comparisons against traditional evolution-ignored methods showing discrepancies in results.",
    "validation": "Rigorous execution-based validation ensuring task instances are covered by the project's test framework."
  },
  "targeted_risks": {
    "risk_categories": [
      "Fairness",
      "Accuracy"
    ],
    "atlas_risks": {
      "risks": [
        {
          "category": "Accuracy",
          "subcategory": [
            "Unrepresentative data"
          ]
        },
        {
          "category": "Fairness",
          "subcategory": [
            "Decision bias"
          ]
        }
      ]
    },
    "demographic_analysis": "N/A",
    "harm": []
  },
  "ethical_and_legal_considerations": {
    "privacy_and_anonymity": "N/A",
    "data_licensing": "N/A",
    "consent_procedures": "N/A",
    "compliance_with_regulations": "N/A"
  }
}