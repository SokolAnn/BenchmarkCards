{
  "benchmark_details": {
    "is_benchmark": true,
    "name": "DateLogicQA",
    "abbreviation": "N/A",
    "overview": "DateLogicQA is a human-curated benchmark of 190 questions designed to understand temporal bias in Large Language Models (LLMs), covering seven date formats across past, present, and future contexts, examining commonsense, factual, conceptual, and numerical reasoning types.",
    "data_type": "question-answering pairs",
    "domains": [
      "Natural Language Processing"
    ],
    "languages": [
      "English"
    ],
    "similar_benchmarks": [],
    "resources": [
      "https://arxiv.org/abs/2412.13377v2"
    ]
  },
  "purpose_and_intended_users": {
    "goal": "The primary objective of DateLogicQA is to assess LLMs’ tokenization and understanding of dates, aiming to evaluate context-rich date interpretation.",
    "audience": [
      "ML Researchers",
      "Model Developers"
    ],
    "tasks": [
      "Temporal Reasoning"
    ],
    "limitations": "N/A",
    "out_of_scope_uses": []
  },
  "data": {
    "source": "Human curated benchmark consisting of 190 questions.",
    "size": "190 questions",
    "format": "N/A",
    "annotation": "Curated by human evaluators."
  },
  "methodology": {
    "methods": [
      "Human evaluation"
    ],
    "metrics": [
      "Accuracy"
    ],
    "calculation": "Evaluated using human-led assessments of model responses.",
    "interpretation": "Correct answers indicate successful tokenization and logical reasoning.",
    "baseline_results": null,
    "validation": "High inter-annotator agreement with a Cohen’s kappa (K) score of 0.80."
  },
  "targeted_risks": {
    "risk_categories": [
      "Bias",
      "Accuracy"
    ],
    "atlas_risks": {
      "risks": []
    },
    "demographic_analysis": "N/A",
    "harm": []
  },
  "ethical_and_legal_considerations": {
    "privacy_and_anonymity": "N/A",
    "data_licensing": "N/A",
    "consent_procedures": "N/A",
    "compliance_with_regulations": "N/A"
  }
}