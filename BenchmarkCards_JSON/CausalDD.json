{
  "benchmark_details": {
    "is_benchmark": true,
    "name": "Causal Document-Grounded Dialogue (CausalDD)",
    "abbreviation": "CausalDD",
    "overview": "We present the first causally-complete dataset construction strategy for developing million-scale DocGD pre-training corpora. Additionally, we propose a causally-perturbed pre-training strategy to better capture causality by introducing perturbations on the variables and optimizing the overall causal effect.",
    "data_type": "text (document-grounded dialogue pairs: dialogue context, supporting document, evidence, response)",
    "domains": [
      "Natural Language Processing"
    ],
    "languages": [
      "English",
      "Chinese"
    ],
    "similar_benchmarks": [
      "Doc2dial",
      "MultiDoc2dial",
      "Doc2bot",
      "CoQA",
      "QuAC",
      "DoQA",
      "Wizard"
    ],
    "resources": [
      "https://github.com/Vamsi995/Paraphrase-Generator",
      "https://creativecommons.org/licenses/by-sa/3.0",
      "https://edition.cnn.com/2023/01/07/health/dog-and-cat-new-year-resolutions-wellness/index.html"
    ]
  },
  "purpose_and_intended_users": {
    "goal": "Create causally-complete pre-training datasets for document-grounded dialogue and design a causally-perturbed pre-training strategy to model causal relationships among document, evidence, dialogue context, and response, improving DocGD performance in fully-supervised, few-shot, low-resource, and zero-shot settings.",
    "audience": [],
    "tasks": [
      "Document-Grounded Dialogue",
      "Knowledge Grounding",
      "Response Generation"
    ],
    "limitations": "Pre-training data are generated by models (dialogue inpainter and paraphrase model), so data quality is slightly inferior to manually annotated data; the pre-training data construction strategy may not be applicable to other tasks such as knowledge graph-grounded dialogue; effectiveness of task-specific pre-training decreases as the amount of labeled data increases.",
    "out_of_scope_uses": []
  },
  "data": {
    "source": "Two causally-complete pre-training corpora constructed by the authors: (1) WikiDialog: transform Wikipedia documents into dialogues by generating pseudo-user utterances with a dialogue inpainter and paraphrasing evidence for agent responses; (2) Reddit: crawl Reddit submissions containing URLs, use linked web pages as documents and the conversations/replies as dialogues, inserting virtual evidence into documents.",
    "size": "WikiDialog: 1.00M dialogues, 0.12M documents, 3.00M turns; Reddit: 1.00M dialogues, 1.00M documents, 1.39M turns; All: 2.00M dialogues, 1.12M documents, 4.39M turns",
    "format": "N/A",
    "annotation": "Automatically generated: pseudo-user utterances via a dialogue inpainter; paraphrases via a paraphrase model; Chinese pre-training data produced via translation model for English-to-Chinese conversion. Human annotation used only for downstream human evaluation."
  },
  "methodology": {
    "methods": [
      "DocGD-specific pre-training (optimize LDocGD to sequentially generate evidence and response conditioned on dialogue context and document)",
      "Causally-perturbed pre-training (introduce perturbations to document and evidence variables and optimize NDE and TIE losses)",
      "Fine-tuning on downstream datasets (Doc2dial, MultiDoc2dial, Doc2bot)",
      "Human evaluation (pairwise comparisons on Relevance and Informativeness)"
    ],
    "metrics": [
      "Exact Match (EM)",
      "F1 Score (token-level F1)",
      "BLEU Score",
      "Distinct (Dist-n)",
      "Human evaluation: Relevance",
      "Human evaluation: Informativeness",
      "Statistical significance (p-value under t-test)"
    ],
    "calculation": "Knowledge identification: Exact Match (EM) and token-level F1. Response generation: BLEU (Papineni et al., 2002) and Dist-n for diversity. NDE is measured via Kullback-Leibler divergence between p_theta(e;r|d;c) and p_theta(e;r|d';c) where d' perturbs non-evidence sentences (Eq.5). TIE is optimized via an unlikelihood loss LTIE = -sum log(1 - p_theta(e;r|b_d;c)) where b_d is the document with evidence removed (Eq.6). Total pretraining objective is L = LDocGD + LNDE + LTIE (Eq.7).",
    "interpretation": "Higher EM and token-level F1 indicate better evidence identification; higher BLEU indicates better response generation quality; higher Dist-n indicates more diverse generation. Lower NDE indicates robustness to perturbations in irrelevant document sentences; higher TIE indicates increased reliance on grounding evidence. Improvements are reported as statistically significant with p-value < 0.05 under t-test where noted.",
    "baseline_results": "Representative baselines and results reported in the paper: Doc2dial knowledge identification (EM / F1) - UniGDD: 65.6 / 76.4; CausalDD: 66.0 / 77.3; CausalDD large: 67.0 / 78.1 (Table 2). Doc2dial response generation (BLEU) - UniGDD/others: UniGDD baselines in Table 3; CausalDD: 43.0; CausalDD large: 42.5 (Table 3). MultiDoc2dial (F1 / EM / BLEU) - UniGDD: 61.5 / 45.8 / 31.8; CausalDD: 63.7 / 49.3 / 33.9; CausalDD large: 64.5 / 51.0 / 33.6 (Table 8). The paper reports consistent improvements over UniGDD and other baselines across settings.",
    "validation": "Validated on three downstream DocGD benchmark datasets (Doc2dial, MultiDoc2dial, Doc2bot) across fully-supervised, few-shot (5/50/100 examples), low-resource (1%/5%/10% of training data), and zero-shot settings; additional validation via human evaluation (100 instances, 5 annotators, pairwise comparisons on Relevance and Informativeness) and ablation studies (effects of WikiDialog vs Reddit vs NDE vs TIE). Statistical significance testing reported (p-value < 0.05 under t-test)."
  },
  "targeted_risks": {
    "risk_categories": [
      "Privacy",
      "Intellectual Property"
    ],
    "atlas_risks": {
      "risks": [
        {
          "category": "Privacy",
          "subcategory": [
            "Personal information in data"
          ]
        },
        {
          "category": "Intellectual Property",
          "subcategory": [
            "Data usage rights restrictions",
            "Copyright infringement"
          ]
        }
      ]
    },
    "demographic_analysis": null,
    "harm": null
  },
  "ethical_and_legal_considerations": {
    "privacy_and_anonymity": "The authors state they have taken measures to ensure that the Wikipedia texts do not contain private information and that the Reddit conversation data does not include any personal information; topics are described as public and harmless.",
    "data_licensing": "Wikipedia corpus shared under the CC BY-SA 3.0 license (https://creativecommons.org/licenses/by-sa/3.0). The Reddit dump is referenced as shared for research purposes (Baumgartner et al., 2020).",
    "consent_procedures": "N/A",
    "compliance_with_regulations": "N/A"
  }
}