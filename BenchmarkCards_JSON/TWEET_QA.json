{
  "benchmark_details": {
    "is_benchmark": true,
    "name": "TWEET QA: A Social Media Focused Question Answering Dataset",
    "abbreviation": "TWEET QA",
    "overview": "We present the first large-scale dataset for QA over social media data. To ensure that the tweets we collected are useful, we only gather tweets used by journalists to write news articles. We then ask human annotators to write questions and answers upon these tweets. Unlike other QA datasets like SQuAD in which the answers are extractive, we allow the answers to be abstractive.",
    "data_type": "question-answering pairs (tweets as context)",
    "domains": [
      "Natural Language Processing",
      "Social Media"
    ],
    "languages": [
      "English"
    ],
    "similar_benchmarks": [
      "SQuAD",
      "CNN/Daily Mail",
      "NewsQA",
      "WIKIMOVIES",
      "MS MARCO",
      "SWAG"
    ],
    "resources": [
      "https://tweetqa.github.io/",
      "https://arxiv.org/abs/1907.06292",
      "https://github.com/huggingface/pytorch-pretrained-BERT",
      "https://archive.org/"
    ]
  },
  "purpose_and_intended_users": {
    "goal": "Create the first large-scale question answering dataset that focuses on social media (Twitter) to enable development and evaluation of QA systems for informal social media text and real-time event understanding.",
    "audience": [
      "Researchers in Natural Language Processing",
      "Machine Learning researchers",
      "Model developers"
    ],
    "tasks": [
      "Question Answering"
    ],
    "limitations": "N/A",
    "out_of_scope_uses": [
      "Videos, images or inserted links should not be considered.",
      "No background knowledge should be required to answer the question.",
      "No Yes-no questions should be asked."
    ]
  },
  "data": {
    "source": "Tweets embedded in news articles crawled from archived snapshots of two major news websites (CNN and NBC); tweets were extracted from tweet blocks embedded in these news articles.",
    "size": "13,757 question-answer pairs; 17,794 tweets; 10,898 articles. Split: 10,692 training triples, 1,086 development triples, 1,979 test triples.",
    "format": "N/A",
    "annotation": "Crowdsourced via Amazon Mechanical Turk (workers located in major English-speaking countries with acceptance rate >95%); post-filtering of QA pairs; separate validation HITs for test and development sets; 492 workers wrote QA pairs; 59 workers participated in validation."
  },
  "methodology": {
    "methods": [
      "Automated metrics (BLEU-1, METEOR, ROUGE-L)",
      "Human performance evaluation (validation HITs)",
      "Model-based evaluation (baselines: Query-Matching, Generative RNN-based model, BiDAF, fine-tuned BERT)"
    ],
    "metrics": [
      "BLEU-1",
      "METEOR",
      "ROUGE-L"
    ],
    "calculation": "Scores computed using both the original answer and the validation answer as references. For human performance, validation answers are treated as generated outputs and original answers as references.",
    "interpretation": "Higher metric scores indicate better performance. Human performance provides a reference point (e.g., HUMAN BLEU-1 ~76.4 (dev) / 78.2 (test)); baseline model scores substantially lower (e.g., BERT BLEU-1 67.3 (dev) / 69.6 (test)), indicating room for improvement.",
    "baseline_results": "HUMAN: BLEU-1 76.4 (dev) / 78.2 (test); METEOR 63.7 / 66.7; ROUGE-L 70.9 / 73.5. EXTRACT-UB (extractive upper bound): BLEU-1 79.5 / 80.3; METEOR 68.8 / 69.8; ROUGE-L 74.3 / 75.6. Query-Matching: BLEU-1 30.3 / 29.4; METEOR 12.0 / 12.1; ROUGE-L 17.0 / 17.4. BiDAF: BLEU-1 48.3 / 48.7; METEOR 31.6 / 31.4; ROUGE-L 38.9 / 38.6. Generative: BLEU-1 53.4 / 53.7; METEOR 32.1 / 31.8; ROUGE-L 39.5 / 39.0. BERT (fine-tuned): BLEU-1 67.3 / 69.6; METEOR 56.9 / 58.6; ROUGE-L 62.6 / 64.1.",
    "validation": "Validation HITs: workers answered questions in test/dev and could label questions as 'NA' if unanswerable (3.1% labeled unanswerable). Manual agreement check on 200 random development samples: 90% semantically equivalent, 2% partially equivalent, 8% totally inconsistent. 59 workers participated in validation."
  },
  "targeted_risks": {
    "risk_categories": [],
    "atlas_risks": {
      "risks": null
    },
    "demographic_analysis": "N/A",
    "harm": "N/A"
  },
  "ethical_and_legal_considerations": {
    "privacy_and_anonymity": "N/A",
    "data_licensing": "N/A",
    "consent_procedures": "N/A",
    "compliance_with_regulations": "N/A"
  }
}