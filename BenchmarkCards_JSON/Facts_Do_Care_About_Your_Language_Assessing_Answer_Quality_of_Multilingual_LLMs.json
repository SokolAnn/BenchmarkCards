{
  "benchmark_details": {
    "is_benchmark": true,
    "name": "Facts Do Care About Your Language: Assessing Answer Quality of Multilingual LLMs",
    "abbreviation": "N/A",
    "overview": "This work evaluates the correctness of the Llama3.1 family of models in answering factual questions appropriate for middle and high school students across different languages, revealing inconsistencies and biases in language model responses.",
    "data_type": "question-answering pairs",
    "domains": [
      "Natural Language Processing",
      "Education"
    ],
    "languages": [
      "Japanese",
      "Arabic",
      "French",
      "Chinese",
      "Persian",
      "Hebrew",
      "Hindi",
      "Nepali",
      "Haitian Creole",
      "Tulu",
      "MƒÅori",
      "English"
    ],
    "similar_benchmarks": [
      "Factcheck-Bench"
    ],
    "resources": [
      "https://arxiv.org/abs/2506.03051"
    ]
  },
  "purpose_and_intended_users": {
    "goal": "To measure the factuality and consistency of LLM responses to middle and high school level questions in various languages.",
    "audience": [
      "ML Researchers",
      "Educators",
      "Model Developers"
    ],
    "tasks": [
      "Question Answering"
    ],
    "limitations": "N/A",
    "out_of_scope_uses": []
  },
  "data": {
    "source": "A set of 54 factual questions covering middle and high school curriculum designed to evaluate LLM factuality.",
    "size": "54 questions",
    "format": "N/A",
    "annotation": "Manual evaluation by bilingual individuals"
  },
  "methodology": {
    "methods": [
      "Manual evaluation",
      "Automated analysis"
    ],
    "metrics": [
      "Keyword Coverage",
      "Incorrectness Score",
      "Extraneous Score"
    ],
    "calculation": "Responses evaluated against predefined target keywords and rated for factual accuracy and extraneous information.",
    "interpretation": "Higher scores indicate better keyword coverage and lower incorrectness; lower extraneous scores indicate more relevant answers.",
    "baseline_results": null,
    "validation": "Manual analysis by bilingual evaluators"
  },
  "targeted_risks": {
    "risk_categories": [
      "Bias",
      "Fairness",
      "Accuracy"
    ],
    "atlas_risks": {
      "risks": []
    },
    "demographic_analysis": "N/A",
    "harm": "N/A"
  },
  "ethical_and_legal_considerations": {
    "privacy_and_anonymity": "N/A",
    "data_licensing": "N/A",
    "consent_procedures": "N/A",
    "compliance_with_regulations": "N/A"
  }
}