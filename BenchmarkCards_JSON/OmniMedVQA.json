{
  "benchmark_details": {
    "is_benchmark": true,
    "name": "OmniMedVQA: A New Large-Scale Comprehensive Evaluation Benchmark for Medical LVLM",
    "abbreviation": "OmniMedVQA",
    "overview": "This benchmark introduces OmniMedVQA, a large-scale and comprehensive Visual Question Answering benchmark tailored to the medical domain. It collects data from 73 different medical datasets, encompassing 12 different modalities and covering more than 20 unique human anatomical regions, thereby facilitating a thorough evaluation of large vision-language models (LVLMs) in addressing medical challenges.",
    "data_type": "question-answering pairs",
    "domains": [
      "Healthcare"
    ],
    "languages": [
      "English"
    ],
    "similar_benchmarks": [
      "VQA-RAD",
      "SLAKE",
      "Path-VQA",
      "VQA-Med"
    ],
    "resources": [
      "https://github.com/OpenGVLab/Multi-Modality-Arena"
    ]
  },
  "purpose_and_intended_users": {
    "goal": "To provide a comprehensive evaluation benchmark for medical vision-language models, facilitating the understanding of their applicability and performance in the medical domain.",
    "audience": [
      "ML Researchers",
      "Medical Practitioners",
      "Healthcare AI Developers"
    ],
    "tasks": [
      "Visual Question Answering"
    ],
    "limitations": "N/A",
    "out_of_scope_uses": []
  },
  "data": {
    "source": "Collected from 73 different medical classification datasets across 12 modalities.",
    "size": "118,010 images and 127,995 question-answering items",
    "format": "N/A",
    "annotation": "Constructed based on classification attributes of collected datasets and validated through human checks."
  },
  "methodology": {
    "methods": [
      "Human evaluation",
      "Accuracy score measurement"
    ],
    "metrics": [
      "Question-answering Score",
      "Prefix-based Score"
    ],
    "calculation": "Calculated based on the correctness of the responses generated by the LVLMs against the provided questions and options.",
    "interpretation": "Higher scores indicate better performance on the benchmark tasks.",
    "baseline_results": "N/A",
    "validation": "Data was human-checked for validity."
  },
  "targeted_risks": {
    "risk_categories": [
      "Bias",
      "Fairness",
      "Accuracy"
    ],
    "atlas_risks": {
      "risks": []
    },
    "demographic_analysis": "N/A",
    "harm": "N/A"
  },
  "ethical_and_legal_considerations": {
    "privacy_and_anonymity": "N/A",
    "data_licensing": "N/A",
    "consent_procedures": "N/A",
    "compliance_with_regulations": "N/A"
  }
}