{
  "benchmark_details": {
    "is_benchmark": true,
    "name": "ODIN (On-demand Data Formulation to Mitigate Dataset Lock-in)",
    "abbreviation": "ODIN",
    "overview": "ODIN attempts to mitigate dataset constraints by generating on-demand datasets based on user requirements. ODIN consists of three main modules: a prompt generator, a text-to-image generator, and an image post-processor. ODIN-generated images (the \"ODIN dataset\") are used to train models and are evaluated in terms of model accuracy and data diversity.",
    "data_type": "image (generated images with associated prompts/labels)",
    "domains": [
      "Computer Vision",
      "Natural Language Processing"
    ],
    "languages": [
      "English"
    ],
    "similar_benchmarks": [
      "Oxford-IIIT Pet (Oxford Pet)",
      "Caltech",
      "Indian Food",
      "CIFAR-100"
    ],
    "resources": [
      "https://openai.com",
      "https://novelai.net",
      "https://tinyurl.com/ODIN-Colab",
      "https://www.kaggle.com/datasets/l33tc0d3r/indian-food-classification"
    ]
  },
  "purpose_and_intended_users": {
    "goal": "To dynamically formulate on-demand datasets to mitigate dataset lock-in by generating image data from unseen labels using text-to-image generation techniques.",
    "audience": [
      "Users"
    ],
    "tasks": [
      "Image Classification",
      "Image Recognition",
      "Image Segmentation"
    ],
    "limitations": "Constrained by the knowledge boundaries of the pre-trained models used (e.g., Stable Diffusion, ChatGPT); optimization for generated images (hyperparameter optimization) was out of scope; limited performance on low-resolution datasets (e.g., CIFAR-100).",
    "out_of_scope_uses": []
  },
  "data": {
    "source": "Generated images via a text-to-image generator (Stable Diffusion-v2) using prompts produced by a prompt generator (e.g., ChatGPT) or extracted via blip-v2 image captioning; Oxford-IIIT Pet dataset was used as source images for prompt extraction in one approach.",
    "size": "Proof-of-concept: 180 images per class (generated as 10 prompts × 18 images per prompt); in PoC rounds used 20 classes → 3,600 generated images per round; reported per-class and per-experiment sizes as described in paper.",
    "format": "Image files generated at 768x768 resolution (generated), resized to 224x224 for training.",
    "annotation": "Automatically labeled with the user-provided class labels (generated images associated with the target label supplied to the prompt generator)."
  },
  "methodology": {
    "methods": [
      "Automated metrics (model accuracy Top-1 and Top-3)",
      "Diversity measurement using Structural Similarity Index (SSIM)",
      "Binary classification (real vs generated) to detect differences between real and generated images",
      "Controlled training experiments across multiple model architectures (ResNet-152, ResNeXt101, ViT-b-16, Swin-v2, RegNet) with fixed hyperparameters"
    ],
    "metrics": [
      "Top-1 Accuracy",
      "Top-3 Accuracy",
      "Structural Similarity Index (SSIM) mean and standard deviation",
      "Binary classification Accuracy (real vs generated)"
    ],
    "calculation": "Top-1/Top-3 accuracy reported from models trained on ODIN datasets using a common hyperparameter setting (learning rate=2e-5, optimizer=adam, loss=cross-entropy, batch size=50), reporting best accuracy within 50 epochs. SSIM: average SSIM score over all pairs in a class; lower average SSIM interpreted as greater diversity.",
    "interpretation": "Lower average SSIM indicates higher image structural diversity. A dataset generated by ODIN is useful to train models but yields slightly lower accuracy compared to models trained on real images. Prompt quality strongly correlates with diversity and model performance. ODIN shows limited performance on low-resolution datasets.",
    "baseline_results": "Design-phase results: initial naive prompt achieved 78.6% (Oxford Pet); ChatGPT-based prompt generation achieved 85.4% (Oxford Pet); blip-v2-based extraction achieved 85.8% (Oxford Pet). Proof-of-concept: average accuracy 88.4% (SD 3.25) across 101 rounds (20 classes, 3,600 images per round). Best reported top-1 accuracy up to 91.4% (ResNeXt101 on Caltech). Comparison to real-data baselines: e.g., ResNeXt101 real dataset accuracy reported as 96.5% vs ODIN 89.6% (difference 6.9%); Swin-v2 real 94.8% vs ODIN 86.4% (difference 8.4%).",
    "validation": "Validation via multi-dataset experiments (Oxford Pet, Caltech, Indian Food, CIFAR-100) and multiple model architectures; PoC with 101 rounds of random 20-class selections; reporting mean and standard deviation for accuracies and SSIM; binary classification tests distinguishing real vs generated images to confirm distributional gap."
  },
  "targeted_risks": {
    "risk_categories": [
      "Robustness",
      "Value Alignment"
    ],
    "atlas_risks": {
      "risks": [
        {
          "category": "Robustness",
          "subcategory": [
            "Prompt leaking",
            "Prompt injection attack"
          ]
        },
        {
          "category": "Value Alignment",
          "subcategory": [
            "Over- or under-reliance"
          ]
        }
      ]
    },
    "demographic_analysis": null,
    "harm": null
  },
  "ethical_and_legal_considerations": {
    "privacy_and_anonymity": "N/A",
    "data_licensing": "N/A",
    "consent_procedures": "N/A",
    "compliance_with_regulations": "N/A"
  }
}