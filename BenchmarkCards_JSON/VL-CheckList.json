{
  "benchmark_details": {
    "is_benchmark": true,
    "name": "VL-CheckList",
    "abbreviation": "N/A",
    "overview": "VL-CheckList is a novel explainable framework to understand the capabilities of pre-trained Vision-Language Pretraining (VLP) models. The method divides image-texting ability into three categories (object, attribute, relation), applies a taxonomy to break down these aspects, uses a linguistic-aware negative sampling strategy to generate hard negatives, evaluates models via Image-Text Matching (ITM) without fine-tuning, and provides a toolbox for generating capability-specific evaluation reports.",
    "data_type": "image-text pairs (image-text matching pairs)",
    "domains": [
      "Computer Vision",
      "Natural Language Processing"
    ],
    "languages": [],
    "similar_benchmarks": [
      "CheckList",
      "Vision CheckList",
      "Dynabench",
      "TIDE"
    ],
    "resources": [
      "https://github.com/om-ai-lab/VL-CheckList",
      "https://arxiv.org/abs/2207.00221",
      "https://www.sbert.net/"
    ]
  },
  "purpose_and_intended_users": {
    "goal": "Evaluate VLP models' fundamental capabilities (object, attribute, relation) using Image-Text Matching (ITM) and a taxonomy-driven, linguistic-aware negative sampling strategy to generate fine-grained, disentangled evaluation reports.",
    "audience": [
      "ML Researchers",
      "Model Developers",
      "System Designers",
      "The research community (users of the open-source toolbox)"
    ],
    "tasks": [
      "Image-Text Matching",
      "Object presence recognition",
      "Attribute recognition/classification",
      "Relation recognition/classification (spatial and action relations)"
    ],
    "limitations": "The paper states they have not yet comprehensively investigated why high ITM-scored models sometimes achieve lower scores on downstream tasks; for a certain downstream task, how it is fine-tuned may be more critical to model performance.",
    "out_of_scope_uses": []
  },
  "data": {
    "source": "Four existing corpora used to build capability-specific evaluation sets: Visual Genome (VG), SWiG, VAW, and HAKE.",
    "size": "Visual Genome (VG): 108K examples; VAW: 72K examples; HAKE: 104K examples; SWiG: 126K examples.",
    "format": "N/A",
    "annotation": "Uses dataset-provided structured annotations: Visual Genome provides attributes, relationships, and region graphs; SWiG provides structured semantic summaries with roles (agent, tool, etc.); HAKE provides relationship annotations between instance activity and body part states; VAW provides attribute annotations."
  },
  "methodology": {
    "methods": [
      "Image-Text Matching (ITM) evaluation using pre-trained VLP models' ITM heads (no fine-tuning)",
      "Linguistic-aware negative sampling to generate hard negative examples by replacing objects/attributes/relations in captions",
      "Use of structured annotations from VG, SWiG, VAW, HAKE to create capability-specific test sets",
      "Visualization and analysis including quantitative tables, radar charts, and Grad-CAM attention heatmaps"
    ],
    "metrics": [
      "Accuracy"
    ],
    "calculation": "Accuracy (acc) is defined as acc = (1/N) * sum_{i=1..N} f(xp_i, xn_i), where f(xp_i, xn_i) = 1 if p(xp_i | I_i) > p(xn_i | I_i) else 0. xp_i is the positive sample, xn_i is the generated negative sample, I_i is the image.",
    "interpretation": "A model is counted as correct for a pair if the model's ITM score for the original (positive) text is higher than for the generated negative; accuracy is the fraction of such positive outcomes. Higher accuracy indicates better capability on the tested aspect.",
    "baseline_results": "Table 2 (Overall performance) reports average accuracies for seven compared models: LXMERT 72.35, UNITER 72.63, OSCAR 69.31, ViLT 76.82, ALBEF 75.62, TCL 74.57, CLIP 71.65 (per Table 2 in the paper).",
    "validation": "Validated by applying VL-CheckList to seven popular VLP models (CLIP, OSCAR, UNITER, LXMERT, ViLT, ALBEF, TCL) using four corpora (VG, SWiG, VAW, HAKE) and showing fine-grained differences and insights not visible from downstream task-only evaluation."
  },
  "targeted_risks": {
    "risk_categories": [
      "Bias",
      "Robustness",
      "Accuracy",
      "Governance"
    ],
    "atlas_risks": {
      "risks": [
        {
          "category": "Fairness",
          "subcategory": [
            "Data bias"
          ]
        },
        {
          "category": "Accuracy",
          "subcategory": [
            "Unrepresentative data"
          ]
        },
        {
          "category": "Robustness",
          "subcategory": [
            "Evasion attack"
          ]
        },
        {
          "category": "Governance",
          "subcategory": [
            "Lack of testing diversity"
          ]
        }
      ]
    },
    "demographic_analysis": null,
    "harm": null
  },
  "ethical_and_legal_considerations": {
    "privacy_and_anonymity": "N/A",
    "data_licensing": "N/A",
    "consent_procedures": "N/A",
    "compliance_with_regulations": "N/A"
  }
}