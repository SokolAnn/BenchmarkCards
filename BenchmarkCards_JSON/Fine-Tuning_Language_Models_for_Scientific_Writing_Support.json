{
  "benchmark_details": {
    "is_benchmark": true,
    "name": "Fine-Tuning Language Models for Scientific Writing Support",
    "abbreviation": "N/A",
    "overview": "We support scientific writers in determining whether a written sentence is scientific, to which section it belongs, and suggest paraphrasings to improve the sentence.",
    "data_type": "text (sentences; parallel sentence pairs for paraphrasing)",
    "domains": [
      "Natural Language Processing",
      "Computer Science"
    ],
    "languages": [
      "English"
    ],
    "similar_benchmarks": [
      "GYAFC (Grammarly’s Yahoo Answers Formality Corpus)",
      "unarXive",
      "unarXive 2022"
    ],
    "resources": [
      "https://github.com/JustinMuecke/SciSen",
      "http://portal.core.edu.au/conf-ranks/",
      "https://production-media.paperswithcode.com/about/papers-with-abstracts.json.gz",
      "https://files.pushshift.io/reddit/comments/",
      "https://www.kaggle.com/datasets/jannesklaas/scifi-stories-text-corpus",
      "https://www.kaggle.com/datasets/jojo1000/facebook-last-names-with-count",
      "https://huggingface.co/tuner007/pegasusparaphrase",
      "https://app.grammarly.com/",
      "https://openai.com/blog/chatgpt"
    ]
  },
  "purpose_and_intended_users": {
    "goal": "Support scientific writers by (i) assigning a scientificness score to sentences, (ii) classifying sentences to paper sections, and (iii) suggesting paraphrases to improve scientific writing.",
    "audience": [
      "Scientific writers",
      "Researchers",
      "Students"
    ],
    "tasks": [
      "Regression (Scientificness Scoring)",
      "Multi-label Classification (Section Classification)",
      "Paraphrase Generation (Sentence Paraphrasing)"
    ],
    "limitations": "Selection and labeling of the non-scientific datasets may pose a limitation. Unchanged (identity) outputs from paraphrasers can be a problem for general paraphrasing tasks but are acceptable for this application where inputs may already be scientific.",
    "out_of_scope_uses": []
  },
  "data": {
    "source": "Papers from arXiv (LaTeX available) accepted at A*, A, B, and C ranked conferences (CORE2021) mapped via Papers With Code; non-scientific sentences from Reddit comments, a sci-fi stories corpus, and Twitter subsets; paraphrasing parallel datasets Pegasus-DS (created using Pegasus fine-tuned for paraphrasing) and IDM-DS (created by inserting/deleting/modifying tokens using BERT MLM). GYAFC (Grammarly’s Yahoo Answers Formality Corpus) used for testing.",
    "size": "26,201 papers (21,774 A*, 3,665 A, 530 B, 232 C). Sentence counts reported: arXiv raw sentences 5,283,451 (4,958,863 remaining after filters); arXiv with section ID 2,864,755 (2,687,364 remaining); Books 1,763,465 (1,613,244 remaining); Reddit 279,288 (217,225 remaining); Twitter 268,419 (35,108 remaining). GYAFC test used: 1,332 sentences.",
    "format": "Extracted sentence-level text from LaTeX sources (sentences split at ., ?, !). Citations replaced by <reference> tokens and math by <equation> tokens. Filtered to ASCII, 4–100 words, proper capitalization and end punctuation.",
    "annotation": "Section labels obtained by extracting LaTeX \\section{...} titles and mapping titles to predefined classes (\"introduction\", \"related work\", \"method\", \"experiment\", \"result\", \"discussion\", \"conclusion\"). Paraphrase parallel data created automatically: Pegasus-DS via Pegasus paraphraser, IDM-DS via random insert/delete/modify up to half tokens using BERT MLM. No manual expert annotation is described."
  },
  "methodology": {
    "methods": [
      "Automated metrics",
      "Model fine-tuning (BERT, SciBERT, WideMLP, BART, T5, Pegasus, GPT-2)",
      "Regression",
      "Multi-label classification",
      "Paraphrase generation"
    ],
    "metrics": [
      "Mean Squared Error (MSE)",
      "Sample-based F1",
      "BLEU (n=4)",
      "METEOR",
      "BERTScore",
      "self-BLEU",
      "Word Error Rate (WER)"
    ],
    "calculation": "Scientificness scored as regression evaluated with MSE. Section classification evaluated using sample-based F1 (following Galke et al.). BLEU computed with n-gram similarity (n=4). METEOR includes synonym matching. BERTScore computes cosine similarity of sentence embeddings using SciBERT. WER used to measure amount of changes for Pegasus-DS. self-BLEU computed between input and output.",
    "interpretation": "Lower MSE indicates better scientificness scoring. Higher sample-based F1 indicates better section classification. Higher BLEU, METEOR, and BERTScore indicate paraphrase outputs closer to the gold standard. Lower self-BLEU indicates larger changes from the input (more aggressive paraphrasing). WER buckets indicate corruption/change levels.",
    "baseline_results": "Scientificness MSE: fine-tuned BERT 0.181%, fine-tuned SciBERT 0.213%, WideMLP 0.049%. Section classification: BERT achieved up to 90.10% sample-based F1 (3-sentence input, all data). Paraphrasing: T5 Large performed best on fine-tuning datasets; T5 Base achieved highest BLEU on GYAFC test.",
    "validation": "Random 70:20:10 train/validation/test split. Hyperparameter tuning on a 10% subset of train+validation. Early stopping for transformer fine-tuning (validation loss) and fixed epoch budgets described per task (e.g., BERT/SciBERT trained up to 5 epochs for regression). Test splits divided into buckets by amount of changes (0%–50%)."
  },
  "targeted_risks": {
    "risk_categories": [
      "Privacy",
      "Fairness",
      "Robustness",
      "Value Alignment"
    ],
    "atlas_risks": {
      "risks": [
        {
          "category": "Fairness",
          "subcategory": [
            "Data bias"
          ]
        },
        {
          "category": "Privacy",
          "subcategory": [
            "Exposing personal information"
          ]
        },
        {
          "category": "Robustness",
          "subcategory": [
            "Hallucination"
          ]
        },
        {
          "category": "Value Alignment",
          "subcategory": [
            "Over- or under-reliance"
          ]
        }
      ]
    },
    "demographic_analysis": null,
    "harm": [
      "Authorship concerns",
      "Hallucinations (incorrect/invented content)",
      "Extraction of training samples / exposing training data"
    ]
  },
  "ethical_and_legal_considerations": {
    "privacy_and_anonymity": "Authors note models were fine-tuned on publicly available papers only and pre-training checkpoints are public; they acknowledge the possibility of extracting training samples and advise users to verify suggested paraphrases.",
    "data_licensing": "N/A",
    "consent_procedures": "N/A",
    "compliance_with_regulations": "N/A"
  }
}