{
  "benchmark_details": {
    "is_benchmark": true,
    "name": "WXI MPACT BENCH",
    "abbreviation": "WXI MPACT BENCH",
    "overview": "The first benchmark for evaluating the capacity of LLMs on disruptive weather impacts, which includes two evaluation tasks: multi-label classification and ranking-based question answering.",
    "data_type": "disruptive weather impact annotations and question-answering pairs",
    "domains": [
      "Natural Language Processing"
    ],
    "languages": [
      "English"
    ],
    "similar_benchmarks": [
      "CLLMate"
    ],
    "resources": [
      "https://github.com/Michaelyya/WXImpactBench"
    ]
  },
  "purpose_and_intended_users": {
    "goal": "To evaluate the ability of large language models to understand disruptive weather impacts, aiding in climate change adaptation understanding.",
    "audience": [
      "ML Researchers",
      "Climate Scientists",
      "Policy Makers"
    ],
    "tasks": [
      "Multi-Label Classification",
      "Ranking-based Question Answering"
    ],
    "limitations": "Potential biases in underrepresented historical events and linguistic variations.",
    "out_of_scope_uses": []
  },
  "data": {
    "source": "Historical newspaper articles collected and processed through a multi-stage data construction pipeline.",
    "size": "350 historical articles, 1,386 mixed context articles",
    "format": "JSON",
    "annotation": "Annotated by domain experts following specific guidelines for impact categories."
  },
  "methodology": {
    "methods": [
      "Human evaluation",
      "Automated metrics"
    ],
    "metrics": [
      "Accuracy",
      "F1 Score",
      "Hit@1",
      "nDCG@5",
      "Recall@5",
      "Mean Reciprocal Rank (MRR)"
    ],
    "calculation": "Metrics calculated based on model predictions against ground-truth labels.",
    "interpretation": "A higher score indicates better understanding of disruptive weather impacts by the models.",
    "baseline_results": null,
    "validation": "Extensive evaluation across various LLMs to benchmark their performance."
  },
  "targeted_risks": {
    "risk_categories": [
      "Bias",
      "Accuracy",
      "Robustness"
    ],
    "atlas_risks": {
      "risks": [
        {
          "category": "Accuracy",
          "subcategory": [
            "Unrepresentative data"
          ]
        },
        {
          "category": "Fairness",
          "subcategory": [
            "Data bias"
          ]
        },
        {
          "category": "Robustness",
          "subcategory": [
            "Evasion attack"
          ]
        }
      ]
    },
    "demographic_analysis": null,
    "harm": []
  },
  "ethical_and_legal_considerations": {
    "privacy_and_anonymity": "N/A",
    "data_licensing": "Data is publicly available under specific collaboration agreements.",
    "consent_procedures": "Permission obtained from the organizations preserving the copyright.",
    "compliance_with_regulations": "N/A"
  }
}