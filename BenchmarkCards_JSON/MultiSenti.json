{
  "benchmark_details": {
    "is_benchmark": true,
    "name": "MultiSenti",
    "abbreviation": "N/A",
    "overview": "A labeled dataset called MultiSenti for sentiment classification of code-switched informal short text (Roman Urdu). The paper presents the dataset, explores adapting resources from a resource-rich language for an informal one, and proposes a deep learning model (McM) for sentiment classification of code-switched informal short text without lexical normalization, translation, or code-switching indication.",
    "data_type": "Short social media text (tweets) labeled for sentiment",
    "domains": [
      "Natural Language Processing"
    ],
    "languages": [
      "Roman Urdu",
      "English",
      "Mixed"
    ],
    "similar_benchmarks": [],
    "resources": [
      "https://github.com/haroonshakeel/multisenti"
    ]
  },
  "purpose_and_intended_users": {
    "goal": "Provide an annotated dataset (MultiSenti) for sentiment classification of Roman Urdu code-switched informal short text and to evaluate/adapt embedding and model choices for this task.",
    "audience": [],
    "tasks": [
      "Sentiment Classification"
    ],
    "limitations": "N/A",
    "out_of_scope_uses": []
  },
  "data": {
    "source": "Collected from Twitter during and after the general elections of Pakistan in 2018.",
    "size": "20,735 examples (manually annotated gold-standard); combined corpus for multilingual embeddings: more than 6.5 million words (used to train embeddings).",
    "format": "N/A",
    "annotation": "Manually annotated into 'negative', 'positive', and 'neutral' by two annotators under supervision of a domain-expert; domain-expert decision used in case of annotator conflict. Class-based stratified sampling (80-20%) used for train/test splits."
  },
  "methodology": {
    "methods": [
      "Model-based evaluation",
      "Automated metrics"
    ],
    "metrics": [
      "Accuracy",
      "Precision (macro-averaged)",
      "Recall (macro-averaged)",
      "F1 Score (macro-averaged)"
    ],
    "calculation": "Metrics are computed on the test split. Precision, recall, and F1 are reported as macro-averaged across classes. Class-based stratified sampling (80-20%) is used to generate train and test splits.",
    "interpretation": "The paper focuses discussion and comparison primarily on macro-averaged F1-score.",
    "baseline_results": "Proposed model McM achieves F1-score 0.65 with ELMo finetuning (With Finetuning McM ELMo F1=0.65). McM multilingual without finetuning achieves F1-score 0.65. ConvNet best reported F1-score 0.61. Attention-LSTM best reported F1-score 0.64. SimpleConv best reported F1-score 0.63. (See Table 2 in paper for full results.)",
    "validation": "Class-based stratified sampling with an 80-20 train/test split. A 20% stratified validation set (taken from the training set) was used for hyperparameter grid search. Early stopping was used (training stopped if testing error did not decrease for 10 epochs) and checkpoints of learned weights were saved at the epoch with best predictive performance on the test split."
  },
  "targeted_risks": {
    "risk_categories": [],
    "atlas_risks": {
      "risks": null
    },
    "demographic_analysis": null,
    "harm": null
  },
  "ethical_and_legal_considerations": {
    "privacy_and_anonymity": "N/A",
    "data_licensing": "N/A",
    "consent_procedures": "N/A",
    "compliance_with_regulations": "N/A"
  }
}