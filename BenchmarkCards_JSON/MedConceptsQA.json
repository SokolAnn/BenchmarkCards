{
  "benchmark_details": {
    "is_benchmark": true,
    "name": "MedConceptsQA",
    "abbreviation": "N/A",
    "overview": "MedConceptsQA is an open-source benchmark for evaluating the understanding and reasoning abilities of Large Language Models on medical concepts across diagnoses, procedures, and drugs. It includes over 800,000 questions categorized into various difficulty levels.",
    "data_type": "question-answering pairs",
    "domains": [
      "Healthcare"
    ],
    "languages": [
      "English"
    ],
    "similar_benchmarks": [
      "BioASQ-QA",
      "EmrQA",
      "PubMedQA"
    ],
    "resources": [
      "https://huggingface.co/datasets/ofir408/MedConceptsQA",
      "https://github.com/nadavlab/MedConceptsQA"
    ]
  },
  "purpose_and_intended_users": {
    "goal": "To evaluate the understanding and reasoning capabilities of Large Language Models regarding medical concepts.",
    "audience": [
      "Research Scientists",
      "ML Researchers"
    ],
    "tasks": [
      "Question Answering"
    ],
    "limitations": "N/A",
    "out_of_scope_uses": []
  },
  "data": {
    "source": "MedConceptsQA benchmark database comprising over 800,000 medical concept Q&A.",
    "size": "800,000 questions",
    "format": "N/A",
    "annotation": "Automatically generated"
  },
  "methodology": {
    "methods": [
      "Zero-shot learning",
      "Few-shot learning"
    ],
    "metrics": [
      "Accuracy"
    ],
    "calculation": "Accuracy is calculated as the proportion of correctly answered questions among the total number of questions.",
    "interpretation": "Higher accuracy indicates better understanding and reasoning of medical concepts by the evaluated models.",
    "baseline_results": "GPT-4 achieves the highest accuracy among all models tested.",
    "validation": "Evaluations conducted on various medical concept questions across difficulty levels."
  },
  "targeted_risks": {
    "risk_categories": [
      "Accuracy",
      "Fairness"
    ],
    "atlas_risks": {
      "risks": [
        {
          "category": "Accuracy",
          "subcategory": [
            "Poor model accuracy"
          ]
        },
        {
          "category": "Fairness",
          "subcategory": [
            "Data bias"
          ]
        }
      ]
    },
    "demographic_analysis": "N/A",
    "harm": []
  },
  "ethical_and_legal_considerations": {
    "privacy_and_anonymity": "N/A",
    "data_licensing": "N/A",
    "consent_procedures": "N/A",
    "compliance_with_regulations": "N/A"
  }
}