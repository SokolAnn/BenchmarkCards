{
  "benchmark_details": {
    "is_benchmark": true,
    "name": "LinguaSafe: A Comprehensive Multilingual Safety Benchmark for Large Language Models",
    "abbreviation": "LinguaSafe",
    "overview": "LinguaSafe is a comprehensive multilingual safety benchmark crafted with meticulous attention to linguistic authenticity, comprising 45k entries across 12 languages. It addresses the critical need for multilingual safety evaluations of LLMs, filling the void in safety assessment across diverse under-represented languages.",
    "data_type": "text",
    "domains": [
      "Natural Language Processing"
    ],
    "languages": [
      "English",
      "Hungarian",
      "Malay",
      "Russian",
      "Chinese",
      "Vietnamese",
      "Arabic",
      "Korean",
      "Serbian",
      "Thai",
      "Bengali"
    ],
    "similar_benchmarks": [],
    "resources": [
      "https://huggingface.co/datasets/telegraphpolehead/linguasafe",
      "https://github.com/telegraph-pole-head/LinguaSafeDatasetsMultilingual"
    ]
  },
  "purpose_and_intended_users": {
    "goal": "To provide a comprehensive suite of metrics for in-depth safety evaluation of multilingual LLMs.",
    "audience": [
      "ML Researchers",
      "Model Developers",
      "Ethicists"
    ],
    "tasks": [
      "Safety Evaluation"
    ],
    "limitations": "The dataset covers 12 languages which is relatively limited compared to other multilingual benchmarks.",
    "out_of_scope_uses": []
  },
  "data": {
    "source": "Curated from a mix of translated, transcreated, and natively-sourced data across various online platforms including forums and social media.",
    "size": "45,000 examples",
    "format": "N/A",
    "annotation": "Annotated by a combination of human annotators and AI with a multi-stage evaluation process."
  },
  "methodology": {
    "methods": [
      "Human evaluation",
      "Automated metrics"
    ],
    "metrics": [
      "Severity-Weighted Confusion Matrix",
      "Unsafe Rate",
      "Oversensitivity Rate"
    ],
    "calculation": "Metrics are calculated based on collected responses from the models evaluated on safety recognition tasks and oversensitivity scenarios.",
    "interpretation": "Results help in assessing the safety alignment of LLMs across different languages and domains.",
    "baseline_results": null,
    "validation": "Validation against existing multilingual safety benchmarks and inter-annotator agreement measures."
  },
  "targeted_risks": {
    "risk_categories": [
      "Safety",
      "Bias"
    ],
    "atlas_risks": {
      "risks": []
    },
    "demographic_analysis": null,
    "harm": []
  },
  "ethical_and_legal_considerations": {
    "privacy_and_anonymity": "Data processing involves ethical guidelines; potential sources of sensitive content are screened and reviewed.",
    "data_licensing": "Released under CC BY-NC-SA 4.0 License (non-commercial, research-only).",
    "consent_procedures": "N/A",
    "compliance_with_regulations": "N/A"
  }
}