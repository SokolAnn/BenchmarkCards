{
  "benchmark_details": {
    "is_benchmark": true,
    "name": "VideoCoT (Video Chain-of-Thought)",
    "abbreviation": "VideoCoT",
    "overview": "VideoCoT is designed to supplement CoT between question and answer from existing datasets, employing an automatic annotation tool under the active learning paradigm to assist in generating complex reasoning capabilities in multimodal large language models (MLLMs).",
    "data_type": "question-answering pairs",
    "domains": [
      "Natural Language Processing"
    ],
    "languages": [
      "English",
      "Chinese"
    ],
    "similar_benchmarks": [
      "TopicQA",
      "TopicCoT"
    ],
    "resources": [
      "N/A"
    ]
  },
  "purpose_and_intended_users": {
    "goal": "To explore the collection of Chain-of-Thought datasets on videos to bootstrap OpenQA on videos and improve the inference ability of multimodal large language models.",
    "audience": [
      "ML Researchers",
      "Model Developers"
    ],
    "tasks": [
      "Question Answering"
    ],
    "limitations": "In regards to the active annotation tool, using our tool on additional datasets can enhance the visual reasoning abilities of more models. However, funding constraints limited the invitation of annotation experts.",
    "out_of_scope_uses": []
  },
  "data": {
    "source": "Kinetics-700 dataset and existing datasets",
    "size": "11,182 samples",
    "format": "N/A",
    "annotation": "Automatic annotation using machine and human experts under the active learning paradigm."
  },
  "methodology": {
    "methods": [
      "Human evaluation",
      "Automated metrics"
    ],
    "metrics": [
      "Accuracy"
    ],
    "calculation": "Metrics are derived from the proportion of correct answers generated by models compared to ground-truth.",
    "interpretation": "Higher accuracy indicates better performance in generating relevant answers.",
    "baseline_results": null,
    "validation": "Results are tested against multiple baselines and evaluated using training, validation, and testing subsets."
  },
  "targeted_risks": {
    "risk_categories": [
      "Fairness",
      "Accuracy"
    ],
    "atlas_risks": {
      "risks": [
        {
          "category": "Fairness",
          "subcategory": []
        },
        {
          "category": "Accuracy",
          "subcategory": []
        }
      ]
    },
    "demographic_analysis": null,
    "harm": []
  },
  "ethical_and_legal_considerations": {
    "privacy_and_anonymity": "N/A",
    "data_licensing": "N/A",
    "consent_procedures": "N/A",
    "compliance_with_regulations": "N/A"
  }
}