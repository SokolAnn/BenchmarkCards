{
  "benchmark_details": {
    "is_benchmark": true,
    "name": "ImgEdit: A Unified Image Editing Dataset and Benchmark",
    "abbreviation": "ImgEdit",
    "overview": "ImgEdit introduces a large-scale, high-quality image-editing dataset comprising 1.2 million carefully curated edit pairs for both single-turn and complex multi-turn editing tasks. It includes a benchmark designed to evaluate image editing performance across various dimensions, significantly advancing the capabilities of open-source image-editing models.",
    "data_type": "image-editing pairs",
    "domains": [
      "Computer Vision"
    ],
    "languages": [
      "English"
    ],
    "similar_benchmarks": [],
    "resources": [
      "https://github.com/PKU-YuanGroup/ImgEdit"
    ]
  },
  "purpose_and_intended_users": {
    "goal": "To provide a comprehensive, high-quality dataset and benchmark for evaluating image editing models, facilitating advancements in the field by overcoming limitations found in existing datasets.",
    "audience": [
      "ML Researchers",
      "Model Developers",
      "AI Practitioners"
    ],
    "tasks": [
      "Image Editing",
      "Multi-Turn Interaction"
    ],
    "limitations": "N/A",
    "out_of_scope_uses": []
  },
  "data": {
    "source": "Curated from various high-quality image datasets using advanced data generation pipelines.",
    "size": "1.2 million image-editing pairs",
    "format": "N/A",
    "annotation": "Carefully curated and verified using a multi-stage pipeline integrating vision-language models, segmentation models, and human-assisted evaluation."
  },
  "methodology": {
    "methods": [
      "Human evaluation",
      "Automated metrics"
    ],
    "metrics": [
      "Instruction adherence",
      "Image-editing quality",
      "Detail preservation"
    ],
    "calculation": "Metrics are calculated based on ratings assigned by a vision-language model (GPT-4o), which assesses adherence to instructions and the quality of the edited images.",
    "interpretation": "Scores are interpreted based on adherence to prompts and quality of edits, with a higher score indicating better performance.",
    "baseline_results": null,
    "validation": "Validation was conducted through comparative analysis of outputs from various models against the key metrics."
  },
  "targeted_risks": {
    "risk_categories": [
      "Accuracy",
      "Fairness"
    ],
    "atlas_risks": {
      "risks": []
    },
    "demographic_analysis": "N/A",
    "harm": []
  },
  "ethical_and_legal_considerations": {
    "privacy_and_anonymity": "N/A",
    "data_licensing": "N/A",
    "consent_procedures": "N/A",
    "compliance_with_regulations": "N/A"
  }
}