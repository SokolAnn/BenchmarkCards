{
  "benchmark_details": {
    "is_benchmark": true,
    "name": "ParamBench",
    "abbreviation": "N/A",
    "overview": "ParamBench is a graduate-level benchmark consisting of around 11.5K questions in Hindi language covering 16 diverse subjects specific to the Indian context, examining language understanding and culturally nuanced reasoning through various question formats.",
    "data_type": "question-answering pairs",
    "domains": [
      "Education"
    ],
    "languages": [
      "Hindi"
    ],
    "similar_benchmarks": [
      "KMMLU",
      "CMMLU",
      "BIG-Bench",
      "HELM",
      "SANSKRITI",
      "MILU"
    ],
    "resources": [
      "N/A"
    ]
  },
  "purpose_and_intended_users": {
    "goal": "Identify and quantify current gaps in LLM performance for the Indian context, guiding development of culturally and linguistically aligned models.",
    "audience": [
      "ML Researchers",
      "Model Developers",
      "Domain Experts"
    ],
    "tasks": [
      "Question Answering"
    ],
    "limitations": "N/A",
    "out_of_scope_uses": []
  },
  "data": {
    "source": "Questions are collected from UGC-NET and UPSC Civil Services examination papers.",
    "size": "11,446 questions",
    "format": "CSV",
    "annotation": "Annotated by subject-matter experts proficient in Hindi"
  },
  "methodology": {
    "methods": [
      "Zero-shot evaluation",
      "Direct answer matching"
    ],
    "metrics": [
      "Accuracy"
    ],
    "calculation": "Accuracy is reported as the proportion of correct outputs based on direct answer matching generated responses compared with the gold key.",
    "interpretation": "Higher accuracy indicates better performance on culturally grounded questioning.",
    "baseline_results": null,
    "validation": "Quality assurance protocols included both manual and automated checks"
  },
  "targeted_risks": {
    "risk_categories": [
      "Fairness",
      "Accuracy"
    ],
    "atlas_risks": {
      "risks": [
        {
          "category": "Fairness",
          "subcategory": [
            "Data bias"
          ]
        },
        {
          "category": "Accuracy",
          "subcategory": [
            "Unrepresentative data"
          ]
        }
      ]
    },
    "demographic_analysis": "N/A",
    "harm": []
  },
  "ethical_and_legal_considerations": {
    "privacy_and_anonymity": "N/A",
    "data_licensing": "N/A",
    "consent_procedures": "N/A",
    "compliance_with_regulations": "N/A"
  }
}