{
  "benchmark_details": {
    "name": "Detecting Emergent Intersectional Biases",
    "overview": "This paper presents methods to identify and measure biases in neural language models, particularly focusing on intersectional and emergent biases associated with members of multiple minority groups, such as African American and Mexican American females.",
    "data_type": "text",
    "domains": [
      "Natural Language Processing",
      "Social Bias Detection"
    ],
    "languages": [
      "English"
    ],
    "similar_benchmarks": [
      "Word Embedding Association Test (WEAT)",
      "Bias in Bios"
    ],
    "resources": [
      "https://github.com/weiguowilliam/CEAT"
    ]
  },
  "purpose_and_intended_users": {
    "goal": "To detect and measure intersectional biases in word embeddings and neural language models.",
    "audience": [
      "Researchers in AI and NLP",
      "Social scientists studying bias",
      "Policy makers"
    ],
    "tasks": [
      "Identify intersectional biases",
      "Measure overall bias in language models"
    ],
    "limitations": null,
    "out_of_scope_uses": null
  },
  "data": {
    "source": "Common Crawl corpus and Reddit comments",
    "size": "840 billion tokens",
    "format": "text",
    "annotation": "Human subjects provided validation for intersectional attributes."
  },
  "methodology": {
    "methods": [
      "Contextualized Embedding Association Test (CEAT)",
      "Intersectional Bias Detection (IBD)",
      "Emergent Intersectional Bias Detection (EIBD)"
    ],
    "metrics": [
      "Effect size (Cohen's d)",
      "Accuracy in detecting biases"
    ],
    "calculation": "Combined effect sizes from multiple tests using random-effects model.",
    "interpretation": "Significant result indicates a substantial bias in the context of measurements.",
    "baseline_results": null,
    "validation": "Validation conducted using previously published datasets."
  },
  "targeted_risks": {
    "risk_categories": [
      "Accuracy",
      "Fairness",
      "Privacy"
    ],
    "atlas_risks": {
      "risks": [
        {
          "category": "Fairness",
          "subcategory": [
            "Data bias"
          ]
        },
        {
          "category": "Accuracy",
          "subcategory": [
            "Poor model accuracy"
          ]
        }
      ]
    },
    "demographic_analysis": null,
    "harm": [
      "Perpetuation of stereotypes",
      "Exacerbation of social inequalities"
    ]
  },
  "ethical_and_legal_considerations": {
    "privacy_and_anonymity": "Data handling complies with ethical standards.",
    "data_licensing": "Openly available datasets are used.",
    "consent_procedures": "Validation sets utilized in accordance with ethical research guidelines.",
    "compliance_with_regulations": "Meets academic and research compliance standards."
  }
}