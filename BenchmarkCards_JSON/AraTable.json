{
  "benchmark_details": {
    "is_benchmark": true,
    "name": "AraTable",
    "abbreviation": "N/A",
    "overview": "AraTable is a novel and comprehensive benchmark designed to evaluate the reasoning and understanding capabilities of LLMs when applied to Arabic tabular data. The benchmark consists of various evaluation tasks, such as direct question answering, fact verification, and complex reasoning, involving a wide range of Arabic tabular sources.",
    "data_type": "question-answering pairs",
    "domains": [
      "Natural Language Processing"
    ],
    "languages": [
      "Arabic"
    ],
    "similar_benchmarks": [
      "TableBench",
      "DataBench",
      "MMTU"
    ],
    "resources": [
      "https://github.com/rana-alshaikh/AraTable-Benchmark"
    ]
  },
  "purpose_and_intended_users": {
    "goal": "The primary objective of AraTable is to provide a framework for evaluating LLMs on Arabic tabular data processing, specifically aimed at enhancing the understanding of reasoning and cognitive capabilities of models.",
    "audience": [
      "ML Researchers",
      "Model Developers"
    ],
    "tasks": [
      "Direct Question Answering",
      "Fact Verification",
      "Reasoning"
    ],
    "limitations": "N/A",
    "out_of_scope_uses": []
  },
  "data": {
    "source": "The dataset consists of Arabic tabular data collected from various sources including Wikipedia, real-world data from platforms such as Kaggle and government portals, and LLM-generated tables.",
    "size": "615 human-validated QA pairs",
    "format": "CSV",
    "annotation": "Initial content is generated by LLMs and subsequently filtered and verified by human experts to ensure high dataset quality."
  },
  "methodology": {
    "methods": [
      "Automated metrics",
      "Human evaluation"
    ],
    "metrics": [
      "Accuracy"
    ],
    "calculation": "Accuracy is calculated by comparing LLM responses to ground-truth answers validated by human annotators.",
    "interpretation": "Higher accuracy indicates better model understanding and reasoning ability over Arabic tabular data.",
    "baseline_results": "The evaluation shows that DeepSeek-V3 performed best across question types, while Jais models struggled.",
    "validation": "Multi-round human evaluation and an assisted self-deliberation mechanism are used for accuracy validation."
  },
  "targeted_risks": {
    "risk_categories": [
      "Fairness",
      "Accuracy"
    ],
    "atlas_risks": {
      "risks": []
    },
    "demographic_analysis": "N/A",
    "harm": "N/A"
  },
  "ethical_and_legal_considerations": {
    "privacy_and_anonymity": "N/A",
    "data_licensing": "N/A",
    "consent_procedures": "N/A",
    "compliance_with_regulations": "N/A"
  }
}