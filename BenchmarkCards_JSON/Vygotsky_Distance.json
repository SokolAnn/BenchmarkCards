{
  "benchmark_details": {
    "is_benchmark": true,
    "name": "Vygotsky Distance",
    "abbreviation": "N/A",
    "overview": "This paper presents a theoretical instrument and a practical algorithm to calculate similarity between benchmark tasks, called Vygotsky distance. It helps to significantly reduce the evaluation tasks while maintaining high validation quality and increases the generalization potential of future NLP models.",
    "data_type": "task similarity measure",
    "domains": [
      "Natural Language Processing"
    ],
    "languages": [
      "English"
    ],
    "similar_benchmarks": [
      "GLUE",
      "SuperGLUE",
      "CLUE",
      "RussianSuperGLUE"
    ],
    "resources": [
      "https://arxiv.org/abs/2402.14890"
    ]
  },
  "purpose_and_intended_users": {
    "goal": "To provide a method for evaluating the similarity of benchmark tasks to optimize resource usage in NLP evaluations.",
    "audience": [
      "ML Researchers",
      "Industry Practitioners",
      "Model Developers"
    ],
    "tasks": [
      "Task Similarity Evaluation"
    ],
    "limitations": "N/A",
    "out_of_scope_uses": []
  },
  "data": {
    "source": "GLUE, SuperGLUE, CLUE, RussianSuperGLUE benchmarks and data from Papers With Code.",
    "size": "Over 30 benchmarks with approximately 2600 divisions.",
    "format": "tabular representation of model scores on tasks",
    "annotation": "Model evaluation results from existing datasets."
  },
  "methodology": {
    "methods": [
      "Graph Representation Analysis",
      "Benchmark Compression Algorithm"
    ],
    "metrics": [
      "Accuracy",
      "F1 Score",
      "Precision",
      "Recall",
      "ROC-AUC"
    ],
    "calculation": "Vygotsky distance is calculated based on the relative ranking of models on benchmark tasks.",
    "interpretation": "A lower Vygotsky distance indicates higher similarity between tasks.",
    "baseline_results": "N/A",
    "validation": "Rigorously tested using model evaluation results from NLP benchmarks."
  },
  "targeted_risks": {
    "risk_categories": [
      "Accuracy",
      "Fairness",
      "Robustness"
    ],
    "atlas_risks": {
      "risks": [
        {
          "category": "Accuracy",
          "subcategory": [
            "Unrepresentative data"
          ]
        }
      ]
    },
    "demographic_analysis": "N/A",
    "harm": "N/A"
  },
  "ethical_and_legal_considerations": {
    "privacy_and_anonymity": "N/A",
    "data_licensing": "N/A",
    "consent_procedures": "N/A",
    "compliance_with_regulations": "N/A"
  }
}