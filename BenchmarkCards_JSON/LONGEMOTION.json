{
  "benchmark_details": {
    "is_benchmark": true,
    "name": "LONGEMOTION",
    "abbreviation": "N/A",
    "overview": "LONGEMOTION is a benchmark specifically designed for long-context Emotional Intelligence (EI) tasks that includes diverse tasks such as Emotion Classification, Emotion Detection, Emotion QA, Emotion Conversation, Emotion Summary, and Emotion Expression. It addresses the need for evaluating LLMs in long-context scenarios.",
    "data_type": "text",
    "domains": [
      "Natural Language Processing"
    ],
    "languages": [
      "English"
    ],
    "similar_benchmarks": [],
    "resources": [
      "https://github.com/LongEmotion/LongEmotion",
      "https://longemotion.github.io/"
    ]
  },
  "purpose_and_intended_users": {
    "goal": "To assess the Emotional Intelligence of LLMs in realistic long-context interactions.",
    "audience": [
      "ML Researchers",
      "Psychology Researchers",
      "Model Developers"
    ],
    "tasks": [
      "Emotion Classification",
      "Emotion Detection",
      "Emotion QA",
      "Emotion Conversation",
      "Emotion Summary",
      "Emotion Expression"
    ],
    "limitations": "N/A",
    "out_of_scope_uses": []
  },
  "data": {
    "source": "Constructed using existing emotional excerpts inserted into long-context texts, with manual adjustments to ensure coherence.",
    "size": "N/A",
    "format": "N/A",
    "annotation": "Manual annotation by psychology experts and evaluation through automated metrics."
  },
  "methodology": {
    "methods": [
      "Human evaluation",
      "Model-based evaluation"
    ],
    "metrics": [
      "Accuracy",
      "F1 Score",
      "Custom metrics based on psychological theories"
    ],
    "calculation": "Metrics are calculated based on model performance against predefined emotional tasks and human annotations.",
    "interpretation": "Scores reflect the model's ability to understand and generate emotional responses appropriately in long-context formats.",
    "baseline_results": "N/A",
    "validation": "Extensive experiments evaluating models' performance under Base, RAG, and CoEM settings."
  },
  "targeted_risks": {
    "risk_categories": [
      "Bias",
      "Safety",
      "Privacy",
      "Robustness",
      "Fairness",
      "Accuracy"
    ],
    "atlas_risks": {
      "risks": [
        {
          "category": "Fairness",
          "subcategory": [
            "Data bias"
          ]
        },
        {
          "category": "Accuracy",
          "subcategory": [
            "Unrepresentative data"
          ]
        }
      ]
    },
    "demographic_analysis": "N/A",
    "harm": "N/A"
  },
  "ethical_and_legal_considerations": {
    "privacy_and_anonymity": "N/A",
    "data_licensing": "N/A",
    "consent_procedures": "N/A",
    "compliance_with_regulations": "N/A"
  }
}