{
  "benchmark_details": {
    "is_benchmark": true,
    "name": "SEED-Bench-2",
    "abbreviation": "N/A",
    "overview": "SEED-Bench-2 is a comprehensive benchmark that evaluates the hierarchical capabilities of Multimodal Large Language Models (MLLMs) up to level L3, including the generation of both texts and images given interleaved image-text inputs. It consists of 24K multiple-choice questions with human annotations across 27 evaluation dimensions.",
    "data_type": "multiple-choice questions with texts and images",
    "domains": [
      "Natural Language Processing",
      "Computer Vision"
    ],
    "languages": [
      "English"
    ],
    "similar_benchmarks": [
      "SEED-Bench-1",
      "MMBench",
      "MME"
    ],
    "resources": [
      "https://github.com/AILab-CVC/SEED-Bench"
    ]
  },
  "purpose_and_intended_users": {
    "goal": "The primary objective of SEED-Bench-2 is to evaluate and benchmark the capabilities of MLLMs in comprehending and generating multimodal content effectively.",
    "audience": [
      "ML Researchers",
      "Model Developers",
      "Domain Experts"
    ],
    "tasks": [
      "Scene Understanding",
      "Instance Identity",
      "Instance Attribute",
      "Instance Location",
      "Instance Counting",
      "Spatial Relation",
      "Instance Interaction",
      "Visual Reasoning",
      "Text Recognition",
      "Celebrity Recognition",
      "Landmark Recognition",
      "Chart Understanding",
      "Visual Referring Expression",
      "Science Knowledge",
      "Emotion Recognition",
      "Visual Mathematics",
      "Difference Spotting",
      "Meme Comprehension",
      "Global Video Understanding",
      "Action Recognition",
      "Action Prediction",
      "Procedure Understanding",
      "In-Context Captioning",
      "Interleaved Image-Text Analysis",
      "Text-to-Image Generation",
      "Next Image Prediction",
      "Text-Image Creation"
    ],
    "limitations": "N/A",
    "out_of_scope_uses": []
  },
  "data": {
    "source": "Generated multiple-choice questions based on various datasets including CC3M for textual and visual understanding.",
    "size": "24,000 multiple-choice questions",
    "format": "JSON",
    "annotation": "Human annotation with groundtruth answers derived from collected data."
  },
  "methodology": {
    "methods": [
      "Human evaluation",
      "Automated metrics"
    ],
    "metrics": [
      "Accuracy"
    ],
    "calculation": "Accuracy is computed based on the proportion of correctly answered questions against the total number of questions in each evaluation dimension.",
    "interpretation": "Higher accuracy indicates better performance of the MLLMs on given tasks. The evaluation results provide insights into model strengths and weaknesses.",
    "baseline_results": null,
    "validation": "Evaluation against multiple open-source MLLMs to validate performance across various dimensions."
  },
  "targeted_risks": {
    "risk_categories": [
      "Bias",
      "Safety"
    ],
    "atlas_risks": {
      "risks": [
        {
          "category": "Fairness",
          "subcategory": [
            "Data bias"
          ]
        },
        {
          "category": "Robustness",
          "subcategory": [
            "Evasion attack"
          ]
        }
      ]
    },
    "demographic_analysis": null,
    "harm": []
  },
  "ethical_and_legal_considerations": {
    "privacy_and_anonymity": "N/A",
    "data_licensing": "N/A",
    "consent_procedures": "N/A",
    "compliance_with_regulations": "N/A"
  }
}