{
  "benchmark_details": {
    "is_benchmark": true,
    "name": "TopicMisinfo",
    "abbreviation": "N/A",
    "overview": "The TopicMisinfo dataset contains 160 fact-checked claims supplemented by nearly 1600 human annotations with subjective perceptions and annotator demographics, aimed at analyzing the implications of using large language models for claim prioritization in the context of misinformation.",
    "data_type": "fact-checked claims with human annotations",
    "domains": [
      "Natural Language Processing"
    ],
    "languages": [
      "English"
    ],
    "similar_benchmarks": [],
    "resources": [
      "https://doi.org/10.1145/nnnnnnn.nnnnnnn"
    ]
  },
  "purpose_and_intended_users": {
    "goal": "To investigate whether large language models can reflect diverse opinions regarding the harms of misinformation based on gendered perceptions.",
    "audience": [
      "Fact-checkers",
      "AI Researchers",
      "Model Developers"
    ],
    "tasks": [
      "Claim Prioritization",
      "Fact-Checking"
    ],
    "limitations": "The study is limited as it only includes data from men and women without non-binary perspectives, which may not capture the full diversity of opinions.",
    "out_of_scope_uses": []
  },
  "data": {
    "source": "Claims sourced from the Google Fact-Check API covering a wide range of topics with human annotations provided via Amazon Mechanical Turk.",
    "size": "160 claims",
    "format": "N/A",
    "annotation": "Crowdsourced human annotations using a Likert scale for assessing subjective perceptions."
  },
  "methodology": {
    "methods": [
      "Human evaluation",
      "LLM-based evaluation"
    ],
    "metrics": [
      "Rate of perceived harm",
      "Alignment with human assessments"
    ],
    "calculation": "Z-score transformation of annotations for normalization.",
    "interpretation": "Higher scores indicate higher perceived harm and alignment towards gendered opinions, particularly focusing on claim prioritization.",
    "baseline_results": null,
    "validation": "Data validated through attention checks and criteria established prior to annotation."
  },
  "targeted_risks": {
    "risk_categories": [
      "Bias",
      "Fairness",
      "Accuracy"
    ],
    "atlas_risks": {
      "risks": [
        {
          "category": "Fairness",
          "subcategory": [
            "Output bias"
          ]
        },
        {
          "category": "Accuracy",
          "subcategory": [
            "Unrepresentative data"
          ]
        }
      ]
    },
    "demographic_analysis": "Annotations predominantly from men (69%) and women (31%).",
    "harm": [
      "Potential misinformation amplification based on gendered biases in LLM responses."
    ]
  },
  "ethical_and_legal_considerations": {
    "privacy_and_anonymity": "N/A",
    "data_licensing": "N/A",
    "consent_procedures": "Informed consent obtained via demographic surveys.",
    "compliance_with_regulations": "N/A"
  }
}