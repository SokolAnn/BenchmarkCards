{
  "benchmark_details": {
    "is_benchmark": true,
    "name": "TEXTEE (A Standardized Benchmark for Event Extraction)",
    "abbreviation": "TEXTEE",
    "overview": "TEXTEE is a standardized, fair, and reproducible benchmark for event extraction, comprising standardized data preprocessing scripts and splits for 16 datasets spanning eight diverse domains. It includes 14 methodologies for comprehensive reevaluation and evaluation of events.",
    "data_type": "text",
    "domains": [
      "Natural Language Processing"
    ],
    "languages": [
      "English"
    ],
    "similar_benchmarks": [],
    "resources": [
      "https://github.com/ej0cl6/TextEE"
    ]
  },
  "purpose_and_intended_users": {
    "goal": "To standardize and improve the evaluation of event extraction methodologies and facilitate further research in this area.",
    "audience": [
      "ML Researchers",
      "Domain Experts"
    ],
    "tasks": [
      "Event Extraction"
    ],
    "limitations": "Limited to the annotated datasets and methodologies discussed in the paper, may overlook certain datasets and models.",
    "out_of_scope_uses": []
  },
  "data": {
    "source": "Variety of standard datasets for event extraction including ACE05, RichERE, MLEE, Genia2001, Genia2013, M2E2, CASIE, PHEE, MA VEN, FewEvent, SPEED, RAMS, WikiEvents, MUC-4, and GENEV A.",
    "size": "Various sizes depending on the dataset, e.g., 10,000 instances, 1,000 documents.",
    "format": "Standardized formats depending on the respective datasets.",
    "annotation": "Annotation details vary; rely on previously established datasets for event extraction."
  },
  "methodology": {
    "methods": [
      "Standardized evaluation setups",
      "Data preprocessing scripts",
      "Event extraction model implementations"
    ],
    "metrics": [
      "Trigger F1 Score",
      "Argument F1 Score",
      "AI Score (Argument Identification)",
      "AC Score (Argument Classification)"
    ],
    "calculation": "Calculated based on varying methodologies and event extraction models as indicated in the paper.",
    "interpretation": "Guided by aggregated results across multiple datasets indicating model usability and performance standards.",
    "baseline_results": null,
    "validation": "Validation achieved through comprehensive reevaluation of multiple datasets and models."
  },
  "targeted_risks": {
    "risk_categories": [
      "Bias",
      "Fairness",
      "Accuracy"
    ],
    "atlas_risks": {
      "risks": [
        {
          "category": "Fairness",
          "subcategory": [
            "Data bias"
          ]
        },
        {
          "category": "Accuracy",
          "subcategory": [
            "Poor model accuracy"
          ]
        }
      ]
    },
    "demographic_analysis": "1. Performance may vary across different demographics due to data bias in the event extraction datasets. 2. No specific demographic analysis performed. ",
    "harm": []
  },
  "ethical_and_legal_considerations": {
    "privacy_and_anonymity": "N/A",
    "data_licensing": "N/A",
    "consent_procedures": "N/A",
    "compliance_with_regulations": "N/A"
  }
}