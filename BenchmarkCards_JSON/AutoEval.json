{
  "benchmark_details": {
    "is_benchmark": true,
    "name": "AutoEval",
    "abbreviation": "N/A",
    "overview": "This paper presents AutoEval, a novel benchmark for scaling Large Language Model (LLM) assessment in formal tasks with clear notions of correctness, such as truth maintenance in translation and logical reasoning. AutoEval is the first benchmarking paradigm that offers several key advantages necessary for scaling objective evaluation of LLMs without human labeling.",
    "data_type": "text",
    "domains": [
      "Natural Language Processing"
    ],
    "languages": [
      "English"
    ],
    "similar_benchmarks": [
      "HumanEval",
      "LogiEval",
      "FOLIO"
    ],
    "resources": [
      "https://github.com/AAIR-lab/autoeval"
    ]
  },
  "purpose_and_intended_users": {
    "goal": "Provide a robust framework for evaluating the suitability and safety of LLMs in FL-based tasks such as autoformalization and code generation.",
    "audience": [
      "ML Researchers",
      "Industry Practitioners",
      "Model Developers"
    ],
    "tasks": [
      "Logical Reasoning",
      "Translation"
    ],
    "limitations": "N/A",
    "out_of_scope_uses": []
  },
  "data": {
    "source": "Automatically generated datasets through context-free grammars and vocabulary generation.",
    "size": "170,000 examples",
    "format": "N/A",
    "annotation": "Automatically generated ground truth without human annotation."
  },
  "methodology": {
    "methods": [
      "Automated metrics",
      "Formal verification"
    ],
    "metrics": [
      "Accuracy",
      "F1 Score"
    ],
    "calculation": "Metrics are calculated based on the performance comparison with other benchmarks.",
    "interpretation": "Higher scores signify better performance in maintaining truth across translations.",
    "baseline_results": null,
    "validation": "Empirical analysis shows AutoEval's performance is indicative of other benchmarks."
  },
  "targeted_risks": {
    "risk_categories": [
      "Accuracy",
      "Fairness",
      "Societal Impact"
    ],
    "atlas_risks": {
      "risks": [
        {
          "category": "Accuracy",
          "subcategory": [
            "Unrepresentative data",
            "Poor model accuracy"
          ]
        },
        {
          "category": "Fairness",
          "subcategory": [
            "Data bias"
          ]
        },
        {
          "category": "Societal Impact",
          "subcategory": [
            "Impact on education: plagiarism"
          ]
        }
      ]
    },
    "demographic_analysis": "N/A",
    "harm": []
  },
  "ethical_and_legal_considerations": {
    "privacy_and_anonymity": "N/A",
    "data_licensing": "N/A",
    "consent_procedures": "N/A",
    "compliance_with_regulations": "N/A"
  }
}