{
  "benchmark_details": {
    "is_benchmark": true,
    "name": "NUSAAKSARA",
    "abbreviation": "N/A",
    "overview": "NUSAAKSARA is a novel public benchmark for Indonesian languages that focuses on preserving indigenous scripts. It covers diverse tasks such as image segmentation, OCR, transliteration, translation, and language identification, constructed through expert annotation from various historical manuscripts and documents.",
    "data_type": "multimodal",
    "domains": [
      "Natural Language Processing"
    ],
    "languages": [
      "Indonesian"
    ],
    "similar_benchmarks": [],
    "resources": [
      "https://huggingface.co/datasets/NusaAksara/NusaAksara"
    ]
  },
  "purpose_and_intended_users": {
    "goal": "To safeguard and revitalize Indonesia's traditional scripts by providing a comprehensive benchmark dataset for research and development in NLP technologies.",
    "audience": [
      "ML Researchers",
      "Industry Practitioners",
      "Linguists",
      "Cultural Preservationists"
    ],
    "tasks": [
      "Image Segmentation",
      "Optical Character Recognition (OCR)",
      "Transliteration",
      "Machine Translation",
      "Language Identification"
    ],
    "limitations": "This study observed only eight of the recognized local scripts, and the lack of Unicode support for Lampung scripts presents a significant challenge for transcription-related tasks.",
    "out_of_scope_uses": []
  },
  "data": {
    "source": "The dataset contains scanned documents written in 8 different scripts created from historical manuscripts, literary works, and educational literature.",
    "size": "10,000 examples",
    "format": "Image and text files",
    "annotation": "Annotated by native speakers, linguists, and educators through expert validation processes."
  },
  "methodology": {
    "methods": [
      "Human evaluation",
      "Automated metrics"
    ],
    "metrics": [
      "Character Error Rate (CER)",
      "Word Error Rate (WER)",
      "BLEU Score",
      "chrF++"
    ],
    "calculation": "Metric calculations are done based on the output of systems against gold standard annotations for transliteration and translation tasks.",
    "interpretation": "Lower CER and WER indicate better OCR and transliteration performances, while higher BLEU and chrF++ scores reflect better translation quality.",
    "baseline_results": "Models struggle with OCR tasks, often yielding high error rates; specifically, proprietary models like GPT-4o perform better on Lontara scripts compared to open-source solutions.",
    "validation": "Results from validation indicated high agreement rates among annotators, particularly for transcription and translation tasks."
  },
  "targeted_risks": {
    "risk_categories": [
      "Bias",
      "Safety",
      "Privacy",
      "Robustness",
      "Fairness"
    ],
    "atlas_risks": {
      "risks": [
        {
          "category": "Accuracy",
          "subcategory": [
            "Unrepresentative data"
          ]
        },
        {
          "category": "Fairness",
          "subcategory": [
            "Data bias"
          ]
        }
      ]
    },
    "demographic_analysis": "The benchmark includes a variety of languages and scripts, focusing primarily on underrepresented and low-resource languages.",
    "harm": [
      "Cultural loss due to neglect of indigenous scripts"
    ]
  },
  "ethical_and_legal_considerations": {
    "privacy_and_anonymity": "N/A",
    "data_licensing": "N/A - Data is released under a non-commercial license.",
    "consent_procedures": "N/A",
    "compliance_with_regulations": "N/A"
  }
}