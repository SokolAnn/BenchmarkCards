{
  "benchmark_details": {
    "is_benchmark": true,
    "name": "Identifying Products in Online Cybercrime Marketplaces: A Dataset for Fine-grained Domain Adaptation",
    "abbreviation": "N/A",
    "overview": "A dataset and evaluation task for identifying products being bought and sold in online cybercrime forums. The authors formulate a token-level product identification task (a hybrid of slot-filling information extraction and named entity recognition), annotate data from four forums (Darkode, Hack Forums, Blackhat, and Nulled), characterize cross-forum (fine-grained domain) differences, evaluate supervised and semi-supervised/domain-adaptation methods, and release a dataset of 1,938 annotated posts.",
    "data_type": "text (token-level annotations of forum posts: product mentions)",
    "domains": [
      "Natural Language Processing",
      "Cybersecurity"
    ],
    "languages": [],
    "similar_benchmarks": [
      "TAC KBP",
      "CoNLL",
      "ACE"
    ],
    "resources": [
      "N/A"
    ]
  },
  "purpose_and_intended_users": {
    "goal": "To provide a dataset and evaluation setup for extracting product mentions in online cybercrime marketplace forums and to study fine-grained domain adaptation for this task.",
    "audience": [
      "Researchers in Natural Language Processing",
      "Researchers in Cybersecurity"
    ],
    "tasks": [
      "Named Entity Recognition",
      "Information Extraction",
      "Slot Filling",
      "Token-level product extraction"
    ],
    "limitations": "Annotation excludes product mentions outside the first and last 10 lines of each post; annotations are token-level (head word of multiword product NP annotated) rather than full span NP annotations; dataset consists of 1,938 annotated posts across four forums; standard semi-supervised and domain-adaptation techniques show limited effectiveness on cross-forum generalization.",
    "out_of_scope_uses": [
      "Features of products are not annotated",
      "Generic product references (e.g., this, them) are not annotated",
      "Product mentions inside “vouches” (reviews from other users) are not annotated",
      "Product mentions outside of the first and last 10 lines of each post are not annotated"
    ]
  },
  "data": {
    "source": "Posts collected from four online forums: Darkode, Hack Forums, Blackhat, and Nulled (all available posts were collected and a subset was annotated).",
    "size": "1,938 annotated posts; 130,336 annotated tokens; annotators considered 478,176 tokens during labeling.",
    "format": "Tokenized text (annotations performed on data tokenized with the Stanford CoreNLP toolkit); full annotation guide provided as a PDF with the data release.",
    "annotation": "Token-level annotation labeling each token as a product or not. Annotation guidelines developed over six preliminary rounds (560 posts). Three annotators labeled many portions (majority vote used to merge annotations); development/test disagreements were discussed and resolved. Multiword product expressions annotated by head word; methods (verbs) may be annotated in addition to nominal references."
  },
  "methodology": {
    "methods": [
      "Supervised learning: binary token classifier (SVM) and token-level CRF",
      "Post-level latent SVM classifier (select one token/NP per post)",
      "Retrained Stanford NER system",
      "Baselines: most-frequent noun/verb, dictionary/gazetteer lookup, first noun phrase baseline",
      "Semi-supervised features: Brown clusters",
      "Type-level annotation via gazetteers",
      "Token-level annotation and domain adaptation via feature-duplication (Daume III, 2007)",
      "Evaluation with human annotator baselines"
    ],
    "metrics": [
      "Precision",
      "Recall",
      "F1 Score (token-level)",
      "Precision",
      "Recall",
      "F1 Score (type-level product extraction per post)",
      "Post-level Accuracy",
      "NP-level (phrase-level) Precision/Recall/F1"
    ],
    "calculation": "Token-level: compute precision, recall, and F1 over tokens labeled as products. Type-level product extraction: compare the set of product types in a post (predicted vs. gold) using precision, recall, and F1; product type matching uses lowercasing and stemming with an edit-distance threshold (two product tokens are same type if after lowercasing and stemming edit distance ≤0 for token length ≤4, ≤1 for length 5–7, ≤2 for length ≥8). Post-level accuracy: check whether the first product type extracted by the system is contained in the annotated set. Phrase-level evaluation: project token-level annotations to noun phrases by parsing (using Chen and Manning parser) and labeling the largest NP containing the annotated token up to length 7; verbs left as single tokens.",
    "interpretation": "Token-level metrics closely mimic the annotation process. Type-level (per-post) metrics approximate KBP-style slot-filling and favor systems that correctly identify product types for posts even if not all token occurrences are retrieved. Post-level accuracy is appropriate because most posts contain a single core product; NP-level evaluation is more forgiving since token distinctions within NPs are erased.",
    "baseline_results": "Provided in-paper (e.g., human annotator token F1 ~89.8 on Darkode; various system results in Tables 2–4).",
    "validation": "Annotation quality assessed via inter-annotator agreement measured with Fleiss' Kappa (reported values in Table 1, e.g., Darkode 0.62/0.66; Hack Forums 0.58/0.65; Blackhat 0.66/0.67; Nulled 0.77). Annotation guidelines refined across six preliminary rounds with full discussion and resolution of disagreements; majority-vote merging for annotations; development/test disagreements were discussed and resolved to produce final annotations."
  },
  "targeted_risks": {
    "risk_categories": [],
    "atlas_risks": {
      "risks": null
    },
    "demographic_analysis": null,
    "harm": null
  },
  "ethical_and_legal_considerations": {
    "privacy_and_anonymity": "N/A",
    "data_licensing": "N/A",
    "consent_procedures": "N/A",
    "compliance_with_regulations": "N/A"
  }
}