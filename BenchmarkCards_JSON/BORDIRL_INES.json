{
  "benchmark_details": {
    "is_benchmark": true,
    "name": "BORDIRL INES (BorderLines for Information Retrieval and In Real Life)",
    "abbreviation": "BORDIRL INES",
    "overview": "BORDIRL INES is a multilingual dataset covering territorial disputes paired with retrieved Wikipedia documents across 49 languages. It evaluates the cross-lingual robustness of retrieval-augmented generation (RAG) systems.",
    "data_type": "query-document pairs",
    "domains": [
      "Natural Language Processing"
    ],
    "languages": [
      "English",
      "Spanish",
      "Chinese",
      "Hindi",
      "Arabic",
      "Korean",
      "Vietnamese",
      "French",
      "Russian",
      "Turkish",
      "Portuguese",
      "Indonesian",
      "Urdu",
      "Japanese",
      "Thai",
      "Burmese",
      "Hungarian",
      "Greek",
      "Swahili",
      "Somali",
      "Armenian",
      "Serbian",
      "Malay",
      "Azerbaijani",
      "Georgian",
      "Uighur",
      "Bene",
      "Tigrinya",
      "Basque",
      "Ukrainian",
      "Haitian"
    ],
    "similar_benchmarks": [],
    "resources": [
      "https://www.example.com/dataset/BORDIRL_INES",
      "https://github.com/username/BORDIRL_INES"
    ]
  },
  "purpose_and_intended_users": {
    "goal": "To provide a benchmark for assessing the cross-lingual robustness of retrieval-augmented generation systems on culturally-sensitive tasks.",
    "audience": [
      "ML Researchers",
      "Geopolitical Analysts",
      "Natural Language Processing Practitioners"
    ],
    "tasks": [
      "Multilingual Information Retrieval",
      "Cross-lingual Knowledge Retrieval"
    ],
    "limitations": "This study focuses on territorial disputes, which may not generalize to other types of queries.",
    "out_of_scope_uses": []
  },
  "data": {
    "source": "Wikipedia",
    "size": "19,916 unique query-document pairs",
    "format": "JSON",
    "annotation": "Annotated for relevance and viewpoint"
  },
  "methodology": {
    "methods": [
      "Cross-lingual retrieval evaluation",
      "Human evaluation for annotation",
      "LLM-based evaluation using various prompts"
    ],
    "metrics": [
      "Factuality",
      "Consistency",
      "Geopolitical Bias"
    ],
    "calculation": "Metrics calculated based on concurrence scores comparing responses across languages.",
    "interpretation": "Higher scores indicate better alignment with factual information sources and consistent outputs across languages.",
    "baseline_results": "Comparisons made against existing multilingual RAG systems.",
    "validation": "Results validated across 720 queries for 251 territories."
  },
  "targeted_risks": {
    "risk_categories": [
      "Bias",
      "Accuracy",
      "Robustness"
    ],
    "atlas_risks": {
      "risks": [
        {
          "category": "Fairness",
          "subcategory": [
            "Data bias"
          ]
        },
        {
          "category": "Accuracy",
          "subcategory": [
            "Poor model accuracy"
          ]
        }
      ]
    },
    "demographic_analysis": "Annotated data includes diverse language perspectives to mitigate biases.",
    "harm": "Potential for misinformation due to cultural sensitivities around territorial disputes."
  },
  "ethical_and_legal_considerations": {
    "privacy_and_anonymity": "N/A",
    "data_licensing": "Creative Commons",
    "consent_procedures": "Annotation participants were fully informed and could opt out of tasks due to sensitivity.",
    "compliance_with_regulations": "N/A"
  }
}