{
  "benchmark_details": {
    "is_benchmark": true,
    "name": "Human Robot Social Interaction (HSRI) Dataset",
    "abbreviation": "HSRI",
    "overview": "The HSRI Dataset aims to benchmark the capabilities of language models and foundational models to reason about social interactions, with annotations of robot social errors, competencies, rationale, and corrective actions in real-world human-robot interactions, consisting of âˆ¼400 real-world interaction videos and over 10K annotations.",
    "data_type": "video with annotations",
    "domains": [
      "Robotics",
      "Natural Language Processing"
    ],
    "languages": [
      "English"
    ],
    "similar_benchmarks": [
      "SocialIQA"
    ],
    "resources": [
      "N/A"
    ]
  },
  "purpose_and_intended_users": {
    "goal": "To provide a dataset and benchmark for measuring the social reasoning capabilities of AI agents in real-world human-robot interactions.",
    "audience": [
      "ML Researchers",
      "Robotics Developers"
    ],
    "tasks": [
      "Error Detection",
      "Competency Detection",
      "Social Attribute Identification",
      "Rationale and Correction Reasoning"
    ],
    "limitations": "N/A",
    "out_of_scope_uses": []
  },
  "data": {
    "source": "Collected from natural conversational interactions between humans and physical social robots.",
    "size": "440 interaction videos, 10,214 annotations",
    "format": "Video and annotation files",
    "annotation": "Manual annotation by multiple annotators based on defined categories of social errors and competencies."
  },
  "methodology": {
    "methods": [
      "Human evaluation",
      "Automated metrics"
    ],
    "metrics": [
      "Accuracy",
      "F1 Score"
    ],
    "calculation": "Metrics are calculated based on the predictions of models against ground truth annotations.",
    "interpretation": "Higher scores indicate better performance in detecting social errors and competencies.",
    "baseline_results": "Performance gaps were observed between AI models and human evaluators.",
    "validation": "Metrics are validated through multiple experimental trials with varying model inputs."
  },
  "targeted_risks": {
    "risk_categories": [
      "Bias",
      "Fairness",
      "Accuracy"
    ],
    "atlas_risks": {
      "risks": [
        {
          "category": "Accuracy",
          "subcategory": [
            "Unrepresentative data",
            "Poor model accuracy"
          ]
        },
        {
          "category": "Fairness",
          "subcategory": [
            "Data bias"
          ]
        }
      ]
    },
    "demographic_analysis": "N/A",
    "harm": "N/A"
  },
  "ethical_and_legal_considerations": {
    "privacy_and_anonymity": "The dataset has been collected under a newly accepted IRB protocol and anonymization processes are in place.",
    "data_licensing": "N/A",
    "consent_procedures": "N/A",
    "compliance_with_regulations": "N/A"
  }
}