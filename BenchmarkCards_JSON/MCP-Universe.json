{
  "benchmark_details": {
    "is_benchmark": true,
    "name": "MCP-Universe",
    "abbreviation": "N/A",
    "overview": "MCP-Universe is the first comprehensive benchmark specifically designed to evaluate LLMs in realistic and challenging tasks through interaction with real-world Model Context Protocol (MCP) servers across six domains, including Location Navigation, Repository Management, Financial Analysis, 3D Design, Browser Automation, and Web Searching.",
    "data_type": "task instances with real-world interaction",
    "domains": [
      "Natural Language Processing"
    ],
    "languages": [
      "English"
    ],
    "similar_benchmarks": [
      "MCP-RADAR",
      "MCPWorld"
    ],
    "resources": [
      "https://github.com/SalesforceAIResearch/MCP-Universe",
      "https://mcp-universe.github.io"
    ]
  },
  "purpose_and_intended_users": {
    "goal": "To evaluate large language models (LLMs) in realistic MCP environments, addressing long-context handling, tool unfamiliarity, and application across diverse scenarios.",
    "audience": [
      "ML Researchers",
      "Industry Practitioners",
      "Model Developers"
    ],
    "tasks": [
      "Location Navigation",
      "Repository Management",
      "Financial Analysis",
      "3D Design",
      "Browser Automation",
      "Web Searching"
    ],
    "limitations": "N/A",
    "out_of_scope_uses": []
  },
  "data": {
    "source": "Real-world MCP servers and user-created tasks.",
    "size": "231 tasks",
    "format": "N/A",
    "annotation": "Task instructions designed and validated by authors."
  },
  "methodology": {
    "methods": [
      "Execution-based evaluation"
    ],
    "metrics": [
      "Success rate"
    ],
    "calculation": "Success is determined through automated checks based on real-time data from MCP servers.",
    "interpretation": "Higher success rates indicate better model performance across diverse real-world tasks.",
    "baseline_results": "Top-performing models include GPT-5 (43.72% success rate), Grok-4 (33.33% success rate), and Claude-4.0-Sonnet (29.44% success rate).",
    "validation": "Automated evaluation processes are designed to ensure rigorous testing and accurate task completion assessment."
  },
  "targeted_risks": {
    "risk_categories": [
      "Accuracy",
      "Robustness"
    ],
    "atlas_risks": {
      "risks": [
        {
          "category": "Accuracy",
          "subcategory": [
            "Unrepresentative data"
          ]
        },
        {
          "category": "Robustness",
          "subcategory": [
            "Prompt injection attack"
          ]
        }
      ]
    },
    "demographic_analysis": "N/A",
    "harm": [
      "Misuse"
    ]
  },
  "ethical_and_legal_considerations": {
    "privacy_and_anonymity": "N/A",
    "data_licensing": "N/A",
    "consent_procedures": "N/A",
    "compliance_with_regulations": "N/A"
  }
}