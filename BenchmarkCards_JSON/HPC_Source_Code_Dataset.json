{
  "benchmark_details": {
    "is_benchmark": true,
    "name": "HPC Source Code Dataset",
    "abbreviation": "N/A",
    "overview": "Introduce a new dataset of HPC and scientific codes from popular open-source repositories and use it to fine-tune pre-trained language models; compare several pre-trained LLMs on HPC-related tasks and introduce HPC-Coder, fine-tuned on parallel codes, which can auto-complete HPC functions, decorate for loops with OpenMP pragmas, and model performance changes in scientific application repositories and programming competition solutions.",
    "data_type": "text (C/C++ source code and code pairs with performance/run-time labels)",
    "domains": [
      "High Performance Computing",
      "Scientific Computing",
      "Software Engineering"
    ],
    "languages": [],
    "similar_benchmarks": [
      "HumanEval"
    ],
    "resources": [
      "N/A"
    ]
  },
  "purpose_and_intended_users": {
    "goal": "To train and evaluate large language models specifically on HPC and scientific code to enable HPC-specific code generation, OpenMP pragma prediction, and relative performance prediction of code changes.",
    "audience": [
      "Computational Scientists",
      "HPC Developers",
      "Software Developers"
    ],
    "tasks": [
      "Code Completion",
      "Program Annotation (OpenMP Pragma Prediction)",
      "Performance Prediction",
      "Binary Classification"
    ],
    "limitations": "Performance depends on numerous variables besides just the code such as input problem, architecture, and current machine load/congestion; these either need to be fixed in the dataset or accounted for within the modeling pipeline.",
    "out_of_scope_uses": []
  },
  "data": {
    "source": "Collected from GitHub repositories: repositories with C/C++ marked as the primary language and with \u000263 stars, filtered by HPC-related GitHub topics; all C/C++ source files pulled by file extension. Additional performance datasets: commits for Kripke and Laghos applications (automated build/run to collect performance for commits) and programming competition solutions aggregated from Aizu, AtCoder, CodeChef, CodeForces, and HackerEarth (code contests dataset).",
    "size": "HPC source code dataset after deduplication and removing small/large files: 196,140 files, 50,017,351 lines of code, 1.62 GB. Performance datasets: 830 commits (Kripke + Laghos); ~1.7 million C++ samples (programming competition solutions) used to create paired samples. OpenMP pragma dataset: 13,900 samples.",
    "format": "Original data: C/C++ source files. Tokenization: pre-trained GPT-2 Byte-Pair Encoding (BPE) tokenizers used for selected models (tokenized integer sequences).",
    "annotation": "Deduplicated by sha256 hash of file contents. OpenMP pragma dataset: extracted every for loop with an OpenMP pragma and included 500 tokens of prior context; pragmas repositioned after the loop with a separating token <begin-omp>. Performance datasets: pairs labeled as 'slower' or 'same/faster' based on measured run times; code pairs separated by tokens <COMMIT> (version control) or <PAIR> (contest pairs)."
  },
  "methodology": {
    "methods": [
      "Fine-tuning pre-trained autoregressive language models (GPT-2/GPT-Neo/PolyCoder variants) on the collected HPC source code dataset using HuggingFace Trainer with DeepSpeed backend",
      "Automated evaluation via compilation and functional testing of generated code",
      "Evaluation of OpenMP pragma generation using syntactic equivalence and functional-correctness comparisons",
      "Binary classification fine-tuning for relative performance prediction"
    ],
    "metrics": [
      "Perplexity",
      "pass@k",
      "Accuracy",
      "Build success rate (compilation rate)"
    ],
    "calculation": "Perplexity is calculated as the exponential of the cross-entropy training loss. pass@k is computed as pass@k = 1 - (comb(Np - cp, k) / comb(Np, k)) where cp is number of functionally correct samples out of Np generated samples for a prompt; average pass@k is averaged over prompts. Accuracy = # correct / total for OpenMP pragma prediction and relative performance classification. Build success rate = percentage of generated samples that compile successfully.",
    "interpretation": "Lower Perplexity indicates better language modeling (model confidence) and generally correlates with better downstream performance. Higher pass@k, higher Accuracy, and higher build success rate indicate better code generation, pragma prediction, and performance prediction respectively.",
    "baseline_results": "Final validation perplexities after fine-tuning on HPC source code dataset: GPT-2: 4.47, GPT-Neo: 2.23, PolyCoder: 2.24. Code generation (PolyCoder+HPC / HPC-Coder) average pass@100 ~71%, pass@1 ~25%. Build success (compilation) rates: PolyCoder+HPC 86%, PolyCoder 84%, GPT-Neo+HPC 74%, GPT-2+HPC 30%. OpenMP pragma functional correctness: PolyCoder+HPC 97% accuracy, PolyCoder 94%; exact textual equivalence: PolyCoder+HPC 67%, PolyCoder 61%. Relative performance classification accuracy: PolyCoder+HPC 88% (proxy applications), 92% (coding competitions); PolyCoder 86% (both datasets reported at 86% in figures).",
    "validation": "During fine-tuning: validation dataset is 5% of full dataset (separate from training). For OpenMP pragma fine-tuning: 10% set aside for validation. For relative performance prediction: 90% train / 10% evaluation split. Code generation prompts: Np=100 samples per prompt; nucleus sampling with p=0.93 and multiple temperatures tested (0.1, 0.2, 0.4, 0.6, 0.8)."
  },
  "targeted_risks": {
    "risk_categories": [
      "Fairness",
      "Transparency"
    ],
    "atlas_risks": {
      "risks": [
        {
          "category": "Fairness",
          "subcategory": [
            "Data bias"
          ]
        },
        {
          "category": "Transparency",
          "subcategory": [
            "Lack of training data transparency"
          ]
        }
      ]
    },
    "demographic_analysis": "N/A",
    "harm": null
  },
  "ethical_and_legal_considerations": {
    "privacy_and_anonymity": "N/A",
    "data_licensing": "N/A",
    "consent_procedures": "N/A",
    "compliance_with_regulations": "N/A"
  }
}