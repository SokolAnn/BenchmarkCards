{
  "benchmark_details": {
    "is_benchmark": true,
    "name": "HW-MLVQA (Handwritten Multilingual Visual Question Answering)",
    "abbreviation": "HW-MLVQA",
    "overview": "HW-MLVQA is a comprehensive benchmark designed for handwritten multilingual visual question answering, addressing the gap in multilingual document comprehension and the intricacy of handwriting. It enhances VQA systems' capabilities by enabling them to process and interpret handwritten questions across multiple languages.",
    "data_type": "handwritten pages and question-answer pairs",
    "domains": [
      "Natural Language Processing",
      "Computer Vision"
    ],
    "languages": [
      "English",
      "Hindi"
    ],
    "similar_benchmarks": [
      "HW-SQuAD",
      "Bentham-QA"
    ],
    "resources": [
      "https://arxiv.org/abs/2507.15655"
    ]
  },
  "purpose_and_intended_users": {
    "goal": "To facilitate pivotal advancements in multilingual handwritten document interpretation, fostering innovation and scholarly inquiry within this specialized domain.",
    "audience": [
      "ML Researchers",
      "Domain Experts"
    ],
    "tasks": [
      "Visual Question Answering"
    ],
    "limitations": "N/A",
    "out_of_scope_uses": []
  },
  "data": {
    "source": "The dataset consists of handwritten documents created from contexts drawn from the SQuAD and MLQA datasets, coupled with a web-based platform for transcription by volunteers.",
    "size": "1,600 handwritten pages and 2,400 question-answer pairs",
    "format": "JSON",
    "annotation": "Manual transcription by volunteers followed by an automatic annotation pipeline to generate and verify bounding boxes."
  },
  "methodology": {
    "methods": [
      "Human evaluation",
      "Automated metrics"
    ],
    "metrics": [
      "Exact Match (EM)",
      "F1 Score",
      "Average Normalized Levenshtein Similarity (ANLS)"
    ],
    "calculation": "Metrics are calculated to evaluate the accuracy of generated answers and quality of evidence retrieval.",
    "interpretation": "Higher scores indicate better performance in accurately answering questions and retrieving evidence from handwritten pages.",
    "baseline_results": "Models evaluated include LLaMA 3.1 for text and Qwen2VL for vision-language tasks.",
    "validation": "Rigorous verification of annotations and consistency checks during the annotation process."
  },
  "targeted_risks": {
    "risk_categories": [
      "Bias",
      "Accuracy",
      "Robustness"
    ],
    "atlas_risks": {
      "risks": [
        {
          "category": "Accuracy",
          "subcategory": [
            "Unrepresentative data"
          ]
        },
        {
          "category": "Robustness",
          "subcategory": [
            "Prompt injection attack",
            "Evasion attack"
          ]
        }
      ]
    },
    "demographic_analysis": "N/A",
    "harm": []
  },
  "ethical_and_legal_considerations": {
    "privacy_and_anonymity": "N/A",
    "data_licensing": "N/A",
    "consent_procedures": "N/A",
    "compliance_with_regulations": "N/A"
  }
}