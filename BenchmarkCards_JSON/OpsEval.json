{
  "benchmark_details": {
    "is_benchmark": true,
    "name": "OpsEval: A Comprehensive Benchmark Suite for Evaluating Large Language Modelsâ€™ Capability in IT Operations Domain",
    "abbreviation": "OpsEval",
    "overview": "OpsEval is a comprehensive benchmark suite designed for evaluating the performance of large language models (LLMs) in the IT operations domain, focusing on ensuring the stability, reliability, and robustness of complex IT systems through specialized question-answering tasks.",
    "data_type": "question-answering pairs",
    "domains": [
      "Natural Language Processing"
    ],
    "languages": [
      "English",
      "Chinese"
    ],
    "similar_benchmarks": [],
    "resources": [
      "https://github.com/NetManAIOps/OpsEval-Datasets",
      "https://opseval.cstcloud.cn/content/leaderboard"
    ]
  },
  "purpose_and_intended_users": {
    "goal": "To provide a reliable evaluation framework for assessing LLMs in IT operations tasks, facilitating the optimization of models for this specialized field.",
    "audience": [
      "ML Researchers",
      "Domain Practitioners"
    ],
    "tasks": [
      "Question Answering"
    ],
    "limitations": "N/A",
    "out_of_scope_uses": []
  },
  "data": {
    "source": "Data collected from various sources including company materials, certification exams, and operations textbooks.",
    "size": "9,070 questions",
    "format": "N/A",
    "annotation": "Manual review and expert annotations were conducted to ensure quality and relevance."
  },
  "methodology": {
    "methods": [
      "Human evaluation",
      "Automated metrics"
    ],
    "metrics": [
      "FAE-Score",
      "BLEU",
      "ROUGE"
    ],
    "calculation": "FAE-Score evaluates model responses based on fluency, accuracy, and evidence, with each criterion assessed through dedicated methodologies.",
    "interpretation": "Higher scores indicate better fluency, factual accuracy, and reliance on supporting evidence. FAE-Score aligns closely with human expert evaluations.",
    "baseline_results": "N/A",
    "validation": "The benchmark underwent data leakage testing to ensure reliability."
  },
  "targeted_risks": {
    "risk_categories": [
      "Bias",
      "Fairness",
      "Safety",
      "Robustness"
    ],
    "atlas_risks": {
      "risks": [
        {
          "category": "Fairness",
          "subcategory": [
            "Data bias"
          ]
        },
        {
          "category": "Robustness",
          "subcategory": [
            "Evasion attack"
          ]
        }
      ]
    },
    "demographic_analysis": "N/A",
    "harm": "N/A"
  },
  "ethical_and_legal_considerations": {
    "privacy_and_anonymity": "Measures to handle sensitive data were employed during data collection.",
    "data_licensing": "The dataset is partially released under the CC BY-NC 4.0 license.",
    "consent_procedures": "N/A",
    "compliance_with_regulations": "N/A"
  }
}