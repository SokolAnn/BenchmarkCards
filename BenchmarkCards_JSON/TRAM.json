{
  "benchmark_details": {
    "is_benchmark": true,
    "name": "TRAM (Temporal Reasoning for Large Language Models)",
    "abbreviation": "TRAM",
    "overview": "TRAM is a temporal reasoning benchmark composed of ten datasets, encompassing various temporal aspects of events such as order, arithmetic, frequency, and duration, designed for evaluating the TeR capabilities of large language models.",
    "data_type": "multiple-choice questions",
    "domains": [
      "Natural Language Processing"
    ],
    "languages": [
      "English"
    ],
    "similar_benchmarks": [
      "TimeBank",
      "TempEval-3",
      "MCTACO"
    ],
    "resources": [
      "https://github.com/EternityYW/TRAM-Benchmark"
    ]
  },
  "purpose_and_intended_users": {
    "goal": "To establish a standard for evaluating temporal reasoning in large language models.",
    "audience": [
      "ML Researchers",
      "Industry Practitioners",
      "Model Developers"
    ],
    "tasks": [
      "Temporal Reasoning"
    ],
    "limitations": "N/A",
    "out_of_scope_uses": [
      "Evaluation tasks that diverge from temporal reasoning such as those requiring contextual emotional intelligence."
    ]
  },
  "data": {
    "source": "Combination of existing datasets (MCTACO, SQuAD) and human-curated problems.",
    "size": "526,668 questions",
    "format": "multiple-choice questions",
    "annotation": "Derived through a combination of expert annotations and programmatic generation."
  },
  "methodology": {
    "methods": [
      "Human evaluation",
      "Automated metrics"
    ],
    "metrics": [
      "Accuracy",
      "F1 Score"
    ],
    "calculation": "Metrics are calculated based on the performance of LLMs on the multiple-choice format of the tasks.",
    "interpretation": "Higher accuracy indicates better temporal reasoning capabilities of the models.",
    "baseline_results": "Human experts achieved an average accuracy of 95.2%.",
    "validation": "Extensive evaluation was conducted on various popular language models."
  },
  "targeted_risks": {
    "risk_categories": [
      "Accuracy",
      "Fairness"
    ],
    "atlas_risks": {
      "risks": [
        {
          "category": "Accuracy",
          "subcategory": [
            "Unrepresentative data"
          ]
        },
        {
          "category": "Fairness",
          "subcategory": [
            "Data bias"
          ]
        }
      ]
    },
    "demographic_analysis": "N/A",
    "harm": "N/A"
  },
  "ethical_and_legal_considerations": {
    "privacy_and_anonymity": "N/A",
    "data_licensing": "N/A",
    "consent_procedures": "N/A",
    "compliance_with_regulations": "N/A"
  }
}