{
  "benchmark_details": {
    "is_benchmark": true,
    "name": "The Qiyas Benchmark",
    "abbreviation": "N/A",
    "overview": "The Qiyas Benchmark is designed to evaluate models' mathematical reasoning and language understanding abilities in Arabic, derived from a General Aptitude Test (GAT) called Qiyas exam, widely used for university admissions in Saudi Arabia.",
    "data_type": "multiple-choice questions",
    "domains": [
      "Natural Language Processing",
      "Education"
    ],
    "languages": [
      "Arabic"
    ],
    "similar_benchmarks": [
      "N/A"
    ],
    "resources": [
      "N/A"
    ]
  },
  "purpose_and_intended_users": {
    "goal": "To comprehensively assess the mathematical reasoning and language understanding capabilities of large language models (LLMs) in the Arabic language.",
    "audience": [
      "ML Researchers",
      "Educational Assessors",
      "Model Developers"
    ],
    "tasks": [
      "Mathematical Reasoning",
      "Language Understanding"
    ],
    "limitations": "N/A",
    "out_of_scope_uses": []
  },
  "data": {
    "source": "Derived from the Qiyas exam, a standardized test widely used for university admissions in Saudi Arabia.",
    "size": "2,407 questions",
    "format": "N/A",
    "annotation": "Questions were written by domain experts experienced in designing and grading Qiyas exams."
  },
  "methodology": {
    "methods": [
      "OpenAI's API evaluation"
    ],
    "metrics": [
      "Accuracy"
    ],
    "calculation": "Accuracy calculated as the number of correct answers in total questions multiplied by 100.",
    "interpretation": "A higher accuracy indicates better model performance in understanding and solving the tasks presented.",
    "baseline_results": "ChatGPT-4 achieved an overall average accuracy of 64%, while ChatGPT-3.5-turbo achieved an overall accuracy of 49%.",
    "validation": "N/A"
  },
  "targeted_risks": {
    "risk_categories": [
      "Bias",
      "Accuracy"
    ],
    "atlas_risks": {
      "risks": [
        {
          "category": "Accuracy",
          "subcategory": [
            "Unrepresentative data",
            "Poor model accuracy"
          ]
        }
      ]
    },
    "demographic_analysis": "N/A",
    "harm": "N/A"
  },
  "ethical_and_legal_considerations": {
    "privacy_and_anonymity": "N/A",
    "data_licensing": "N/A",
    "consent_procedures": "N/A",
    "compliance_with_regulations": "N/A"
  }
}