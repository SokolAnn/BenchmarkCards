{
  "benchmark_details": {
    "is_benchmark": true,
    "name": "FORMOSAN BENCH",
    "abbreviation": "N/A",
    "overview": "FORMOSAN BENCH is the first benchmark for evaluating LLMs on low-resource Austronesian languages, specifically covering three endangered Formosan languages: Atayal, Amis, and Paiwan, across three core NLP tasks: machine translation, automatic speech recognition, and text summarization.",
    "data_type": "text",
    "domains": [
      "Natural Language Processing"
    ],
    "languages": [
      "Atayal",
      "Amis",
      "Paiwan"
    ],
    "similar_benchmarks": [
      "IrokoBench"
    ],
    "resources": [
      "https://anonymous.4open.science/r/FormosanBench-DB43/"
    ]
  },
  "purpose_and_intended_users": {
    "goal": "To evaluate the performance of large language models on low-resource Formosan languages across several NLP tasks and to underscore the need for more effective NLP technologies for endangered languages.",
    "audience": [
      "ML Researchers",
      "NLP Practitioners",
      "Linguists"
    ],
    "tasks": [
      "Machine Translation",
      "Automatic Speech Recognition",
      "Text Summarization"
    ],
    "limitations": "The current study presents results for only a limited number of Formosan languages, and incorporating additional NLP tasks is constrained by substantial human effort for data annotation and validation.",
    "out_of_scope_uses": []
  },
  "data": {
    "source": "Datasets were sourced from the Taiwan Indigenous Languages E-Dictionary and the KlokÂ¯a Digital Platform, curated from Wikipedia articles, and designed for machine translation, ASR, and summarization tasks.",
    "size": "5,000 sentence pairs for machine translation; various counts for ASR and summarization datasets.",
    "format": "JSON",
    "annotation": "Datasets were manually curated and verified for quality and accuracy prior to release."
  },
  "methodology": {
    "methods": [
      "Human evaluation",
      "Automated metrics"
    ],
    "metrics": [
      "BLEU Score",
      "Word Error Rate (WER)",
      "ROUGE-L"
    ],
    "calculation": "Metrics were calculated to evaluate the performance of models across translation, speech recognition, and summarization tasks based on standard evaluation techniques.",
    "interpretation": "BLEU scores provide precision in translation tasks, while WER and ROUGE assess the accuracy of speech recognition and summarization outputs, respectively.",
    "baseline_results": "No explicit baseline results were detailed in the paper.",
    "validation": "Datasets underwent task-specific quality control to ensure meaningful training and evaluation."
  },
  "targeted_risks": {
    "risk_categories": [
      "Bias",
      "Performance"
    ],
    "atlas_risks": {
      "risks": []
    },
    "demographic_analysis": "Analysis of demographic factors is not explicitly discussed.",
    "harm": []
  },
  "ethical_and_legal_considerations": {
    "privacy_and_anonymity": "N/A",
    "data_licensing": "N/A",
    "consent_procedures": "N/A",
    "compliance_with_regulations": "N/A"
  }
}