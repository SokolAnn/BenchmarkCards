{
  "benchmark_details": {
    "is_benchmark": true,
    "name": "Multi-P2A (Multi-perspective Benchmark on Privacy Assessment for Large Vision-Language Models)",
    "abbreviation": "Multi-P2A",
    "overview": "Multi-P2A is a comprehensive benchmark for evaluating the privacy preservation capabilities of large vision-language models (LVLMs) in terms of privacy awareness and leakage. The benchmark covers 26 categories of personal privacy, 15 categories of trade secrets, and 18 categories of state secrets, totaling 31,962 samples. It assesses the modelâ€™s ability to recognize privacy sensitivity in input data and the risk of unintentionally disclosing private information in its output.",
    "data_type": "question-answering pairs",
    "domains": [
      "Natural Language Processing",
      "Computer Vision"
    ],
    "languages": [
      "English"
    ],
    "similar_benchmarks": [
      "TrustLLM",
      "MultiTrust"
    ],
    "resources": [
      "https://arxiv.org/abs/2412.19496"
    ]
  },
  "purpose_and_intended_users": {
    "goal": "To systematically analyze the limitations and vulnerabilities inherent in existing privacy preservation mechanisms of LVLMs, and to inform the development of more robust privacy-preserving models.",
    "audience": [
      "ML Researchers",
      "Model Developers"
    ],
    "tasks": [
      "Privacy Awareness",
      "Privacy Leakage",
      "Visual Question Answering"
    ],
    "limitations": "N/A",
    "out_of_scope_uses": []
  },
  "data": {
    "source": "Collected from existing datasets and social media platforms, including a mix of personal images, trade secrets, and state secrets.",
    "size": "31,962 samples",
    "format": "N/A",
    "annotation": "Manually filtered and annotated to ensure high quality."
  },
  "methodology": {
    "methods": [
      "Human evaluation",
      "Automated metrics"
    ],
    "metrics": [
      "Accuracy",
      "Pearson correlation coefficient",
      "Refuse-to-Answer (RtA)",
      "Expect-to-Answer (EtA)"
    ],
    "calculation": "Metrics are calculated based on the model's performance in recognizing privacy-sensitive data and tasks related to privacy awareness and leakage.",
    "interpretation": "Higher accuracy signifies stronger privacy awareness, and RtA reflects the model's ability to preserve privacy by refusing to answer sensitive questions.",
    "baseline_results": null,
    "validation": "N/A"
  },
  "targeted_risks": {
    "risk_categories": [
      "Privacy",
      "Safety"
    ],
    "atlas_risks": {
      "risks": [
        {
          "category": "Privacy",
          "subcategory": [
            "Personal information in data",
            "Data privacy rights alignment"
          ]
        },
        {
          "category": "Fairness",
          "subcategory": [
            "Data bias"
          ]
        },
        {
          "category": "Robustness",
          "subcategory": [
            "Evasion attack"
          ]
        }
      ]
    },
    "demographic_analysis": "N/A",
    "harm": []
  },
  "ethical_and_legal_considerations": {
    "privacy_and_anonymity": "Images collected are for academic research purposes and adhere to privacy standards.",
    "data_licensing": "N/A",
    "consent_procedures": "N/A",
    "compliance_with_regulations": "N/A"
  }
}