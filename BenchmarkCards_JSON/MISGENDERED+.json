{
  "benchmark_details": {
    "is_benchmark": true,
    "name": "MISGENDERED+",
    "abbreviation": "MISGENDERED+",
    "overview": "MISGENDERED+ is an enhanced and updated benchmark for evaluating large language models' ability to handle inclusive pronoun usage, particularly focusing on nonbinary pronouns and the ability to infer gender identity from pronoun usage in context.",
    "data_type": "templated instances for pronoun fidelity evaluation",
    "domains": [
      "Natural Language Processing"
    ],
    "languages": [
      "English"
    ],
    "similar_benchmarks": [
      "MISGENDERED"
    ],
    "resources": [
      "N/A"
    ]
  },
  "purpose_and_intended_users": {
    "goal": "To provide a comprehensive testbed for evaluating the handling of inclusive pronouns by language models and identify biases in their usage.",
    "audience": [
      "ML Researchers",
      "Gender Studies Scholars",
      "Ethics in AI Practitioners"
    ],
    "tasks": [
      "Pronoun Resolution",
      "Gender Identity Inference"
    ],
    "limitations": "N/A",
    "out_of_scope_uses": []
  },
  "data": {
    "source": "Constructed using diverse names and pronoun categories with a focus on inclusive pronouns.",
    "size": "3.8 million templated instances",
    "format": "N/A",
    "annotation": "Manually annotated with varied context and pronoun usage scenarios."
  },
  "methodology": {
    "methods": [
      "Zero-shot prompting",
      "Few-shot prompting"
    ],
    "metrics": [
      "Accuracy"
    ],
    "calculation": "Metrics calculated based on models' performance on different pronoun types and inference tasks.",
    "interpretation": "Higher accuracy indicates better understanding and handling of gender-inclusive language by LLMs.",
    "baseline_results": "Previously measured results on the MISGENDERED benchmark.",
    "validation": "N/A"
  },
  "targeted_risks": {
    "risk_categories": [
      "Bias",
      "Fairness"
    ],
    "atlas_risks": {
      "risks": [
        {
          "category": "Fairness",
          "subcategory": [
            "Data bias"
          ]
        },
        {
          "category": "Accuracy",
          "subcategory": [
            "Poor model accuracy"
          ]
        }
      ]
    },
    "demographic_analysis": "N/A",
    "harm": [
      "Misgendering risks in language models leading to potential social and emotional harm."
    ]
  },
  "ethical_and_legal_considerations": {
    "privacy_and_anonymity": "N/A",
    "data_licensing": "N/A",
    "consent_procedures": "N/A",
    "compliance_with_regulations": "N/A"
  }
}