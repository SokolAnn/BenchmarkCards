{
  "benchmark_details": {
    "is_benchmark": true,
    "name": "LegalSemi",
    "abbreviation": "N/A",
    "overview": "LegalSemi is a benchmark specifically curated for legal scenario analysis, comprising 54 legal scenarios annotated by legal experts based on the IRAC (Issue, Rule, Application, Conclusion) framework from Malaysian Contract Law. It aims to address limitations in existing datasets by providing a structured knowledge base (SKE) for enhancing legal reasoning tasks.",
    "data_type": "legal scenarios with IRAC annotations",
    "domains": [
      "Legal"
    ],
    "languages": [
      "English"
    ],
    "similar_benchmarks": [
      "SARA",
      "SIRAC",
      "Legal Bench"
    ],
    "resources": [
      "https://doi.org/10.1007/s10506-025-09467-5"
    ]
  },
  "purpose_and_intended_users": {
    "goal": "To provide a curated benchmark for legal scenario analysis using the IRAC framework, enhancing the research and application of large language models in legal reasoning.",
    "audience": [
      "ML Researchers",
      "Legal Technology Developers",
      "Legal Practitioners"
    ],
    "tasks": [
      "Legal Scenario Analysis",
      "IRAC Reasoning Tasks"
    ],
    "limitations": "N/A",
    "out_of_scope_uses": []
  },
  "data": {
    "source": "Curated from legal textbooks and real-world legal problems, annotated by legal experts.",
    "size": "54 legal scenarios",
    "format": "Structured JSON",
    "annotation": "Rigorous annotation by legal experts following the IRAC framework."
  },
  "methodology": {
    "methods": [
      "Human evaluation",
      "Automated metrics"
    ],
    "metrics": [
      "Accuracy",
      "F1 Score",
      "Precision",
      "Recall"
    ],
    "calculation": "Metrics calculated based on the comparison of human and automated evaluations of legal reasoning tasks.",
    "interpretation": "A model's performance is considered good when it generates accurate and coherent legal analyses.",
    "baseline_results": "N/A",
    "validation": "Performance of LLMs evaluated through comparison against a structured knowledge base and expert evaluations."
  },
  "targeted_risks": {
    "risk_categories": [
      "Accuracy",
      "Fairness",
      "Privacy"
    ],
    "atlas_risks": {
      "risks": [
        {
          "category": "Accuracy",
          "subcategory": [
            "Poor model accuracy"
          ]
        },
        {
          "category": "Fairness",
          "subcategory": [
            "Data bias"
          ]
        }
      ]
    },
    "demographic_analysis": "N/A",
    "harm": "N/A"
  },
  "ethical_and_legal_considerations": {
    "privacy_and_anonymity": "N/A",
    "data_licensing": "N/A",
    "consent_procedures": "N/A",
    "compliance_with_regulations": "N/A"
  }
}