{
  "benchmark_details": {
    "is_benchmark": true,
    "name": "AC-EVAL (Ancient Chinese Evaluation Benchmark)",
    "abbreviation": "AC-EVAL",
    "overview": "AC-EVAL is an innovative benchmark designed to assess the advanced knowledge and reasoning capabilities of LLMs within the context of ancient Chinese. It includes tasks across three levels of difficulty, incorporating general historical knowledge, short text understanding, and long text comprehension.",
    "data_type": "multiple-choice questions",
    "domains": [
      "Natural Language Processing"
    ],
    "languages": [
      "Chinese"
    ],
    "similar_benchmarks": [
      "MMLU",
      "BIG-bench",
      "HELM",
      "C-Eval",
      "CMMLU",
      "SuperCLUE"
    ],
    "resources": [
      "https://github.com/yuting-wei/AC-EVAL"
    ]
  },
  "purpose_and_intended_users": {
    "goal": "To provide a multidimensional evaluation tool for assessing LLMsâ€™ proficiency in ancient Chinese while highlighting potential areas for improvement.",
    "audience": [
      "ML Researchers",
      "Educators in Ancient Chinese Studies",
      "Scholars in Classical Literature"
    ],
    "tasks": [
      "Historical Knowledge Assessment",
      "Short Text Understanding",
      "Long Text Comprehension"
    ],
    "limitations": "N/A",
    "out_of_scope_uses": []
  },
  "data": {
    "source": "Derived from comprehensive collections of ancient Chinese texts, specialized books, official exams, and existing datasets.",
    "size": "3,245 multiple-choice questions",
    "format": "Excel",
    "annotation": "Manual annotation by linguistics experts and undergraduate students."
  },
  "methodology": {
    "methods": [
      "Human evaluation"
    ],
    "metrics": [
      "Accuracy"
    ],
    "calculation": "Accuracy is calculated based on the number of correct predictions versus total predictions.",
    "interpretation": "Higher accuracy indicates better LLM understanding and reasoning capabilities in ancient Chinese language.",
    "baseline_results": null,
    "validation": "Evaluation procedures include submitting model predictions for accuracy assessment against predefined ground-truth labels."
  },
  "targeted_risks": {
    "risk_categories": [
      "Bias",
      "Accuracy"
    ],
    "atlas_risks": {
      "risks": [
        {
          "category": "Accuracy",
          "subcategory": [
            "Poor model accuracy",
            "Unrepresentative data"
          ]
        }
      ]
    },
    "demographic_analysis": "N/A",
    "harm": "N/A"
  },
  "ethical_and_legal_considerations": {
    "privacy_and_anonymity": "N/A",
    "data_licensing": "N/A",
    "consent_procedures": "N/A",
    "compliance_with_regulations": "N/A"
  }
}