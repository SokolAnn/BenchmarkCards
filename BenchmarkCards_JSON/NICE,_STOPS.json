{
  "benchmark_details": {
    "is_benchmark": true,
    "name": "NICE and STOPS (Short Texts Of Products and Services)",
    "abbreviation": "NICE, STOPS",
    "overview": "We introduce two new real-world short text datasets in the domain of goods and services (NICE and STOPS) to cover additional dataset characteristics in a realistic use-case and examine the performance of a variety of short text classifiers as well as top performing traditional text classifiers. NICE is a classification system for goods and services that divides them into 45 classes and is based on the Nice Classification of the World Intellectual Property Organization. The Short Texts Of Products and Services (STOPS) dataset is based on Amazon product descriptions and Yelp business entries.",
    "data_type": "short text (product and service descriptions) - classification pairs",
    "domains": [
      "Natural Language Processing",
      "Tax Auditing",
      "Product/Service Classification"
    ],
    "languages": [],
    "similar_benchmarks": [
      "R8",
      "MR",
      "SearchSnippets",
      "Twitter",
      "TREC",
      "SST-2"
    ],
    "resources": [
      "https://github.com/FKarl/short-text-classification",
      "https://arxiv.org/abs/2211.16878",
      "http://www.daviddlewis.com/resources/testcollections/reuters21578/",
      "https://www.cs.cornell.edu/people/pabo/movie-review-data/",
      "http://jwebpro.sourceforge.net/data-web-snippets.tar.gz",
      "https://www.nltk.org/howto/twitter.html#Using-a-Tweet-Corpus",
      "https://cogcomp.seas.upenn.edu/Data/QA/QC/",
      "https://nlp.stanford.edu/sentiment/",
      "http://webdatacommons.org/largescaleproductcorpus/",
      "https://www.wipo.int/nice/its4nice/ITSupport_and_download_area/20220101/MasterFiles/index.html",
      "https://github.com/google-research-datasets/MAVE",
      "https://www.yelp.com/dataset/download"
    ]
  },
  "purpose_and_intended_users": {
    "goal": "Compare various modern text classification techniques on short texts, introduce two new real-world datasets in the goods and services domain (NICE and STOPS) to cover additional dataset characteristics, and evaluate whether Transformers achieve state-of-the-art accuracy on short text classification.",
    "audience": [
      "ML Researchers",
      "Model Developers",
      "Industry Practitioners",
      "Tax Auditors"
    ],
    "tasks": [
      "Text Classification",
      "Product/Service Classification",
      "Short Text Classification"
    ],
    "limitations": "Most experiments were conducted once (except SHINE and InducT-GCN which were run five times). STOPS contains user-generated labels, some of which may not be entirely accurate. Existing benchmark datasets share many characteristics and may not cover all real-world use cases.",
    "out_of_scope_uses": []
  },
  "data": {
    "source": "NICE: WIPO Nice Classification data converted and preprocessed. STOPS: derived from MAVE (Amazon product descriptions) and YELP business entries; benchmark datasets used: R8 (Reuters 21578 subset), MR, SearchSnippets, Twitter, TREC, SST-2.",
    "size": "NICE-45: 9,593 documents (6,715 train / 2,878 test). NICE-2: 9,593 documents (6,715 train / 2,878 test). STOPS-41: 200,341 documents (140,238 train / 60,103 test). STOPS-2: 200,341 documents (140,238 train / 60,103 test). Benchmark datasets: R8 7,674; MR 10,662; SearchSnippets 12,340; Twitter 10,000; TREC 5,952; SST-2 9,613 (train/test splits provided in Table 1).",
    "format": "N/A",
    "annotation": "Labels derived from existing sources: NICE uses WIPO Nice Classification labels; STOPS labels derived from MAVE and YELP original labels (multi-label categories were mapped to the most common single label). No additional manual annotation procedure is described."
  },
  "methodology": {
    "methods": [
      "Automated metrics (Accuracy)",
      "Model-based evaluation (comparative experiments using retrieved models from literature and own experiments)",
      "Hyperparameter optimization (as described in Section 4.4)"
    ],
    "metrics": [
      "Accuracy",
      "Subset Accuracy (for multi-class cases)"
    ],
    "calculation": "Accuracy is used to measure classification. For multi-class cases, the subset accuracy is calculated.",
    "interpretation": "Higher accuracy indicates better classification performance. The paper interprets consistently higher accuracy of Transformer models as state-of-the-art performance on short text classification and notes NICE-45 as particularly challenging for models, making it a good benchmark for future work.",
    "baseline_results": "Baseline and experimental results are reported in Tables 2 and 3. Examples: BERT achieves 72.79% on NICE-45 and 89.4% on STOPS-41 (Table 3). DeBERTa achieves 59.42% on NICE-45 (Table 3). WideMLP achieves 58.99% on NICE-45 (Table 3). Detailed per-model and per-dataset accuracies are provided in Tables 2 and 3 of the paper.",
    "validation": "Most experiments were conducted once. SHINE and InducT-GCN were run five times and means and standard deviations are reported due to known high variance in GNN-based models. Train/test splits: datasets were randomly shuffled and split 70% train / 30% test (for NICE and STOPS)."
  },
  "targeted_risks": {
    "risk_categories": [
      "Accuracy",
      "Governance"
    ],
    "atlas_risks": {
      "risks": [
        {
          "category": "Accuracy",
          "subcategory": [
            "Data contamination"
          ]
        },
        {
          "category": "Governance",
          "subcategory": [
            "Lack of testing diversity"
          ]
        }
      ]
    },
    "demographic_analysis": null,
    "harm": "The paper notes that STOPS contains user-generated labels that may not be entirely accurate and that over-reliance on limited benchmark datasets can miss important dataset characteristics; these are cited as threats to validity."
  },
  "ethical_and_legal_considerations": {
    "privacy_and_anonymity": "N/A",
    "data_licensing": "N/A",
    "consent_procedures": "N/A",
    "compliance_with_regulations": "N/A"
  }
}