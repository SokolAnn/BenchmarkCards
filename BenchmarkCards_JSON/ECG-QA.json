{
  "benchmark_details": {
    "is_benchmark": true,
    "name": "ECG-QA: A Comprehensive Question Answering Dataset Combined With Electrocardiogram",
    "abbreviation": "ECG-QA",
    "overview": "ECG-QA is the first question answering (QA) dataset specifically designed for ECG analysis. The dataset comprises 70 question templates covering clinically relevant ECG topics (including comparison questions between two ECGs), validated by an ECG expert, and is intended to support the development and evaluation of QA systems that incorporate ECG signals.",
    "data_type": "time-series ECG signals and question-answering pairs (text)",
    "domains": [
      "Medical Diagnosis",
      "Natural Language Processing"
    ],
    "languages": [
      "English"
    ],
    "similar_benchmarks": [
      "emrQA",
      "MIMICSQL",
      "EHRSQL",
      "VQA",
      "GQA",
      "MultimodalQA",
      "MIMIC-IV-ECG",
      "PTB-XL"
    ],
    "resources": [
      "https://github.com/Jwoo5/ecg-qa",
      "https://arxiv.org/abs/2306.15681",
      "https://physionet.org/content/ptb-xl/1.0.3/"
    ]
  },
  "purpose_and_intended_users": {
    "goal": "Provide a QA dataset that incorporates ECG data for question answering tasks to enable development and evaluation of intelligent QA systems for ECG interpretation.",
    "audience": [
      "Machine Learning Researchers",
      "Model Developers",
      "Clinicians",
      "Medical AI Researchers"
    ],
    "tasks": [
      "Question Answering",
      "Multi-label Classification",
      "ECG Signal Classification"
    ],
    "limitations": "Small number of ECGs: constructed using ∼16k ECGs from PTB-XL, limiting inclusion of very rare symptoms; Upper-bound of the dataset: inherent clinical uncertainty means the dataset upper-bound is not expected to be 100%; Old version of SCP-ECG standard: PTB-XL follows SCP-ECG v0.4 which constrained categorization though authors report only minor differences vs v3.0; Automatic generation of paraphrases: initial paraphrase candidates were generated by ChatGPT which the authors note may not be optimal.",
    "out_of_scope_uses": []
  },
  "data": {
    "source": "Constructed upon the PTB-XL dataset (PhysioNet); QA pairs generated from PTB-XL metadata and templates validated by a board-certified medical expert.",
    "size": "16,054 10-second ECG samples (from PTB-XL); 267,539 training QA samples, 64,663 validation QA samples, 82,146 test QA samples",
    "format": "N/A",
    "annotation": "Attributes and labels derived from PTB-XL expert cardiologist annotations (SCP codes and metadata). Question templates (70 templates) were designed and validated by a board-certified medical expert. Paraphrases were machine-generated by ChatGPT and manually refined; numeric features and heart axis were computed using NeuroKit2."
  },
  "methodology": {
    "methods": [
      "Automated metrics",
      "Upper-bound experiments (ECG classification models)",
      "Model-based evaluation (QA baselines and LLM-based experiments)"
    ],
    "metrics": [
      "Exact Match Accuracy",
      "Area Under ROC Curve (AUROC)"
    ],
    "calculation": "Exact Match Accuracy: apply threshold 0.5 to model output vector (ˆy ∈ R103) to obtain multi-hot vector; score 1 if predicted vector exactly equals ground truth multi-hot vector else 0; overall EM accuracy = sum of scores / total questions. AUROC: compute macro-averaged AUROC among valid answer candidates for each question by collecting scores for each answer option across samples and averaging.",
    "interpretation": "Exact Match measures strict full-answer correctness (useful for multi-label QA); AUROC provides partial-credit assessment and is used especially for Query questions with many attributes. The authors estimate an upper bound (perceptual ceiling) using ECG classification models on Single-Verify samples to contextualize QA model performance; closeness to that upper bound indicates stronger perceptual ability.",
    "baseline_results": "Reported QA baselines (macro-average for Single-Verify / overall): M3AE†: Accuracy 80.8, AUROC 0.808; MedViLL†: Accuracy 79.8, AUROC 0.809; Fusion Transformer: Accuracy 76.4, AUROC 0.764. Upper-bound ECG classification models (macro-average for Single-Verify): W2V+CMSC+RLM 83.0 Acc / 0.864 AUROC; Resnet-Attention 82.6 Acc / 0.875 AUROC; SE-WRN 83.1 Acc / 0.883 AUROC; MAX 85.4 Acc / 0.907 AUROC. (Values taken from Tables 2–4 in the paper.)",
    "validation": "Data split by patient IDs to prevent overlap: initial split into train and test by 8:2 based on patient IDs; training set further split 9:1 into training and validation (resulting in 7.2:0.8:2.0 train-validation-test distribution). Confidence intervals reported using 3 random seeds for reported results."
  },
  "targeted_risks": {
    "risk_categories": [
      "Accuracy"
    ],
    "atlas_risks": {
      "risks": [
        {
          "category": "Accuracy",
          "subcategory": [
            "Unrepresentative data",
            "Poor model accuracy"
          ]
        }
      ]
    },
    "demographic_analysis": "N/A",
    "harm": "N/A"
  },
  "ethical_and_legal_considerations": {
    "privacy_and_anonymity": "N/A",
    "data_licensing": "ECG-QA: CC-BY-4.0 (as stated in paper). Source PTB-XL: CC-BY-4.0 (as noted in paper).",
    "consent_procedures": "N/A",
    "compliance_with_regulations": "N/A"
  }
}