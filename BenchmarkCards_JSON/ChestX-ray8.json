{
  "benchmark_details": {
    "is_benchmark": true,
    "name": "ChestX-ray8",
    "abbreviation": "N/A",
    "overview": "ChestX-ray8: a hospital-scale chest X-ray database comprising 108,948 frontal-view X-ray images of 32,717 unique patients with text-mined eight disease image labels (multi-label per image) mined from associated radiological reports via NLP. The dataset is used to validate a unified weakly-supervised multi-label image classification and disease localization framework for common thoracic diseases.",
    "data_type": "image (frontal-view chest X-rays; multi-label image classification labels and bounding boxes)",
    "domains": [
      "Medical Diagnosis",
      "Computer Vision",
      "Natural Language Processing"
    ],
    "languages": [],
    "similar_benchmarks": [
      "OpenI",
      "ChestX-ray14",
      "ImageNet",
      "MS COCO",
      "PASCAL VOC"
    ],
    "resources": [
      "https://nihcc.app.box.com/v/ChestXray-NIHCC",
      "https://openi.nlm.nih.gov",
      "https://www.cc.nih.gov/drd/summers.html",
      "https://arxiv.org/abs/1705.02315"
    ]
  },
  "purpose_and_intended_users": {
    "goal": "To construct a hospital-scale chest X-ray image database (ChestX-ray8) and provide quantitative performance benchmarking on eight common thoracic pathology multi-label classification and weakly-supervised localization to facilitate development of large-scale deep learning based CAD systems.",
    "audience": [
      "ML Researchers",
      "Medical Imaging Researchers",
      "Clinical Researchers",
      "Model Developers"
    ],
    "tasks": [
      "Multi-label Image Classification",
      "Object Detection/Localization",
      "Named Entity Recognition (clinical radiology reports)"
    ],
    "limitations": "Limited number of hand-annotated bounding boxes (1,600 boxes on 983 images) relative to the full dataset; many pathologies occupy small spatial extents making detection/localization difficult; class imbalance (some diseases, e.g., Pneumonia, have <1% instances) affects performance; text-mined labels may be noisy (mitigated by syntactic negation/uncertainty rules).",
    "out_of_scope_uses": []
  },
  "data": {
    "source": "Images and associated radiology reports extracted from the institute's Picture Archiving and Communication System (PACS); disease labels text-mined from associated radiological reports using NLP tools (DNorm and MetaMap) with syntactic negation and uncertainty rules; a subset of images annotated with bounding boxes by a board-certified radiologist.",
    "size": "108,948 frontal-view X-ray images; 32,717 unique patients (collected 1992â€“2015); 24,636 images contain one or more pathologies; 84,312 images labeled as Normal; 983 images with 1,600 hand-labeled bounding boxes.",
    "format": "Images extracted from DICOM and resized to 1024x1024 bitmap images; radiology reports as plain text (processed for NLP).",
    "annotation": "Image-level disease labels automatically generated by text-mining radiology reports using DNorm and MetaMap with hand-crafted syntactic rules for negation and uncertainty; 1,600 bounding boxes manually annotated by a board-certified radiologist for 983 images (used for localization evaluation)."
  },
  "methodology": {
    "methods": [
      "Model-based evaluation (deep convolutional neural networks fine-tuned from ImageNet pre-trained models: AlexNet, GoogLeNet, VGGNet-16, ResNet-50)",
      "Automated metrics (ROC/AUC, Precision, Recall, F1-Score)",
      "Localization evaluation using Intersection over Union (IoU) and Intersection over detected bounding-box area ratio (IoBB)"
    ],
    "metrics": [
      "Area Under the Receiver Operating Characteristic Curve (AUC)",
      "Accuracy",
      "Precision",
      "Recall",
      "F1 Score",
      "Intersection over Union (IoU)",
      "Intersection over detected bounding-box area ratio (IoBB)",
      "Average False Positive (AFP)"
    ],
    "calculation": "AUC values computed from ROC curves on the held-out test split. Labeling evaluation computed using Precision, Recall, and F1-score against OpenI and annotated reports. Localization correctness defined as IoU > T(IoU) or IoBB > T(IoBB) with reported thresholds (IoBB thresholds: 0.1, 0.25, 0.5, 0.75, 0.9; IoU thresholds: reported from 0.1 to 0.7).",
    "interpretation": "Higher AUC indicates better pathology recognition. The paper reports that classes with larger/clearer manifestations (e.g., Cardiomegaly AUC=0.8141, Pneumothorax AUC=0.7891) are better detected, while small or highly variable findings (e.g., Mass AUC=0.5609, Nodule lower) yield lower performance. Localization performance is limited by low-resolution heatmaps and limited bounding-box annotations.",
    "baseline_results": "ResNet-50 (ChestX-ray8) AUCs: Atelectasis 0.7069; Cardiomegaly 0.8141; Effusion 0.7362; Infiltration 0.6128; Mass 0.5609; Nodule 0.7164; Pneumonia 0.6333; Pneumothorax 0.7891. Image labeling (text-mining) on OpenI: total Precision=0.90, Recall=0.91, F1=0.90 (our method) vs. MetaMap total P=0.84, R=0.88, F1=0.86. Localization results (example) reported as localization Accuracy and AFP across multiple IoBB/IoU thresholds (see paper Tables 4 and 7).",
    "validation": "Dataset randomly split at patient level into training (70%), validation (10%), and testing (20%) for CNN fine-tuning. OpenI and a manually annotated report subset used as gold standards to evaluate text-mined labels. The 983 images with 1,600 B-Boxes are reserved for localization testing (not used for training)."
  },
  "targeted_risks": {
    "risk_categories": [
      "Accuracy"
    ],
    "atlas_risks": {
      "risks": [
        {
          "category": "Accuracy",
          "subcategory": [
            "Data contamination",
            "Unrepresentative data",
            "Poor model accuracy"
          ]
        }
      ]
    },
    "demographic_analysis": null,
    "harm": null
  },
  "ethical_and_legal_considerations": {
    "privacy_and_anonymity": "N/A",
    "data_licensing": "N/A",
    "consent_procedures": "N/A",
    "compliance_with_regulations": "N/A"
  }
}