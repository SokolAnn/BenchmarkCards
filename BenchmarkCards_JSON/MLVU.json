{
  "benchmark_details": {
    "is_benchmark": true,
    "name": "MLVU (Multi-task Long Video Understanding Benchmark)",
    "abbreviation": "MLVU",
    "overview": "MLVU is a benchmark proposed for the comprehensive and in-depth evaluation of Long Video Understanding (LVU) performance. It presents substantial extensions of video lengths, diverse video genres, and various tailored evaluation tasks to better assess MLLMs' capabilities in long video understanding.",
    "data_type": "question-answering pairs",
    "domains": [
      "Computer Vision"
    ],
    "languages": [
      "English"
    ],
    "similar_benchmarks": [],
    "resources": [
      "https://arxiv.org/abs/2406.04264"
    ]
  },
  "purpose_and_intended_users": {
    "goal": "To provide a comprehensive and in-depth analysis for MLLMs' long-video understanding performance.",
    "audience": [
      "ML Researchers",
      "Industry Practitioners",
      "Model Developers"
    ],
    "tasks": [
      "Video Summarization",
      "Needle Question Answering",
      "Ego Reasoning",
      "Plot Question Answering",
      "Sub-Scene Captioning",
      "Action Order",
      "Action Count",
      "Anomaly Recognition",
      "Topic Reasoning"
    ],
    "limitations": "N/A",
    "out_of_scope_uses": []
  },
  "data": {
    "source": "Universal Long Video Collection (ULVC) which includes diverse long videos from movies, documentaries, cartoons, etc.",
    "size": "1,730 videos and 3,102 question-answer pairs",
    "format": "N/A",
    "annotation": "Manual annotation by experts"
  },
  "methodology": {
    "methods": [
      "Human evaluation",
      "Automated metrics"
    ],
    "metrics": [
      "Accuracy"
    ],
    "calculation": "Accuracy is calculated by matching model predictions with ground truths in the multiple-choice and generation tasks.",
    "interpretation": "Higher accuracy indicates better model understanding of long videos.",
    "baseline_results": "Baseline models evaluated include various MLLMs such as GPT-4o and Video-XL.",
    "validation": "N/A"
  },
  "targeted_risks": {
    "risk_categories": [
      "Accuracy",
      "Fairness"
    ],
    "atlas_risks": {
      "risks": [
        {
          "category": "Accuracy",
          "subcategory": [
            "Unrepresentative data",
            "Poor model accuracy"
          ]
        },
        {
          "category": "Fairness",
          "subcategory": [
            "Data bias"
          ]
        }
      ]
    },
    "demographic_analysis": "N/A",
    "harm": "N/A"
  },
  "ethical_and_legal_considerations": {
    "privacy_and_anonymity": "N/A",
    "data_licensing": "N/A",
    "consent_procedures": "N/A",
    "compliance_with_regulations": "N/A"
  }
}