# HalluQA

## üìä Benchmark Details

**Name**: HalluQA

**Overview**: HalluQA is a benchmark designed to evaluate hallucination phenomena in Chinese large language models using 450 meticulously crafted adversarial questions, taking into account Chinese historical culture, customs, and social phenomena.

**Data Type**: Question-Answering

**Domains**:
- history
- literature
- folklore
- science
- geography
- art

**Languages**:
- Chinese

**Similar Benchmarks**:
- TruthfulQA
- ChineseFactEval
- HaluEval

**Resources**:
- [GitHub Repository](https://github.com/xiami2019/HalluQA)

## üéØ Purpose and Intended Users

**Goal**: To evaluate hallucination issues in Chinese large language models and assist in the development of trustworthy AI.

**Target Audience**:
- Researchers in Natural Language Processing
- Developers of AI systems

**Tasks**:
- Evaluate hallucinations in language models
- Analyze types of hallucinations

**Limitations**: N/A

**Out of Scope Uses**:
- Non-Chinese language models
- Models not related to hallucination evaluation

## üíæ Data

**Source**: Adversarial samples generated based on GLM-130B and ChatGPT

**Size**: 450 questions

**Format**: Question-Answer pairs

**Annotation**: Adversarial nature of questions ensures diverse coverage of hallucination types.

## üî¨ Methodology

**Methods**:
- Automated evaluation using GPT-4
- Human evaluation for consistency

**Metrics**:
- Non-hallucination rate

**Calculation**: The non-hallucination rate is calculated as the percentage of answers generated by models that do not exhibit hallucinations.

**Interpretation**: A model's performance is assessed based on its ability to produce non-hallucinated responses to adversarial questions.

**Baseline Results**: 18 out of 24 models achieved non-hallucination rates lower than 50%.

**Validation**: Evaluations are confirmed through consistency checks with human assessments.

## ‚ö†Ô∏è Targeted Risks

**Risk Categories**:
- Accuracy
- Fairness
- Transparency

**Atlas Risks**:
- **Accuracy**: Poor model accuracy
- **Fairness**: Data bias
- **Transparency**: Lack of training data transparency

**Demographic Analysis**: N/A

**Potential Harm**: N/A

## üîí Ethical and Legal Considerations

**Privacy And Anonymity**: Not Applicable

**Data Licensing**: Not Applicable

**Consent Procedures**: Not Applicable

**Compliance With Regulations**: Not Applicable
