{
  "benchmark_details": {
    "is_benchmark": true,
    "name": "TheAgentCompany",
    "abbreviation": "N/A",
    "overview": "TheAgentCompany is an extensible benchmark for evaluating AI agents that interact with the world in ways similar to digital workers by browsing the web, writing code, running programs, and communicating with coworkers. It includes a simulated environment with 175 realistic and professional tasks performed in a software company.",
    "data_type": "task completion",
    "domains": [
      "Natural Language Processing"
    ],
    "languages": [
      "English"
    ],
    "similar_benchmarks": [
      "SWE-Bench",
      "Mind2Web",
      "WebArena"
    ],
    "resources": [
      "https://the-agent-company.com",
      "https://github.com/TheAgentCompany/TheAgentCompany",
      "https://github.com/TheAgentCompany/experiments"
    ]
  },
  "purpose_and_intended_users": {
    "goal": "To measure the progress of LLM agents' performance on real-world professional tasks and provide insights into AI automation potential.",
    "audience": [
      "ML Researchers",
      "Industry Practitioners",
      "Model Developers"
    ],
    "tasks": [
      "Task Automation",
      "Software Development",
      "Project Management",
      "Data Science",
      "Administrative Tasks",
      "Human Resources"
    ],
    "limitations": "Tasks are generally on the more straightforward side and do not cover complex creative tasks. Current evaluation only includes two agent scaffolds without comparison to human performance.",
    "out_of_scope_uses": []
  },
  "data": {
    "source": "Tasks generated based on a simulated software company modeled after real-world job categories.",
    "size": "175 tasks",
    "format": "N/A",
    "annotation": "Tasks are crafted by domain experts with detailed descriptions and structured checkpoints for evaluation."
  },
  "methodology": {
    "methods": [
      "Automated metrics",
      "Checkpoint-based evaluation"
    ],
    "metrics": [
      "Success Rate",
      "Task Completion Score"
    ],
    "calculation": "Scores are calculated based on agent performance across predefined checkpoints, awarding partial credit for substantial task milestones.",
    "interpretation": "Higher scores indicate better performance and more successful task execution.",
    "baseline_results": null,
    "validation": "Validation of task completion is based on an evaluation framework that includes deterministic checks and LLM reviewers for complex tasks."
  },
  "targeted_risks": {
    "risk_categories": [
      "Accuracy",
      "Privacy",
      "Fairness"
    ],
    "atlas_risks": {
      "risks": [
        {
          "category": "Accuracy",
          "subcategory": [
            "Unrepresentative data"
          ]
        },
        {
          "category": "Privacy",
          "subcategory": [
            "Personal information in data"
          ]
        },
        {
          "category": "Fairness",
          "subcategory": [
            "Data bias"
          ]
        }
      ]
    },
    "demographic_analysis": "N/A",
    "harm": []
  },
  "ethical_and_legal_considerations": {
    "privacy_and_anonymity": "All tasks and evaluations are designed to respect user privacy and anonymize sensitive data.",
    "data_licensing": "N/A",
    "consent_procedures": "N/A",
    "compliance_with_regulations": "N/A"
  }
}