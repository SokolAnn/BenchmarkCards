{
  "benchmark_details": {
    "is_benchmark": true,
    "name": "C3: A Bilingual Benchmark for Spoken Dialogue Models Exploring Challenges in Complex Conversations",
    "abbreviation": "C3",
    "overview": "The C3 benchmark is designed to evaluate Spoken Dialogue Models (SDMs) on their capability to handle complexity in spoken dialogue, addressing challenges like phonological ambiguity, semantic ambiguity, omission, coreference, and multi-turn interaction.",
    "data_type": "dialogue instances with audio and text pairs",
    "domains": [
      "Natural Language Processing"
    ],
    "languages": [
      "English",
      "Chinese"
    ],
    "similar_benchmarks": [],
    "resources": [
      "https://github.com/username/repo",
      "https://c3benchmark.com"
    ]
  },
  "purpose_and_intended_users": {
    "goal": "To evaluate SDMs' abilities in handling various complex situations in spoken dialogues.",
    "audience": [
      "ML Researchers",
      "Industry Practitioners",
      "Model Developers"
    ],
    "tasks": [
      "Dialogue Understanding",
      "Contextual Understanding"
    ],
    "limitations": "The dataset's phenomena are particularly focused on English and Chinese dialogues; its applicability to other languages is not resolved in this work.",
    "out_of_scope_uses": []
  },
  "data": {
    "source": "Real-world spoken dialogues collected from web sources and existing datasets.",
    "size": "1,079 instances comprising 1,586 audio-text pairs",
    "format": "Audio and Text",
    "annotation": "Manually checked and annotated for ambiguity types."
  },
  "methodology": {
    "methods": [
      "LLM-based evaluation",
      "Human evaluation for accuracy"
    ],
    "metrics": [
      "Accuracy"
    ],
    "calculation": "Accuracy is calculated as the proportion of instances judged correct out of the total number of instances.",
    "interpretation": "Higher accuracy indicates better performance in understanding and generating responses in dialogues.",
    "baseline_results": null,
    "validation": "Reliability of the evaluation method confirmed through correlation with human assessments."
  },
  "targeted_risks": {
    "risk_categories": [
      "Fairness",
      "Accuracy"
    ],
    "atlas_risks": {
      "risks": []
    },
    "demographic_analysis": "The benchmark includes data from two different language groups (English and Chinese).",
    "harm": []
  },
  "ethical_and_legal_considerations": {
    "privacy_and_anonymity": "N/A",
    "data_licensing": "N/A",
    "consent_procedures": "N/A",
    "compliance_with_regulations": "N/A"
  }
}