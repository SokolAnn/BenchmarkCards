{
  "benchmark_details": {
    "is_benchmark": true,
    "name": "CodeClarQA",
    "abbreviation": "CodeClarQA",
    "overview": "A dataset (CodeClarQA) of natural language descriptionâ€“code pairs augmented with synthetically generated clarification questions and answers (CQAs). It is designed to introduce interactivity into text-to-code generation: identifying missing API/operation-level specifications in a natural language description and resolving under-specification by asking clarification questions so that models can generate more precise Python code.",
    "data_type": "text (natural language descriptions and clarification question-answer pairs) and text (Python code snippets)",
    "domains": [
      "Natural Language Processing",
      "Software Engineering"
    ],
    "languages": [],
    "similar_benchmarks": [
      "notebookCDG"
    ],
    "resources": [
      "https://github.com/UKPLab/codeclarqa",
      "https://arxiv.org/abs/2212.09885"
    ]
  },
  "purpose_and_intended_users": {
    "goal": "Introduce interactivity into text-to-code generation by collecting CodeClarQA (clarification questions and answers on API usage) to resolve under-specified natural language descriptions and to evaluate interactive code generation pipelines.",
    "audience": [],
    "tasks": [
      "Text-to-Code Generation",
      "Clarification Question Generation",
      "Question Answering",
      "Ranking",
      "Text Classification"
    ],
    "limitations": "Method primarily focuses on operation-level specifications; can only be scaled to Python codes that involve heavy API usage; limited in identifying specifications missing from the NLD beyond the level of operations (e.g., argument-level specifications or \"mentioned but not specified enough\" cases).",
    "out_of_scope_uses": []
  },
  "data": {
    "source": "NLD-Code pairs from the notebookCDG dataset (Liu et al., 2021) (Jupyter Notebooks) and API documentation; clarification questions and answers are synthetically generated by the authors' method described in the paper.",
    "size": "19,368 NLD-Code samples total (17,431 train; 968 dev; 969 test). 12,339 samples with CQAs; 7,029 samples without CQAs. 17,506 total CQAs (8,952 multiple-choice, 8,554 yes/no). 817 operations and 89 packages.",
    "format": "N/A",
    "annotation": "CQAs are synthetically generated; human annotation was performed for validation and test sets: two Ph.D. students annotated 100 NLD-Code pairs for validation and 100 for test with binary labels (aligned / missing) to evaluate the method."
  },
  "methodology": {
    "methods": [
      "Automated metrics evaluation",
      "Human evaluation (annotation of validation and test sets)",
      "Model fine-tuning and evaluation of pretrained language models (classification, ranking, and code generation)"
    ],
    "metrics": [
      "Accuracy",
      "Precision",
      "Recall",
      "F1 Score",
      "BLEU Score",
      "CodeBLEU Score",
      "Exact Match (EM)",
      "Recall@k (R@k)",
      "Fleiss Kappa",
      "Pearson correlation"
    ],
    "calculation": "Threshold t for identifying missing operations is optimized via grid search on a validation set maximizing F1. Model results are averaged across multiple runs (numbers in tables averaged across four runs). Standard definitions of Accuracy, Precision, Recall, F1, BLEU, CodeBLEU, and Exact Match are used as reported in the paper. Fleiss Kappa was computed for annotator agreement (reported for validation and test sets).",
    "interpretation": "Higher BLEU, CodeBLEU, and Exact Match indicate better code generation quality. Improvements in these metrics when conditioning on CQAs indicate that clarifications improve the precision of generated code. Higher Accuracy/Precision/Recall/F1 indicate better performance for clarification-need prediction and CQ ranking.",
    "baseline_results": "Selected baselines (averaged across runs): Code generation - PLBART base: BLEU 24.63, CodeBLEU 28.04, EM 12.00; PLBART base + CQAs: BLEU 38.91, CodeBLEU 38.54, EM 18.03. CodeT5 base: BLEU 27.03, CodeBLEU 32.66, EM 10.84; CodeT5 base + CQAs: BLEU 39.13, CodeBLEU 38.99, EM 13.93. Clarification-need prediction - BERT base accuracy ~71.49 (F1 78.13); other models reported in Table 5. Identification of missing key operations: best text encoder MPNet baseqa-cos results shown in Table 2.",
    "validation": "Human annotation on validation and test sets (100 pairs each) to evaluate identification of missing key operations; Fleiss Kappa reported (0.74 overall; 0.83 validation; 0.66 test). Grid search on validation set to choose similarity threshold t maximizing F1. Additional evaluation of pipeline end-to-end using best-performing models for each module."
  },
  "targeted_risks": {
    "risk_categories": [
      "Intellectual Property",
      "Privacy"
    ],
    "atlas_risks": {
      "risks": [
        {
          "category": "Intellectual Property",
          "subcategory": [
            "Copyright infringement",
            "Data usage rights restrictions"
          ]
        },
        {
          "category": "Privacy",
          "subcategory": [
            "Personal information in data"
          ]
        }
      ]
    },
    "demographic_analysis": "N/A",
    "harm": [
      "Copyright concerns",
      "Potential privacy leakage in dataset (noted by authors as minimum risk)"
    ]
  },
  "ethical_and_legal_considerations": {
    "privacy_and_anonymity": "Authors state minimum risk of privacy leakage because data is from 20 Kaggle competitions checked by Liu et al. (2021); API documentation is also checked and considered minimal risk.",
    "data_licensing": "Authors report they checked licenses of open-source APIs used for documentation and state there is no concern about copyright issues; Liu et al. (2021) checked data policies of the 20 Kaggle competitions.",
    "consent_procedures": "N/A",
    "compliance_with_regulations": "N/A"
  }
}