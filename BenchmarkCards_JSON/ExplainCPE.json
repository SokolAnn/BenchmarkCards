{
  "benchmark_details": {
    "is_benchmark": true,
    "name": "ExplainCPE: A Free-text Explanation Benchmark of Chinese Pharmacist Examination",
    "abbreviation": "ExplainCPE",
    "overview": "We propose ExplainCPE, a challenging medical dataset consisting of over 7K problems from Chinese Pharmacist Examination, specifically tailored to assess the model-generated explanations. ExplainCPE is a free-text explanation benchmark in Chinese aimed to evaluate the capacity of model explainability in a specialized, high-stakes medical domain.",
    "data_type": "text (multiple-choice questions with free-text explanations)",
    "domains": [
      "Natural Language Processing",
      "Healthcare"
    ],
    "languages": [
      "Chinese"
    ],
    "similar_benchmarks": [
      "MMLU",
      "AGIEval",
      "C-EVAL",
      "GAOKAO-Benchmark (GAOKAO-Bench)"
    ],
    "resources": [
      "https://github.com/HITsz-TMG/ExplainCPE"
    ]
  },
  "purpose_and_intended_users": {
    "goal": "Provide a challenging benchmark for generating free-text explanations in Chinese medical QA, offer a baseline for future research on explanation generation by LLMs, and enable study of how to improve model explanation ability.",
    "audience": [
      "Research community",
      "ML Researchers",
      "Model Developers"
    ],
    "tasks": [
      "Question Answering",
      "Explanation Generation"
    ],
    "limitations": "We only contribute to Explanation Generation. Most current interpretable methods are aimed at classification tasks; automatic assessment of interpretability is still lacking. The dataset and analysis are a preliminary exploration with room for further improvement in quality and diversity of explanations.",
    "out_of_scope_uses": []
  },
  "data": {
    "source": "Questions sourced from the National Licensed Pharmacist Examination in China (official examination examples from 2020-2021 with official examination solutions); additional instances collected from the internet and exercise books; 320 samples manually reviewed by three doctoral students from Peking Union Medical College.",
    "size": "7,556 examples total (6,867 training; 500 development; 189 test)",
    "format": "N/A",
    "annotation": "Gold explanations sourced from official examination solutions; manual review of 320 samples by three doctoral students (resulting in 318/317 correct out of 320 in two checks, reported 99.4%/99.0% accuracy). Duplicates and incomplete questions removed; instances with edit distance < 0.1 inspected and semantically duplicate questions removed."
  },
  "methodology": {
    "methods": [
      "Automated metrics (ROUGE, Accuracy)",
      "Human evaluation (annotators rate explanations 1-5 on multiple criteria)",
      "Model-based evaluation (GPT-4 used to evaluate responses)"
    ],
    "metrics": [
      "Accuracy",
      "ROUGE-1",
      "ROUGE-2",
      "ROUGE-L",
      "Human evaluation ratings (Well-formed, Support, Correctness, Validity, Novelty; 1-5 scale)"
    ],
    "calculation": "Accuracy measured as percentage of correct answers on the test set. ROUGE computed between model-generated explanations and gold explanations. Human evaluation: annotators rate each explanation from 1 to 5 on five criteria (Well-formed, Support, Correctness, Validity, Novelty).",
    "interpretation": "Higher Accuracy indicates better question-answering performance (e.g., GPT-4 achieved 75.7% and is reported to pass the pharmacist examination). Higher ROUGE and higher human evaluation scores indicate better explanation quality and interpretability.",
    "baseline_results": "GPT-4: Accuracy 75.7%; ROUGE-1 0.384; ROUGE-2 0.140; ROUGE-L 0.247. ChatGPT: Accuracy 54.5%; ROUGE-1 0.341; ROUGE-2 0.114; ROUGE-L 0.216. GPT-3: Accuracy 40.2% (ROUGE scores not reported). ChatGLM-6B: Accuracy 29.1%; ROUGE-1 0.315; ROUGE-2 0.099; ROUGE-L 0.184. BELLE-7B-2M: Accuracy 33.3% (ROUGE scores not reported). ChatYuan: Accuracy 27.0% (ROUGE scores not reported).",
    "validation": "Manual review of 320 randomly selected samples by three doctoral students from Peking Union Medical College (reported 99.4% / 99.0% correctness for reviewed samples). Duplicate and incomplete questions removed; manual inspection of low edit-distance pairs to remove semantically duplicate instances."
  },
  "targeted_risks": {
    "risk_categories": [
      "Interpretability",
      "Safety",
      "Accuracy",
      "Robustness",
      "Fairness"
    ],
    "atlas_risks": {
      "risks": [
        {
          "category": "Explainability",
          "subcategory": [
            "Unexplainable output"
          ]
        },
        {
          "category": "Robustness",
          "subcategory": [
            "Hallucination"
          ]
        },
        {
          "category": "Accuracy",
          "subcategory": [
            "Poor model accuracy"
          ]
        },
        {
          "category": "Fairness",
          "subcategory": [
            "Data bias"
          ]
        },
        {
          "category": "Transparency",
          "subcategory": [
            "Uncertain data provenance"
          ]
        }
      ]
    },
    "demographic_analysis": null,
    "harm": [
      "Reduce misdiagnosis"
    ]
  },
  "ethical_and_legal_considerations": {
    "privacy_and_anonymity": "The cases in the exercises are fictitious; the authors state there is no personal privacy, discrimination or attack content in the dataset.",
    "data_licensing": "N/A",
    "consent_procedures": "N/A",
    "compliance_with_regulations": "N/A"
  }
}