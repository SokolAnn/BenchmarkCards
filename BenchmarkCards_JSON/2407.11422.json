{
  "benchmark_details": {
    "name": "REVERIE",
    "overview": "REVERIE is the first large-scale visual instruction tuning dataset with reflective rationale annotations, encompassing 115k machine-generated reasoning instructions. Each instruction is paired with correct and confusing responses alongside comprehensive rationales elucidating the justification behind their correctness or erroneousness.",
    "data_type": "Visual Instruction Tuning Dataset",
    "domains": [
      "Vision-Language Tasks"
    ],
    "languages": [
      "English"
    ],
    "similar_benchmarks": [
      "LLaVA-Instruct-158k",
      "ScienceQA",
      "A-OK-VQA"
    ],
    "resources": [
      "Project page: https://zjr2000.github.io/projects/reverie"
    ]
  },
  "purpose_and_intended_users": {
    "goal": "To enhance the reasoning capabilities of large vision-language models and mitigate hallucinations through reflective instruction tuning.",
    "audience": [
      "Researchers",
      "Data Scientists",
      "AI Developers"
    ],
    "tasks": [
      "Visual Instruction Tuning",
      "Reasoning Tasks in AI",
      "Mitigating Hallucinations in AI Models"
    ],
    "limitations": "Focused on visual instruction tuning tasks; other AI model training tasks are outside scope.",
    "out_of_scope_uses": [
      "General AI applications outside visual instruction tasks"
    ]
  },
  "data": {
    "source": "Visual Genome, COCO, ScienceQA, A-OK-VQA",
    "size": "254,177 training instances",
    "format": "Instruction-Response-Rationale tuples",
    "annotation": "Contains both positive and negative rationales for responses."
  },
  "methodology": {
    "methods": [
      "Reflective Instruction Tuning",
      "Multi-turn Conversation format for rationale generation"
    ],
    "metrics": [
      "Accuracy",
      "Precision",
      "F1 Score"
    ],
    "calculation": "Performance improvement measured against baseline models across multiple benchmarks.",
    "interpretation": "Interprets the effectiveness of rationale inclusion in training models.",
    "baseline_results": "Substantial performance gains observed over baseline models in all evaluated benchmarks.",
    "validation": "Validated through experiments on six different evaluation benchmarks."
  },
  "targeted_risks": {
    "risk_categories": [
      "Data bias",
      "Output bias",
      "Privacy concerns"
    ],
    "atlas_risks": {
      "risks": [
        {
          "category": "Fairness",
          "subcategory": [
            "Data bias"
          ]
        },
        {
          "category": "Privacy",
          "subcategory": [
            "Exposing personal information"
          ]
        }
      ]
    },
    "demographic_analysis": "N/A",
    "harm": "N/A"
  },
  "ethical_and_legal_considerations": {
    "privacy_and_anonymity": "Data is anonymized where applicable.",
    "data_licensing": "Dataset details regarding licensing are not specified.",
    "consent_procedures": "N/A",
    "compliance_with_regulations": "The dataset complies with standard research regulations."
  }
}