{
  "benchmark_details": {
    "is_benchmark": true,
    "name": "Kencorpus: A Kenyan Language Corpus of Swahili, Dholuo and Luhya for Natural Language Processing Tasks",
    "abbreviation": "N/A",
    "overview": "The Kencorpus dataset is a text and speech corpus for three languages predominantly spoken in Kenya: Swahili, Dholuo and Luhya (three dialects of Lumarachi, Lulogooli and Lubukusu). The project collected and stored text and speech data to support data-driven solutions in applications such as machine translation, question answering and transcription. The Kencorpus collection comprises 5,594 items: 4,442 texts (about 5.6 million words) and 1,152 speech files (about 176.5 hours). From this corpus the project also developed POS-tagged datasets, parallel translations, and a Kiswahili QA dataset (KenSwQuAD).",
    "data_type": "text (cleaned text documents, tweets, QA pairs, POS-tagged texts, parallel sentence translations) and audio (speech files)",
    "domains": [
      "Natural Language Processing"
    ],
    "languages": [
      "Swahili",
      "Dholuo",
      "Luhya (Lumarachi)",
      "Luhya (Lulogooli)",
      "Luhya (Lubukusu)"
    ],
    "similar_benchmarks": [
      "SQuAD",
      "TyDiQA",
      "MCTest",
      "WikiQA",
      "TREC-QA"
    ],
    "resources": [
      "https://kencorpus.maseno.ac.ke/",
      "https://doi.org/10.7910/DVN/OTL0LM",
      "https://doi.org/10.7910/DVN/YHXJSU"
    ]
  },
  "purpose_and_intended_users": {
    "goal": "Collect and curate text and speech data for low-resource Kenyan languages (Swahili, Dholuo, Luhya) and develop derived datasets (POS annotations, translations, QA pairs) to enable machine learning applications such as machine translation, question answering and transcription.",
    "audience": [
      "Machine learning researchers",
      "Natural Language Processing researchers",
      "Enterprises interested in human language technology",
      "Model developers",
      "Domain experts working on low-resource languages"
    ],
    "tasks": [
      "Machine Translation",
      "Question Answering",
      "Speech-to-Text (Transcription)",
      "Part of Speech Tagging"
    ],
    "limitations": "Challenges in developing the corpus included deficiencies in the data sources, data cleaning challenges, relatively short project timelines and the Coronavirus disease (COVID-19) pandemic that restricted movement and hence the ability to get the data in a timely manner.",
    "out_of_scope_uses": []
  },
  "data": {
    "source": "Primary data (communities, schools, colleges, volunteers) and secondary data (partnering media houses, publishers). Additional sources: story-writing competitions, web-scraped tweets (Swahili and Dholuo) collected via Twitter API. Research assistants (native speakers) collected and digitized data; media houses provided speech MP3 files.",
    "size": "Total 5,594 items: 4,442 texts (5,601,915 words) and 1,152 speech files (176 hours 29 minutes 46 seconds). Swahili: 2,585 texts (1,829,727 words), 104 speech files (19:10:57). Dholuo: 546 texts (1,346,481 words), 512 speech files (99:03:08). Luhya: 987 texts (2,272,957 words), 536 speech files (58:15:41). Additional datasets: 13,400 translation sentence pairs (Dholuo-Swahili and Luhya-Swahili), 143,000 words POS-tagged (Dholuo and Luhya dialects), 7,537 QA pairs (from 1,445 Swahili texts).",
    "format": "Cleaned text files in TXT format for texts; speech files converted to WAV format for speech. (Tweets were processed and retained as text.)",
    "annotation": "POS annotation: manual tagging by at least two research assistants per document using the 12-tag universal tagset (Petrov et al., 2011) with lead linguist resolving disagreements. QA annotation: five QA pairs per eligible Swahili text created via Google Forms following methodology similar to SQuAD and TyDiQA; 70% of Kiswahili texts were annotated. Translation: sentence-level translations (literal/equivalence) performed by native speakers and linguistic experts; dictionaries used as references. Annotation work used spreadsheets and Google Forms as described."
  },
  "methodology": {
    "methods": [
      "Participatory primary data collection (community, schools)",
      "Secondary data acquisition from media houses and publishers",
      "Web scraping (Twitter API) for tweets",
      "Digitization and OCR (Pen-to-Print, Expert PDF OCR, Google Docs converter)",
      "Human data cleaning with multi-stage pipeline (Digitize, Screening, Diagnosis, Treatment, Documentation)",
      "Manual annotation (POS tagging, translation, QA creation) with multi-annotator checks",
      "Proof-of-concept model evaluation (QA models using XLM-RoBERTa/BERT and semantic networks; STT using CMU Sphinx)"
    ],
    "metrics": [
      "Exact Match (EM)",
      "F1 Score",
      "Word Error Rate (WER)"
    ],
    "calculation": "QA models evaluated using held-out test split (example: 80% training / 20% testing for a 100-story subset with 500 QA pairs). EM and F1 computed for QA evaluation as reported. WER computed for the speech-to-text evaluation on the evaluated speech subset.",
    "interpretation": "For QA: higher Exact Match (EM) and higher F1 Score indicate better performance. For STT: lower Word Error Rate (WER) indicates better performance. Reported example comparisons: semantic network method achieved higher EM (80%) than the data-driven BERT-based method (59.4% F1) on the tested subset; STT WER of 18.87% reported as relatively good for low-resource settings.",
    "baseline_results": "QA system using deep learning (XLM-RoBERTa/BERT) on a 100-story / 500 QA-pair subset: 59.4% F1 score. QA system using semantic network: 80% Exact Match. STT using CMU Sphinx on a 27h31m50s Swahili subset: 18.87% WER.",
    "validation": "Quality control included: informed consent procedures; metadata capture for each item; primary and secondary data cleaners with swap-and-review; linguist random sampling checks; POS tagging rechecked by subject matter experts on a random 10% sample of annotated words; translation and QA samples checked (10% sampling for translations; annotators cross-checked QA pairs with lead researcher resolving disagreements)."
  },
  "targeted_risks": {
    "risk_categories": [
      "Privacy",
      "Data Laws",
      "Accuracy"
    ],
    "atlas_risks": {
      "risks": [
        {
          "category": "Privacy",
          "subcategory": [
            "Personal information in data"
          ]
        },
        {
          "category": "Data Laws",
          "subcategory": [
            "Data usage restrictions"
          ]
        },
        {
          "category": "Accuracy",
          "subcategory": [
            "Unrepresentative data",
            "Poor model accuracy"
          ]
        }
      ]
    },
    "demographic_analysis": "Respondents were purposively drawn from different genders, age groups and geographical locations; metadata captured includes gender and age group information as per project metadata forms.",
    "harm": null
  },
  "ethical_and_legal_considerations": {
    "privacy_and_anonymity": "Informed consent forms were developed and used. Consent forms captured participant full names and contacts; group consent (e.g., by school managers) was used for institutions with individual willingness respected. Metadata capture included personally identifying information as described in the project documentation.",
    "data_licensing": "All Kencorpus collections and derived datasets are available under the Creative Commons Attribution (CC BY 4.0) international license.",
    "consent_procedures": "Informed consent forms were provided and signed by willing participants; group consent for institutions was executed by school managers with individual participation optional. Copies of consent forms were retained.",
    "compliance_with_regulations": "N/A"
  }
}