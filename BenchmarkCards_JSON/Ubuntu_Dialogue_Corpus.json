{
  "benchmark_details": {
    "is_benchmark": true,
    "name": "Ubuntu Dialogue Corpus",
    "abbreviation": "N/A",
    "overview": "A dataset containing almost 1 million multi-turn two-person conversations (totaling over 7 million utterances and 100 million words) extracted from Ubuntu chat logs, provided as a large resource for research into building dialogue managers based on neural language models. The corpus preserves multi-turn properties of conversations and the unstructured nature of chat-room interactions; benchmark performance is provided on the task of selecting the best next response.",
    "data_type": "multi-turn dialogue text (context, response, flag triples / context-response pairs)",
    "domains": [
      "Natural Language Processing",
      "Technical Support",
      "Dialogue Systems"
    ],
    "languages": [],
    "similar_benchmarks": [
      "Switchboard",
      "Dialogue State Tracking Challenge (DSTC) datasets",
      "Twitter Corpus (Ritter et al.)",
      "Twitter Triple Corpus",
      "Sina Weibo",
      "Ubuntu Chat Corpus"
    ],
    "resources": [
      "http://irclogs.ubuntu.com/",
      "https://github.com/rkadlec/ubuntu-ranking-dataset-creator",
      "http://github.com/npow/ubottu",
      "https://arxiv.org/abs/1506.08909"
    ]
  },
  "purpose_and_intended_users": {
    "goal": "Provide a very large corpus of unstructured multi-turn dialogues to enable research into dialogue systems (especially neural-network based dialogue managers) and to provide a benchmark for the task of selecting the best next response.",
    "audience": [
      "Machine Learning Researchers",
      "Dialogue System Researchers",
      "Model Developers"
    ],
    "tasks": [
      "Response Selection (Next Utterance Selection)",
      "Next Utterance Classification",
      "Multi-turn Dialogue Modeling"
    ],
    "limitations": "Conversation disentanglement is performed with heuristic rules which may be imperfect; initial questions can appear multiple times across dialogues when multiple users respond; dialogues spanning long time periods are treated as a single dialogue (posting time included so others may filter); conversations longer than five utterances where one user says >80% of utterances are discarded; only extracted dialogues with 3 or more turns are retained; no further preprocessing (e.g., tokenization, stemming) is applied to the released corpus.",
    "out_of_scope_uses": []
  },
  "data": {
    "source": "Extracted from Ubuntu Chat Logs (Freenode IRC network) used for Ubuntu-related technical support; logs available for 2004-2015 at http://irclogs.ubuntu.com/.",
    "size": "930,000 dialogues; 7,100,000 utterances; 100,000,000 words (as reported in Table 2).",
    "format": "Raw text with extracted (context, response, flag) triples for the test set; posting time included; released without additional preprocessing.",
    "annotation": "Automatically derived via extraction heuristics; test set contains (context, response, flag) where flag is a Boolean indicating whether the response is the actual next utterance; negative responses sampled randomly. No human annotation."
  },
  "methodology": {
    "methods": [
      "Automated metrics evaluation using Recall@k",
      "Model-based evaluation (TF-IDF baseline, Recurrent Neural Network, Long Short-Term Memory network)",
      "Train/test split with 2% of conversations set aside as test set; validation set extracted from training for hyper-parameter tuning"
    ],
    "metrics": [
      "Recall@1",
      "Recall@2",
      "Recall@5"
    ],
    "calculation": "Test set constructed by extracting (context, response, flag) triples from dialogues; for each context one correct response (flag=1) and one or more false responses (flag=0) sampled randomly from the test set (experiments used 1 false response and 9 false responses). Recall@k: agent selects k most likely responses and is correct if the true response is among these k candidates.",
    "interpretation": "Higher Recall@k indicates better ability to select the true next utterance given a context. Authors note that good performance on response classification is not necessarily a gauge of generation quality, but improvements on classification may lead to generation improvements.",
    "baseline_results": "TF-IDF, RNN, LSTM results (Table 4): 1 in 2 R@1 — TF-IDF 65.9%, RNN 76.8%, LSTM 87.8%. 1 in 10 R@1 — TF-IDF 41.0%, RNN 40.3%, LSTM 60.4%. 1 in 10 R@2 — TF-IDF 54.5%, RNN 54.7%, LSTM 74.5%. 1 in 10 R@5 — TF-IDF 70.8%, RNN 81.9%, LSTM 92.6%.",
    "validation": "2% of conversations randomly selected as a test set; a validation set was extracted from the training data for hyper-parameter optimization; training examples created by treating each utterance (from third onward) as a potential response with previous utterances as context; negative responses sampled randomly."
  },
  "targeted_risks": {
    "risk_categories": [
      "Accuracy"
    ],
    "atlas_risks": {
      "risks": [
        {
          "category": "Accuracy",
          "subcategory": [
            "Data contamination",
            "Unrepresentative data"
          ]
        }
      ]
    },
    "demographic_analysis": null,
    "harm": null
  },
  "ethical_and_legal_considerations": {
    "privacy_and_anonymity": "N/A",
    "data_licensing": "N/A",
    "consent_procedures": "N/A",
    "compliance_with_regulations": "N/A"
  }
}