{
  "benchmark_details": {
    "is_benchmark": true,
    "name": "NoisywikiHow: A Benchmark for Learning with Real-world Noisy Labels in Natural Language Processing",
    "abbreviation": "NoisywikiHow",
    "overview": "We present NoisywikiHow, a new NLP benchmark for evaluating learning with noisy labels (LNL) methods focusing on the intention identification task. NoisywikiHow is built from wikiHow procedural-event data and explicitly injects multiple, instance-dependent noise sources to simulate heterogeneous human annotation errors, and it provides controlled noise levels (0%, 10%, 20%, 40%, 60%) to support systematic evaluation.",
    "data_type": "text (procedural event -> intention classification pairs)",
    "domains": [
      "Natural Language Processing"
    ],
    "languages": [
      "English"
    ],
    "similar_benchmarks": [
      "NoisyNER",
      "Red MiniImageNet",
      "Red Stanford Cars",
      "Food-101N",
      "Animal-10N",
      "Clothing1M",
      "WebVision",
      "AudioSet",
      "FSDnoisy18K",
      "FSDKaggle2019"
    ],
    "resources": [
      "https://github.com/tangminji/NoisywikiHow",
      "https://arxiv.org/abs/2305.10709",
      "https://www.wikihow.com/robots.txt"
    ]
  },
  "purpose_and_intended_users": {
    "goal": "Provide a large-scale NLP benchmark with heterogeneous, instance-dependent, and controlled real-world label noise to systematically evaluate learning with noisy labels (LNL) methods on the intention identification task.",
    "audience": [
      "ML Researchers",
      "Model Developers",
      "Academic Research Community"
    ],
    "tasks": [
      "Text Classification",
      "Intention Identification"
    ],
    "limitations": "We simplify intention identification into a sentence classification task using a single procedural event rather than an entire event process; a more realistic formulation would use the entire event process.",
    "out_of_scope_uses": []
  },
  "data": {
    "source": "Crawled from the wikiHow website (procedural event headers as inputs and wikiHow article finest-granularity categories as labels).",
    "size": "89,143 instances in 158 classes (Train: 73,343; Validation: 7,900; Test: 7,900).",
    "format": "N/A",
    "annotation": "Each category was annotated by three graduate students in the NLP field and regarded as an event if more than two annotators agreed (Fleiss kappa = 0.84). The dataset was produced with minimal human supervision via a series of automated labeling procedures and controlled programmatic noise injection."
  },
  "methodology": {
    "methods": [
      "Automated metrics",
      "Model-based evaluation (fine-tuning pre-trained language models)",
      "Controlled noise injection experiments"
    ],
    "metrics": [
      "Top-1 Accuracy",
      "Top-5 Accuracy"
    ],
    "calculation": "Models are trained on noisy training sets and evaluated on the clean validation/test sets. Report Top-1 and Top-5 accuracy on the clean test set under controlled noise levels (0%, 10%, 20%, 40%, 60%).",
    "interpretation": "Higher Top-1/Top-5 accuracy indicates better generalization to clean data; increases in noise level generally lead to decreased Top-1/Top-5 accuracy, reflecting reduced robustness to noisy labels.",
    "baseline_results": "Examples: BART (base) achieves Top-1 61.72% (Top-5 86.90%) at 0% noise and Top-1 49.75% (Top-5 78.84%) at 60% noise (Table 4). SEAL achieves Top-1 63.29% (Top-5 87.65%) at 0% noise and Top-1 52.73% (Top-5 81.56%) at 60% noise (Table 8). Full baseline tables are provided in the paper.",
    "validation": "Randomly split 15,800 instances from clean data equally into validation and test sets; remaining 73,343 instances used as training set. Evaluate methods trained on noisy training sets on the clean validation/test sets. Controlled noise levels are 0%, 10%, 20%, 40%, and 60%."
  },
  "targeted_risks": {
    "risk_categories": [
      "Accuracy",
      "Robustness"
    ],
    "atlas_risks": {
      "risks": [
        {
          "category": "Accuracy",
          "subcategory": [
            "Data contamination",
            "Unrepresentative data",
            "Poor model accuracy"
          ]
        }
      ]
    },
    "demographic_analysis": "N/A",
    "harm": "Model overfitting to noisy labels and degraded generalization performance caused by label noise."
  },
  "ethical_and_legal_considerations": {
    "privacy_and_anonymity": "The paper states there is no privacy issue: data is constructed from the wikiHow website which is free and open for academic usage, and the authors declare compliance with the ACL Ethics Policy.",
    "data_licensing": "N/A",
    "consent_procedures": "N/A",
    "compliance_with_regulations": "Authors state the dataset was obtained and presented following the ACL Ethics Policy and that wikiHow content used is free and open for academic usage."
  }
}