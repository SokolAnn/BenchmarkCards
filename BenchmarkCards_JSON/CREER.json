{
  "benchmark_details": {
    "is_benchmark": true,
    "name": "CREER: A Large-Scale Corpus for Relation Extraction and Entity Recognition",
    "abbreviation": "CREER",
    "overview": "We describe the design and use of the CREER dataset, a large corpus annotated with rich English grammar and semantic attributes. The CREER dataset uses the Stanford CoreNLP Annotator to capture rich language structures from Wikipedia plain text. This large supervised dataset can serve as the basis for improving the performance of NLP tasks in the future.",
    "data_type": "text (annotated sentences with tokens, parse trees, dependencies, named entities, and relation KBP triplets)",
    "domains": [
      "Natural Language Processing"
    ],
    "languages": [
      "English"
    ],
    "similar_benchmarks": [
      "Wet Lab Protocols",
      "CoNLL-2003",
      "SemEval-2010 Task8",
      "OntoNotes 5.0",
      "Penn Treebank",
      "OIE2016",
      "MPQA 3.0",
      "SemEval-2014 Task4"
    ],
    "resources": [
      "https://140.116.82.111/share.cgi?ssid=000dOJ4",
      "https://dumps.wikimedia.org/"
    ]
  },
  "purpose_and_intended_users": {
    "goal": "Construct a large corpus (CREER) for Relation Extraction and Entity Recognition containing syntax (parse trees and dependencies) and world knowledge information (entity mentions and relations) annotated from English Wikipedia using Stanford CoreNLP, to serve as a large supervised dataset and a testbed for NLP tasks and pre-training.",
    "audience": [],
    "tasks": [
      "Named Entity Recognition",
      "Relation Extraction"
    ],
    "limitations": "N/A",
    "out_of_scope_uses": []
  },
  "data": {
    "source": "English Wikipedia Dump dataset annotated using Stanford CoreNLP (Library stanfordcorenlp as a Python wrapper) to produce tokens, parse trees, dependencies, named entities, and relation KBP.",
    "size": "144,732,654 sentences; based on English Wikipedia Dump (2,500M words)",
    "format": "N/A",
    "annotation": "Annotated using Stanford CoreNLP. Annotations include tokens, parse trees, basic and enhanced dependencies, named entities (PERSON, LOCATION, ORGANIZATION, MISC, MONEY, NUMBER, SET, DATE, TIME, DURATION), and knowledge base population (KBP) relation triplets (slot filling and entity linking)."
  },
  "methodology": {
    "methods": [
      "Automated annotation using the Stanford CoreNLP pipeline (via the stanfordcorenlp Python wrapper)",
      "Comparison of dataset statistics to existing corpora"
    ],
    "metrics": [],
    "calculation": "N/A",
    "interpretation": "N/A",
    "baseline_results": null,
    "validation": "N/A"
  },
  "targeted_risks": {
    "risk_categories": [],
    "atlas_risks": {
      "risks": null
    },
    "demographic_analysis": null,
    "harm": null
  },
  "ethical_and_legal_considerations": {
    "privacy_and_anonymity": "N/A",
    "data_licensing": "N/A",
    "consent_procedures": "N/A",
    "compliance_with_regulations": "N/A"
  }
}