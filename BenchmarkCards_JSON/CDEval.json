{
  "benchmark_details": {
    "is_benchmark": true,
    "name": "CDEval",
    "abbreviation": "N/A",
    "overview": "CDEval is designed to evaluate the cultural dimensions of Large Language Models (LLMs) by covering six key cultural dimensions across seven domains. It provides insights into the cultural orientations of mainstream LLMs.",
    "data_type": "question-answering pairs",
    "domains": [
      "Natural Language Processing"
    ],
    "languages": [
      "English",
      "Chinese",
      "German"
    ],
    "similar_benchmarks": [
      "MMLU",
      "CValues"
    ],
    "resources": [
      "https://huggingface.co/datasets/Rykeryuhang/CDEval"
    ]
  },
  "purpose_and_intended_users": {
    "goal": "To provide a benchmark for assessing the cultural dimensions within large language models and to enhance their alignment with cultural diversity.",
    "audience": [
      "ML Researchers",
      "Cultural Analysts",
      "Ethics Researchers"
    ],
    "tasks": [
      "Cultural Evaluation",
      "Cross-Cultural Assessment"
    ],
    "limitations": "The cultural dimensions explored are confined to six, which may limit real-world applications.",
    "out_of_scope_uses": []
  },
  "data": {
    "source": "Generated using GPT-4 and verified by humans.",
    "size": "2,953 questions",
    "format": "JSON",
    "annotation": "Human verification of generated questions"
  },
  "methodology": {
    "methods": [
      "Automated metrics",
      "Human evaluation"
    ],
    "metrics": [
      "Cultural Orientation Likelihood"
    ],
    "calculation": "Cultural orientation likelihood is calculated based on user prompts and model responses.",
    "interpretation": "Higher values indicate a stronger alignment towards a specified cultural orientation.",
    "baseline_results": null,
    "validation": "Evaluated across different LLMs to ensure stability and consistency"
  },
  "targeted_risks": {
    "risk_categories": [
      "Bias",
      "Fairness"
    ],
    "atlas_risks": {
      "risks": [
        {
          "category": "Fairness",
          "subcategory": [
            "Data bias",
            "Output bias"
          ]
        }
      ]
    },
    "demographic_analysis": "N/A",
    "harm": []
  },
  "ethical_and_legal_considerations": {
    "privacy_and_anonymity": "N/A",
    "data_licensing": "N/A",
    "consent_procedures": "N/A",
    "compliance_with_regulations": "N/A"
  }
}