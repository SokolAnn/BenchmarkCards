{
  "benchmark_details": {
    "is_benchmark": true,
    "name": "XCODEEVAL: AN EXECUTION-BASED LARGE SCALE MULTILINGUAL MULTITASK BENCHMARK FOR CODE UNDERSTANDING, GENERATION, TRANSLATION AND RETRIEVAL",
    "abbreviation": "XCODEEVAL",
    "overview": "XCODEEVAL is an executable multilingual multitask benchmark consisting of document-level coding examples from codeforces.com (about 7.5K unique problems). It features seven tasks spanning code understanding, generation, translation and retrieval, adopts execution-based evaluation via unit tests, and provides a multilingual code execution engine (ExecEval).",
    "data_type": "text (document-level code examples, natural language problem descriptions, and unit tests)",
    "domains": [
      "Programming Language",
      "Competitive Programming"
    ],
    "languages": [],
    "similar_benchmarks": [
      "HumanEval",
      "HumanEval-x",
      "MBPP",
      "APPS",
      "MBXP",
      "CodeContests",
      "TransCoder",
      "CodeXGLUE"
    ],
    "resources": [
      "https://github.com/ntunlp/xCodeEval",
      "https://huggingface.co/datasets/NTU-NLP-sg/xCodeEval",
      "https://github.com/ntunlp/ExecEval"
    ]
  },
  "purpose_and_intended_users": {
    "goal": "To provide a large-scale, executable, multilingual and multitask benchmark for evaluating code understanding, generation, translation and retrieval, using execution-based (unit test) evaluation and a multilingual execution engine (ExecEval).",
    "audience": [
      "LLM Builders",
      "ML Researchers",
      "Model Developers"
    ],
    "tasks": [
      "Tag Classification",
      "Code Compilation",
      "Program Synthesis",
      "Automatic Program Repair",
      "Code Translation",
      "Code-Code Retrieval",
      "Natural Language-Code Retrieval"
    ],
    "limitations": "Data collected from a single source (codeforces.com) which limits domain diversity; discrepancy in resource availability across programming languages; most codes are document-level and often non-modular without doc-strings; data has not been humanly audited (automated tools were used to remove sensitive details and approximately 2 million samples were removed); possibility of data leakage/contamination from pretraining corpora is discussed.",
    "out_of_scope_uses": []
  },
  "data": {
    "source": "Collected from codeforces.com (openly available submissions).",
    "size": "25M document-level coding examples (16.5B tokens); 7,514 distinct problems (as stated in the paper).",
    "format": "jsonl, json, Apache Arrow (datasets loader)",
    "annotation": "Human-written metadata labels from codeforces (tags updated by experienced problem solvers); execution outcomes derived from compilers/interpreters; validation/test unit tests provided (hidden unit tests)."
  },
  "methodology": {
    "methods": [
      "Execution-based evaluation using unit tests (ExecEval)",
      "Automated metrics (pass@k, top-k accuracy, macro-F1, accuracy)",
      "Model-based evaluation (zero-shot evaluation with ChatGPT gpt-3.5-turbo-0301; fine-tuning and evaluation of open models such as Starcoderbase-3B, StarEncoder, CodeLlama)"
    ],
    "metrics": [
      "macro-F1",
      "Accuracy",
      "pass@k",
      "Top-k accuracy (Acc@k)"
    ],
    "calculation": "macro-F1: compute F1 per tag and average (macro) over tags; Accuracy: (TP+TN)/(TP+TN+FP+FN); pass@k: execution-based metric as in Chen et al. (2021) (proportion of problems with a passing sample among k samples); Top-k accuracy (Acc@k): proportion of queries with a correct code retrieved within top-k.",
    "interpretation": "Higher macro-F1, Accuracy, pass@k and Acc@k indicate better performance. The paper presents comparative results indicating current LLMs obtain substantially lower pass@k on XCODEEVAL than on some prior benchmarks, marking the benchmark as challenging.",
    "baseline_results": "Baselines include ChatGPT (gpt-3.5-turbo-0301) and StarEncoder/Starcoder/CodeLlama variants. Example results reported: ChatGPT average Tag Classification (DesCode2Tag) macro-F1 33.6, Code Compilation accuracy 63.27 (average), Program Synthesis pass@5 average ~27.8 (sampling T) and ~30.48 (N), Automatic Program Repair pass@5 average 55.07. Starcoderbase-3B (fine-tuned) pass@5 average on Program Synthesis = 2.25; CodeLlama-13b-Instruct pass@5 average = 3.81 (Table 3 and Table 5 in paper).",
    "validation": "Validation/test created from a held-out set of Nh = 1,354 problems to ensure problems in validation/test are unseen in training. Data splitting uses a tag-distribution score (geometric mean of per-tag ratios) to select splits. To balance samples across problems and tags, a graph-theoretic data selection schema (formulated as a circulation problem with lower and upper bounds in a flow network) is used."
  },
  "targeted_risks": {
    "risk_categories": [
      "Privacy",
      "Fairness",
      "Accuracy",
      "Data Laws"
    ],
    "atlas_risks": {
      "risks": [
        {
          "category": "Privacy",
          "subcategory": [
            "Personal information in data"
          ]
        },
        {
          "category": "Fairness",
          "subcategory": [
            "Data bias"
          ]
        },
        {
          "category": "Accuracy",
          "subcategory": [
            "Data contamination"
          ]
        },
        {
          "category": "Data Laws",
          "subcategory": [
            "Data usage restrictions"
          ]
        }
      ]
    },
    "demographic_analysis": null,
    "harm": [
      "Sensitive information leakage from code samples (paper notes possibility of sensitive information and that ~2M samples were removed using automated tools)",
      "Security vulnerabilities present in code samples",
      "Model performance contamination due to pretraining data leakage (data contamination)"
    ]
  },
  "ethical_and_legal_considerations": {
    "privacy_and_anonymity": "Data is collected from openly available sources and has not been humanly audited. Automated tools were used to identify and remove codes with sensitive details, resulting in the removal of approximately 2 million samples; the authors note sensitive information and security vulnerabilities may still remain.",
    "data_licensing": "CC BY-NC 4.0 (as stated in Table 12).",
    "consent_procedures": "N/A",
    "compliance_with_regulations": "N/A"
  }
}