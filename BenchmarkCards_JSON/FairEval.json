{
  "benchmark_details": {
    "name": "Large Language Models are not Fair Evaluators",
    "overview": "This paper uncovers systematic bias in the evaluation paradigm when using large language models (LLMs) as referees to score and compare responses generated by other models. It identifies that the quality ranking can be skewed by simply altering the order of responses presented to the evaluator, leading to unreliable evaluations. The authors propose a novel calibration framework with three strategies to mitigate this bias.",
    "data_type": "Comparative evaluation of AI responses",
    "domains": [
      "Natural Language Processing",
      "AI Evaluator Bias"
    ],
    "languages": [
      "English"
    ],
    "similar_benchmarks": [
      "Vicuna Benchmark",
      "MT-Bench"
    ],
    "resources": [
      "https://github.com/i-Eval/FairEval"
    ]
  },
  "purpose_and_intended_users": {
    "goal": "To reveal the positional bias in LLM evaluation and provide solutions to mitigate it.",
    "audience": [
      "Researchers in AI and NLP",
      "Developers of LLMs",
      "AI ethics researchers"
    ],
    "tasks": [
      "Evaluating AI assistant performance",
      "Understanding evaluator biases",
      "Improving LLM evaluation methodologies"
    ],
    "limitations": "The proposed methods may not generalize beyond the tested models and benchmarks.",
    "out_of_scope_uses": [
      "Real-time evaluation of untrained models",
      "Evaluation outside of controlled conditions"
    ]
  },
  "data": {
    "source": "Vicuna Benchmark",
    "size": "80 examples with 9 distinct question categories",
    "format": "Question and answer pairs with annotations",
    "annotation": "Manual 'win/tie/lose' outcomes of responses annotated by experts"
  },
  "methodology": {
    "methods": [
      "Multiple Evidence Calibration (MEC)",
      "Balanced Position Calibration (BPC)",
      "Human-in-the-Loop Calibration (HITLC)"
    ],
    "metrics": [
      "Accuracy",
      "Kappa correlation coefficient",
      "Conflict Rate"
    ],
    "calculation": "Average score based on multiple evaluations and human annotations",
    "interpretation": "Higher scores indicate better alignment with human judgments",
    "baseline_results": "Performance compared to standard evaluation methods (VANILLA)",
    "validation": "Methods validated against human annotations across various tasks"
  },
  "targeted_risks": {
    "risk_categories": [
      "Evaluation Bias",
      "Positional Bias",
      "Evaluation Reliability"
    ],
    "atlas_risks": {
      "risks": [
        {
          "category": "Fairness",
          "subcategory": [
            "Output bias"
          ]
        },
        {
          "category": "Robustness",
          "subcategory": [
            "Prompt injection attack"
          ]
        },
        {
          "category": "Explainability",
          "subcategory": [
            "Untraceable attribution"
          ]
        }
      ]
    },
    "demographic_analysis": "Not specifically provided",
    "harm": "Potential reliance on biased evaluations leading to inaccurate model assessments."
  },
  "ethical_and_legal_considerations": {
    "privacy_and_anonymity": "N/A",
    "data_licensing": "N/A",
    "consent_procedures": "N/A",
    "compliance_with_regulations": "N/A"
  }
}