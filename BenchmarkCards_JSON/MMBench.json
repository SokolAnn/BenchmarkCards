{
  "benchmark_details": {
    "is_benchmark": true,
    "name": "MMBench",
    "abbreviation": "MMBench",
    "overview": "MMBench is a bilingual benchmark for assessing the multi-modal capabilities of vision-language models (VLMs). It is a systematically designed objective evaluation benchmark containing over 3,000 multiple-choice questions across 20 fine-grained ability dimensions, introduces a CircularEval strategy, and uses large language models to convert free-form predictions into pre-defined choices to yield accurate evaluation results for models with limited instruction-following capability.",
    "data_type": "multimodal (image + text) multiple-choice question-answer pairs",
    "domains": [
      "Natural Language Processing",
      "Computer Vision",
      "Multimodal"
    ],
    "languages": [
      "English",
      "Chinese"
    ],
    "similar_benchmarks": [
      "VQAv2",
      "COCO Caption",
      "GQA",
      "OK-VQA",
      "TextVQA",
      "ScienceQA",
      "Youcook2",
      "OwlEval",
      "LVLM-eHub",
      "MME",
      "SEEDBench"
    ],
    "resources": [
      "https://arxiv.org/abs/2307.06281",
      "https://arxiv.org/pdf/2307.06281v3"
    ]
  },
  "purpose_and_intended_users": {
    "goal": "To robustly and holistically evaluate the multi-modal perception and reasoning abilities of vision-language models across 20 fine-grained ability dimensions using an objective, scalable benchmark.",
    "audience": [
      "Research community",
      "ML Researchers",
      "Model Developers"
    ],
    "tasks": [
      "Question Answering",
      "Visual Question Answering",
      "Object Localization",
      "Optical Character Recognition",
      "Image Scene Recognition",
      "Image Quality Assessment",
      "Image-Text Understanding"
    ],
    "limitations": "N/A",
    "out_of_scope_uses": []
  },
  "data": {
    "source": "Manually collected from multiple sources: ~80% from the Internet and ~20% from validation sets of public datasets and other sources. Listed image/problem sources include ARAS, CLEVR, COCO, KonIQ-10k, LLaV A, PISC, Places, ScienceQA, ShapeWorld, TextVQA, VSR, W3C School, and Internet. Questions and choices are manually constructed or selected; MMBench-CN translations were generated by GPT-4 and verified by humans.",
    "size": "3,217 examples",
    "format": "N/A",
    "annotation": "Manually collected and constructed by volunteers (undergraduate/graduate students). Quality control includes LLM-based text-only filtering (GPT-4, Gemini-Pro, Qwen-Max majority voting), automatic filtering using multiple VLMs, and manual verification. Translations to Chinese done with GPT-4 and human verification."
  },
  "methodology": {
    "methods": [
      "Automated metrics (exact match / Top-1 accuracy)",
      "Heuristic matching to extract choice labels from model outputs",
      "LLM-based choice extraction (GPT-4 used to map free-form outputs to choices)",
      "CircularEval (multi-pass circular-shift consistency evaluation)"
    ],
    "metrics": [
      "Top-1 Accuracy",
      "Matching Success Rate (heuristic matching)",
      "Alignment Rate with Human (LLM choice extraction alignment)"
    ],
    "calculation": "CircularEval: for an N-choice question, the question is fed to the VLM N times with circularly shifted choices; a model is considered correct only if it predicts the correct choice in all passes. Choice extraction: first attempt heuristic matching for explicit labels; if that fails, use GPT-4 to map free-form prediction to one of the candidate choices (or pseudo-choice 'Z').",
    "interpretation": "Higher Top-1 Accuracy under CircularEval indicates better and more consistent multimodal understanding. CircularEval is stricter than single-pass (VanillaEval) and reveals robustness and instruction-following limitations; alignment rate between LLM extractor and humans indicates reliability of LLM-based matching.",
    "baseline_results": "Selected CircularEval overall Top-1 accuracies on MMBench test (as reported): InternLM-XComposer2: 78.1% overall; GPT-4v: 74.3% overall; Qwen-VL-Max: 75.4% overall; InternLM-XComposer2 reported as the top-performing evaluated model.",
    "validation": "Validated LLM choice extraction by sampling ~420 hard examples (10% of unparsable predictions) and comparing GPT-4 extraction with human annotations; GPT-4 alignment with humans reported as 91.5%. CircularEval vs VanillaEval comparisons demonstrate significant drops for many VLMs, supporting CircularEval's stricter evaluation behavior."
  },
  "targeted_risks": {
    "risk_categories": [
      "Fairness",
      "Accuracy"
    ],
    "atlas_risks": {
      "risks": [
        {
          "category": "Fairness",
          "subcategory": [
            "Decision bias"
          ]
        },
        {
          "category": "Accuracy",
          "subcategory": [
            "Poor model accuracy"
          ]
        }
      ]
    },
    "demographic_analysis": null,
    "harm": null
  },
  "ethical_and_legal_considerations": {
    "privacy_and_anonymity": "N/A",
    "data_licensing": "N/A",
    "consent_procedures": "N/A",
    "compliance_with_regulations": "N/A"
  }
}