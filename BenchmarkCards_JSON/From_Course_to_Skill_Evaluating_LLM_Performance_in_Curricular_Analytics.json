{
  "benchmark_details": {
    "is_benchmark": true,
    "name": "From Course to Skill: Evaluating LLM Performance in Curricular Analytics",
    "abbreviation": "N/A",
    "overview": "This paper systematically evaluates the performance of large language models (LLMs) versus traditional NLP methods in the task of skill extraction from curriculum documents, demonstrating the varying performance of LLMs based on model selection and prompting strategies.",
    "data_type": "curriculum documents",
    "domains": [
      "Natural Language Processing",
      "Higher Education"
    ],
    "languages": [
      "English"
    ],
    "similar_benchmarks": [
      "N/A"
    ],
    "resources": [
      "https://github.com/AEQUITAS-Lab/Evaluation-of-LLM-in-CA-AIED-2025"
    ]
  },
  "purpose_and_intended_users": {
    "goal": "To evaluate and benchmark the performance of LLMs in curricular analytics against traditional NLP methods for skill extraction.",
    "audience": [
      "Researchers",
      "Educators"
    ],
    "tasks": [
      "Skill Extraction"
    ],
    "limitations": "N/A",
    "out_of_scope_uses": []
  },
  "data": {
    "source": "Curriculum Documents from Open Syllabus and a public two-year college catalog.",
    "size": "400 curriculum documents",
    "format": "N/A",
    "annotation": "Human-Large Language Model collaborative evaluation framework."
  },
  "methodology": {
    "methods": [
      "Human evaluation",
      "Automated metrics"
    ],
    "metrics": [
      "Precision",
      "Mean",
      "Normalized Discounted Cumulative Gain (NDCG)"
    ],
    "calculation": "Performance metrics calculated based on human assessments of skill alignment.",
    "interpretation": "Higher scores indicate better skill alignment from extracted skills.",
    "baseline_results": "N/A",
    "validation": "Used Cohen's Kappa and ICC for reliability analysis."
  },
  "targeted_risks": {
    "risk_categories": [
      "Accuracy"
    ],
    "atlas_risks": {
      "risks": [
        {
          "category": "Accuracy",
          "subcategory": [
            "Poor model accuracy"
          ]
        }
      ]
    },
    "demographic_analysis": "N/A",
    "harm": "N/A"
  },
  "ethical_and_legal_considerations": {
    "privacy_and_anonymity": "N/A",
    "data_licensing": "N/A",
    "consent_procedures": "N/A",
    "compliance_with_regulations": "N/A"
  }
}