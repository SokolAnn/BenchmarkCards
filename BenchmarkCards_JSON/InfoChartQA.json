{
  "benchmark_details": {
    "is_benchmark": true,
    "name": "InfoChartQA",
    "abbreviation": "N/A",
    "overview": "InfoChartQA is a benchmark for evaluating multimodal large language models (MLLMs) on infographic chart understanding. It includes 5,642 pairs of infographic and plain charts, each sharing the same underlying data but differing in visual presentations, along with visual-element-based questions designed to capture unique visual designs and communicative intents.",
    "data_type": "question-answering pairs",
    "domains": [
      "Computer Vision"
    ],
    "languages": [
      "English"
    ],
    "similar_benchmarks": [
      "InfographicVQA",
      "ChartQAPro"
    ],
    "resources": [
      "https://github.com/CoolDawnAnt/InfoChartQA"
    ]
  },
  "purpose_and_intended_users": {
    "goal": "To provide a systematic benchmark for evaluating multimodal large language models on the understanding of infographic charts and their visual elements.",
    "audience": [
      "ML Researchers",
      "Model Developers"
    ],
    "tasks": [
      "Visual Question Answering"
    ],
    "limitations": "While InfoChartQA presents a comprehensive benchmark, it has limitations such as difficulties in constructing metaphor-related questions and reliance on templates that may limit diversity.",
    "out_of_scope_uses": []
  },
  "data": {
    "source": "Collected from 11 real-world mainstream visualization platforms.",
    "size": "5,642 infographic and plain chart pairs",
    "format": "N/A",
    "annotation": "Created by combining automated and manual processes involving expert verification."
  },
  "methodology": {
    "methods": [
      "Human evaluation",
      "Automated metrics"
    ],
    "metrics": [
      "Accuracy"
    ],
    "calculation": "Accuracy calculated based on ANLS score and relaxed accuracy metric for numeric answers.",
    "interpretation": "Higher performance indicates better understanding of infographic design elements and data relationships.",
    "baseline_results": "Human baseline performance is set as a benchmark for MLLM comparisons.",
    "validation": "Thorough evaluation using a diverse set of MLLMs on the dataset."
  },
  "targeted_risks": {
    "risk_categories": [
      "Fairness",
      "Accuracy",
      "Robustness"
    ],
    "atlas_risks": {
      "risks": []
    },
    "demographic_analysis": "N/A",
    "harm": []
  },
  "ethical_and_legal_considerations": {
    "privacy_and_anonymity": "N/A",
    "data_licensing": "Custom licensing from data sources.",
    "consent_procedures": "N/A",
    "compliance_with_regulations": "N/A"
  }
}