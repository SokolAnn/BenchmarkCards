{
  "benchmark_details": {
    "is_benchmark": true,
    "name": "CBT-B ENCH",
    "abbreviation": "CBT-B ENCH",
    "overview": "CBT-B ENCH is a systematic benchmark for evaluating the assistance of large language models (LLMs) in Cognitive Behavioral Therapy (CBT) across three levels: basic knowledge acquisition, cognitive model understanding, and therapeutic response generation.",
    "data_type": "multiple-choice questions, classification examples, response generation tasks",
    "domains": [
      "Natural Language Processing",
      "Healthcare"
    ],
    "languages": [
      "English"
    ],
    "similar_benchmarks": [],
    "resources": [
      "https://github.com/mianzhang/CBT-Bench"
    ]
  },
  "purpose_and_intended_users": {
    "goal": "To thoroughly investigate the proficiency and potential of LLMs in supporting various facets and stages of professional mental health care in CBT.",
    "audience": [
      "ML Researchers",
      "Mental Health Professionals"
    ],
    "tasks": [
      "Multiple-Choice Question Answering",
      "Cognitive Distortion Classification",
      "Core Belief Classification",
      "Therapeutic Response Generation"
    ],
    "limitations": "The size of datasets is limited by the cost of annotating in specialized fields like mental health.",
    "out_of_scope_uses": []
  },
  "data": {
    "source": "CBT exam questions for Master of Social Work (MSW) students and compositions by CBT experts.",
    "size": "220 multiple-choice questions, 146 cognitive distortion examples, 184 primary core belief examples, 112 fine-grained core belief examples, 156 therapeutic response exercises",
    "format": "JSON, CSV",
    "annotation": "Annotated by CBT experts, with rigorous verification for quality."
  },
  "methodology": {
    "methods": [
      "Human evaluation",
      "Automated metrics"
    ],
    "metrics": [
      "Accuracy",
      "F1 Score"
    ],
    "calculation": "Accuracy for multiple-choice questions and weighted precision, recall, F1 score for classification datasets.",
    "interpretation": "Performance is compared against human expert scores to evaluate LLMs' capabilities.",
    "baseline_results": "Human baseline accuracy for CBT-QA is 90.7%. LLM performance varies significantly across tasks.",
    "validation": "Validation involved cross-verification and expert evaluations of model outputs."
  },
  "targeted_risks": {
    "risk_categories": [
      "Bias",
      "Safety",
      "Privacy",
      "Robustness",
      "Fairness",
      "Accuracy"
    ],
    "atlas_risks": {
      "risks": []
    },
    "demographic_analysis": "N/A",
    "harm": "N/A"
  },
  "ethical_and_legal_considerations": {
    "privacy_and_anonymity": "Data collected from experts was de-identified and consented for research purposes.",
    "data_licensing": "N/A",
    "consent_procedures": "Informed consent obtained from all participants involved in data annotation.",
    "compliance_with_regulations": "This project was approved by the Institutional Review Board (IRB)."
  }
}