{
  "benchmark_details": {
    "is_benchmark": true,
    "name": "Hard-Bench",
    "abbreviation": "N/A",
    "overview": "Hard-Bench: a challenging low-resource benchmark that selects \"hard examples\" (by loss or gradient-norm scores using a short/early-stopped biased predictor) from existing datasets to better evaluate learning and generalization in low-resource settings. The benchmark covers 11 datasets (3 computer vision datasets and 8 natural language processing datasets) and provides two variants: Hard-Bench (Loss) and Hard-Bench (GradNorm). Code is available at https://github.com/Qian2333/Hard-Bench.",
    "data_type": "image classification samples; text classification and sentence-pair classification examples",
    "domains": [
      "Computer Vision",
      "Natural Language Processing"
    ],
    "languages": [
      "English"
    ],
    "similar_benchmarks": [
      "BigBench",
      "superGLUE",
      "GLUE",
      "miniImageNet",
      "CIFAR-FS",
      "Wilds"
    ],
    "resources": [
      "https://github.com/Qian2333/Hard-Bench",
      "https://arxiv.org/abs/2303.03840"
    ]
  },
  "purpose_and_intended_users": {
    "goal": "To build a challenging low-resource benchmark that includes hard and biased examples to better evaluate the learning ability and robustness of neural networks in low-resource settings.",
    "audience": [
      "ML Researchers",
      "Model Developers"
    ],
    "tasks": [
      "Image Classification",
      "Text Classification",
      "Natural Language Inference",
      "Paraphrase Identification"
    ],
    "limitations": "Default setting selects fixed examples per label for reported results (authors note space limitations and additional experiments in appendices).",
    "out_of_scope_uses": []
  },
  "data": {
    "source": "Subsets selected from CIFAR-10, CIFAR-100, ImageNet-1K (CV) and 8 GLUE datasets (SST-2, CoLA, MNLI, QNLI, MRPC, QQP, RTE, WNLI) (NLP). Hard examples are selected by scoring samples with a biased predictor trained for one epoch and ranking by Loss or GradNorm.",
    "size": "CIFAR-10: 500 examples per label (10% of training set); CIFAR-100: 50 examples per label; ImageNet-1K: 100 examples per label; GLUE datasets: 500 examples per label (100 examples for WNLI).",
    "format": "N/A",
    "annotation": "Selected hard examples undergo a human-check process to avoid mislabeled examples (authors state: \"To avoid involving mislabelled examples, we add the human-check process in this work\")."
  },
  "methodology": {
    "methods": [
      "Automated metrics (test/validation Accuracy)",
      "Model-based evaluation across 11 models (including 7 pre-trained models)",
      "Repeated trials and averaging (experiments run three times and averaged)"
    ],
    "metrics": [
      "Accuracy"
    ],
    "calculation": "Accuracy is computed on the test/validation set (for GLUE tasks the validation set is used as the test set). Reported results are averages over three runs. Hard-Bench variants are constructed by selecting top-k samples per label ranked by Loss score or Gradient Norm score computed using a biased predictor trained for one epoch.",
    "interpretation": "Large drops in Accuracy on Hard-Bench relative to randomly-sampled low-resource benchmarks indicate that Hard-Bench better exposes weaknesses and a robustness/generalization gap of models. The authors report that Hard-Bench (Loss) is generally more challenging than Hard-Bench (GradNorm).",
    "baseline_results": "Representative results reported by authors: On SST-2 Random-Bench RoBERTa: 91.54% test accuracy; On SST-2 Hard-Bench (GradNorm) RoBERTa: 51.01%; On SST-2 Hard-Bench (Loss) RoBERTa: 50.55%. On CIFAR-10 Random-Bench DenseNet-121: 71.33% -> Hard-Bench (GradNorm): 59.87% -> Hard-Bench (Loss): 44.81%.",
    "validation": "Validation procedures include using the GLUE validation set as the test set (to avoid hidden test set issues), averaging results over three runs, human-check of selected examples to avoid mislabeled data, and ablation studies including 100 random samplings to compare worst-case Random-Bench performance."
  },
  "targeted_risks": {
    "risk_categories": [
      "Bias",
      "Accuracy"
    ],
    "atlas_risks": {
      "risks": [
        {
          "category": "Fairness",
          "subcategory": [
            "Data bias"
          ]
        },
        {
          "category": "Accuracy",
          "subcategory": [
            "Poor model accuracy"
          ]
        }
      ]
    },
    "demographic_analysis": "N/A",
    "harm": [
      "Over-estimated model performance on traditional low-resource benchmarks",
      "Generalization failures revealed by hard and biased low-resource examples"
    ]
  },
  "ethical_and_legal_considerations": {
    "privacy_and_anonymity": "N/A",
    "data_licensing": "N/A",
    "consent_procedures": "N/A",
    "compliance_with_regulations": "N/A"
  }
}