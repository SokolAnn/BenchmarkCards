{
  "benchmark_details": {
    "is_benchmark": true,
    "name": "This is Not a Dataset",
    "abbreviation": "N/A",
    "overview": "The largest negation probing dataset, which includes affirmative and negative sentences with and without distractors, incorporating multiple types of relations and negations to improve understanding of negation by large language models.",
    "data_type": "text",
    "domains": [
      "Natural Language Processing"
    ],
    "languages": [
      "English"
    ],
    "similar_benchmarks": [],
    "resources": [
      "https://github.com/hitz-zentroa/This-is-not-a-Dataset"
    ]
  },
  "purpose_and_intended_users": {
    "goal": "To investigate and improve the understanding of negation in large language models by providing a comprehensive dataset for evaluation.",
    "audience": [
      "ML Researchers",
      "NLP Practitioners"
    ],
    "tasks": [
      "Negation Understanding"
    ],
    "limitations": "The dataset contains a limited number of low-quality sentences.",
    "out_of_scope_uses": []
  },
  "data": {
    "source": "Generated from WordNet relations.",
    "size": "381,300 sentences",
    "format": "N/A",
    "annotation": "Semi-automatically generated with human quality assessment."
  },
  "methodology": {
    "methods": [
      "Human evaluation",
      "Automated metrics"
    ],
    "metrics": [
      "Accuracy",
      "Coherence"
    ],
    "calculation": "Accuracy is calculated as (TP + TN) / (TP + TN + FP + FN). Coherence is determined based on the relationship between affirmative and negative sentence labels.",
    "interpretation": "A higher accuracy indicates a model's proficiency in understanding the truth values of sentences. Coherence reflects the model's consistency in labeling affirmative and negative pairs correctly.",
    "baseline_results": null,
    "validation": "The dataset's linguistic quality is validated through human evaluation."
  },
  "targeted_risks": {
    "risk_categories": [
      "Fairness",
      "Accuracy"
    ],
    "atlas_risks": {
      "risks": [
        {
          "category": "Fairness",
          "subcategory": [
            "Data bias"
          ]
        },
        {
          "category": "Accuracy",
          "subcategory": [
            "Unrepresentative data"
          ]
        }
      ]
    },
    "demographic_analysis": null,
    "harm": []
  },
  "ethical_and_legal_considerations": {
    "privacy_and_anonymity": "N/A",
    "data_licensing": "N/A",
    "consent_procedures": "N/A",
    "compliance_with_regulations": "N/A"
  }
}