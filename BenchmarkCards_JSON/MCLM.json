{
  "benchmark_details": {
    "is_benchmark": true,
    "name": "MCLM (Multilingual Competition Level Math)",
    "abbreviation": "MCLM",
    "overview": "MCLM is a multilingual math benchmark featuring competition-level problems in 55 languages. It aims to investigate the linguistic generalizability of test-time scaling methods in mathematical reasoning.",
    "data_type": "mathematical reasoning problems",
    "domains": [
      "Natural Language Processing"
    ],
    "languages": [
      "Afrikaans",
      "Albanian",
      "Arabic",
      "Bengali",
      "Bulgarian",
      "Catalan",
      "Chinese (Simplified)",
      "Chinese (Traditional)",
      "Croatian",
      "Czech",
      "Danish",
      "Dutch",
      "Estonian",
      "Finnish",
      "French",
      "German",
      "Greek",
      "Gujarati",
      "Hebrew",
      "Hindi",
      "Hungarian",
      "Indonesian",
      "Italian",
      "Japanese",
      "Kannada",
      "Korean",
      "Latvian",
      "Lithuanian",
      "Macedonian",
      "Malayalam",
      "Marathi",
      "Nepali",
      "Norwegian",
      "Persian",
      "Polish",
      "Portuguese",
      "Punjabi",
      "Romanian",
      "Russian",
      "Slovak",
      "Slovenian",
      "Somali",
      "Spanish",
      "Swahili",
      "Swedish",
      "Tagalog",
      "Tamil",
      "Telugu",
      "Thai",
      "Turkish",
      "Ukrainian",
      "Urdu",
      "Vietnamese",
      "Welsh",
      "English"
    ],
    "similar_benchmarks": [
      "MGSM",
      "AIME"
    ],
    "resources": [
      "https://github.com/gauss5930/MCLM"
    ]
  },
  "purpose_and_intended_users": {
    "goal": "To provide a benchmark that evaluates the multilingual reasoning capability of models in mathematical contexts.",
    "audience": [
      "ML Researchers",
      "Model Developers",
      "Educators"
    ],
    "tasks": [
      "Mathematical Reasoning"
    ],
    "limitations": "While the benchmark provides high-level competition problems, it does not ensure robust performance across all languages.",
    "out_of_scope_uses": []
  },
  "data": {
    "source": "MATH-500 and AIME datasets translated using GPT-4o, along with human-validated mathematical Olympiad questions.",
    "size": "250,000 questions across 55 languages",
    "format": "JSON",
    "annotation": "Machine-translated and verified human-annotated translations"
  },
  "methodology": {
    "methods": [
      "Human evaluation",
      "Automated metrics"
    ],
    "metrics": [
      "Accuracy",
      "Fleiss' Kappa"
    ],
    "calculation": "Metrics are calculated based on the proportion of correct answers and the consistency across languages.",
    "interpretation": "Higher accuracy indicates better overall performance, while higher Fleiss' Kappa suggests better cross-lingual consistency.",
    "baseline_results": "Qwen2.5-1.5B achieved a score of 35.8 on MCLM.",
    "validation": "Validation conducted through comparing model outputs against verified answers."
  },
  "targeted_risks": {
    "risk_categories": [
      "Bias",
      "Fairness",
      "Accuracy"
    ],
    "atlas_risks": {
      "risks": []
    },
    "demographic_analysis": "N/A",
    "harm": "N/A"
  },
  "ethical_and_legal_considerations": {
    "privacy_and_anonymity": "N/A",
    "data_licensing": "Machine-translated subsets are released under the MIT License; other subsets under a CC BY-NC-ND license.",
    "consent_procedures": "N/A",
    "compliance_with_regulations": "N/A"
  }
}