{
  "benchmark_details": {
    "is_benchmark": true,
    "name": "MultiLoKo",
    "abbreviation": "N/A",
    "overview": "MultiLoKo is a new benchmark for evaluating multilinguality in LLMs covering 31 languages. It consists of locally sourced questions, human and machine translations, aiming to better assess the multilingual capabilities of language models.",
    "data_type": "question-answering pairs",
    "domains": [
      "Natural Language Processing"
    ],
    "languages": [
      "English",
      "Arabic",
      "Bengali",
      "Cantonese",
      "Czech",
      "Dutch",
      "Farsi",
      "French",
      "German",
      "Hebrew",
      "Hindi",
      "Indonesian",
      "Italian",
      "Japanese",
      "Khmer",
      "Korean",
      "Malay",
      "Mandarin (simplified)",
      "Mandarin (traditional)",
      "Marathi",
      "Polish",
      "Portuguese",
      "Romanian",
      "Russian",
      "Spanish",
      "Swedish",
      "Tagalog",
      "Thai",
      "Turkish",
      "Urdu",
      "Vietnamese"
    ],
    "similar_benchmarks": [
      "PAWS-X",
      "XNLI",
      "XCOPA"
    ],
    "resources": [
      "https://github.com/facebookresearch/multiloko/"
    ]
  },
  "purpose_and_intended_users": {
    "goal": "To evaluate the multilinguality of large language models and to study the effects of various design choices in multilingual evaluation.",
    "audience": [
      "ML Researchers",
      "Model Developers",
      "Industry Practitioners"
    ],
    "tasks": [
      "Question Answering"
    ],
    "limitations": "N/A",
    "out_of_scope_uses": []
  },
  "data": {
    "source": "Sourced from the 6K most visited Wikipedia pages for each language, with questions generated by native speakers.",
    "size": "15,500 parallel questions",
    "format": "JSON",
    "annotation": "Questions were generated and reviewed by native speakers, ensuring local relevance and quality."
  },
  "methodology": {
    "methods": [
      "Human evaluation",
      "Automated metrics"
    ],
    "metrics": [
      "Exact Match accuracy (EM)",
      "Gap (between best and worst performing language)",
      "Mother Tongue Effect (MTE)",
      "Locality Effect (LE)"
    ],
    "calculation": "EM is calculated based on the percentage of correct answers that match the reference. Gap is the difference between the best and worst performing languages. MTE and LE are also derived from the scores of different language conditions.",
    "interpretation": "Higher EM indicates better performance, while a smaller Gap indicates better parity across languages. Positive MTE indicates information is easier to access in the local language.",
    "baseline_results": null,
    "validation": "Data was split into a devset and a blind test split to assess generalization."
  },
  "targeted_risks": {
    "risk_categories": [
      "Bias",
      "Accuracy",
      "Fairness"
    ],
    "atlas_risks": {
      "risks": [
        {
          "category": "Fairness",
          "subcategory": [
            "Data bias"
          ]
        },
        {
          "category": "Accuracy",
          "subcategory": [
            "Unrepresentative data"
          ]
        }
      ]
    },
    "demographic_analysis": "N/A",
    "harm": []
  },
  "ethical_and_legal_considerations": {
    "privacy_and_anonymity": "N/A",
    "data_licensing": "N/A",
    "consent_procedures": "N/A",
    "compliance_with_regulations": "N/A"
  }
}