{
  "benchmark_details": {
    "is_benchmark": true,
    "name": "EQ-Bench",
    "abbreviation": "N/A",
    "overview": "EQ-Bench is a novel benchmark designed to evaluate aspects of emotional intelligence in Large Language Models (LLMs), assessing their ability to understand complex emotions and social interactions by rating the intensity of emotional states of characters in dialogues.",
    "data_type": "text",
    "domains": [
      "Natural Language Processing"
    ],
    "languages": [
      "English"
    ],
    "similar_benchmarks": [
      "MMLU (Massive Multitask Language Understanding)",
      "SECEU (Emotional Intelligence of Large Language Models)"
    ],
    "resources": [
      "https://github.com/EQ-bench/EQ-Bench",
      "https://eqbench.com"
    ]
  },
  "purpose_and_intended_users": {
    "goal": "To assess the emotional understanding capabilities of Large Language Models.",
    "audience": [
      "ML Researchers",
      "Industry Practitioners",
      "Model Developers"
    ],
    "tasks": [
      "Emotional Understanding"
    ],
    "limitations": "N/A",
    "out_of_scope_uses": []
  },
  "data": {
    "source": "A set of 60 questions designed by the authors based on dialogues generated by GPT-4.",
    "size": "60 questions",
    "format": "N/A",
    "annotation": "Answers were determined based on author consensus rather than crowd-sourced input."
  },
  "methodology": {
    "methods": [
      "Automated metrics"
    ],
    "metrics": [
      "Correlation with multi-domain benchmarks"
    ],
    "calculation": "Scores are calculated based on the differences between the model's ratings and reference answers, normalized to sum to 10.",
    "interpretation": "Scores are interpreted as reflecting the model's understanding of emotional nuances within dialogues.",
    "baseline_results": "EQ-Bench scores correlate strongly with other benchmarks like MMLU (r=0.97).",
    "validation": "Test results are validated by repeating the benchmark with multiple runs."
  },
  "targeted_risks": {
    "risk_categories": [
      "Bias",
      "Safety",
      "Robustness"
    ],
    "atlas_risks": {
      "risks": [
        {
          "category": "Fairness",
          "subcategory": [
            "Data bias"
          ]
        },
        {
          "category": "Accuracy",
          "subcategory": [
            "Unrepresentative data"
          ]
        }
      ]
    },
    "demographic_analysis": "N/A",
    "harm": []
  },
  "ethical_and_legal_considerations": {
    "privacy_and_anonymity": "N/A",
    "data_licensing": "MIT license for the benchmark code and questions.",
    "consent_procedures": "N/A",
    "compliance_with_regulations": "N/A"
  }
}