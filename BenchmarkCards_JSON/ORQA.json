{
  "benchmark_details": {
    "is_benchmark": true,
    "name": "ORQA (Operations Research Question Answering)",
    "abbreviation": "ORQA",
    "overview": "This benchmark evaluates the generalization capabilities of Large Language Models (LLMs) in the specialized technical domain of Operations Research (OR) by assessing their ability to emulate the knowledge and reasoning skills of OR experts in addressing complex optimization problems.",
    "data_type": "multi-choice question answering",
    "domains": [
      "Operations Research"
    ],
    "languages": [
      "English"
    ],
    "similar_benchmarks": [],
    "resources": [
      "https://github.com/nl4opt/ORQA"
    ]
  },
  "purpose_and_intended_users": {
    "goal": "To assess the reasoning capabilities and generalization abilities of LLMs in the context of Operations Research through a benchmark dataset.",
    "audience": [
      "ML Researchers",
      "Industry Practitioners",
      "Model Developers"
    ],
    "tasks": [
      "Question Answering"
    ],
    "limitations": "N/A",
    "out_of_scope_uses": []
  },
  "data": {
    "source": "The dataset contains real-world optimization problems crafted by Operations Research experts.",
    "size": "1,513 instances",
    "format": "JSON",
    "annotation": "Annotated by Operations Research experts with graduate-level education or extensive experience in optimization modeling."
  },
  "methodology": {
    "methods": [
      "Human evaluation",
      "Automated metrics"
    ],
    "metrics": [
      "Accuracy",
      "F1 Score"
    ],
    "calculation": "Metrics are calculated by comparing LLM-generated answers with the correct answers.",
    "interpretation": "Higher accuracy indicates better reasoning and understanding of the optimization models by the LLMs.",
    "baseline_results": "Human expert accuracy was reported at 93% on a random set of instances.",
    "validation": "The dataset includes a validation set of 45 instances used for in-context learning examples."
  },
  "targeted_risks": {
    "risk_categories": [
      "Bias",
      "Safety",
      "Privacy",
      "Robustness",
      "Fairness",
      "Accuracy"
    ],
    "atlas_risks": {
      "risks": []
    },
    "demographic_analysis": "N/A",
    "harm": "N/A"
  },
  "ethical_and_legal_considerations": {
    "privacy_and_anonymity": "N/A",
    "data_licensing": "N/A",
    "consent_procedures": "N/A",
    "compliance_with_regulations": "N/A"
  }
}