{
  "benchmark_details": {
    "is_benchmark": true,
    "name": "LexRAG (Benchmarking Retrieval-Augmented Generation in Multi-Turn Legal Consultation Conversation)",
    "abbreviation": "LexRAG",
    "overview": "LexRAG is the first benchmark specifically designed to evaluate Retrieval-Augmented Generation (RAG) in multi-turn legal consultations. It consists of 1,013 multi-turn dialogue samples and 17,228 candidate legal articles, with an emphasis on assessing conversational knowledge retrieval and response generation capabilities.",
    "data_type": "multi-turn dialogues",
    "domains": [
      "Legal"
    ],
    "languages": [
      "Chinese"
    ],
    "similar_benchmarks": [
      "LexGLUE"
    ],
    "resources": [
      "https://github.com/CSHaitao/LexRAG"
    ]
  },
  "purpose_and_intended_users": {
    "goal": "To provide a standardized platform for evaluating retrieval and generation capabilities in complex legal consultation conversations.",
    "audience": [
      "ML Researchers",
      "Legal Practitioners",
      "AI Developers"
    ],
    "tasks": [
      "Conversational Knowledge Retrieval",
      "Response Generation"
    ],
    "limitations": "LexRAG primarily focuses on Chinese legal scenarios, which limits its applicability in broader multilingual contexts.",
    "out_of_scope_uses": []
  },
  "data": {
    "source": "Legal statutes and real-world legal consultation platforms.",
    "size": "1,013 multi-turn dialogues",
    "format": "JSON",
    "annotation": "Annotated by legal experts."
  },
  "methodology": {
    "methods": [
      "Human evaluation",
      "Automated metrics"
    ],
    "metrics": [
      "Accuracy",
      "Recall",
      "NDCG",
      "ROUGE",
      "BLEU"
    ],
    "calculation": "Metrics calculated based on the relevance of responses and retrieval accuracy.",
    "interpretation": "Higher scores indicate better performance in generating accurate and relevant legal answers.",
    "baseline_results": null,
    "validation": "Rigorous evaluation through expert validation and comparison with existing benchmarks."
  },
  "targeted_risks": {
    "risk_categories": [
      "Fairness",
      "Accuracy"
    ],
    "atlas_risks": {
      "risks": []
    },
    "demographic_analysis": "Annotations conducted by a diverse team of legal experts.",
    "harm": "Potential for inaccurate legal information leading to misunderstandings."
  },
  "ethical_and_legal_considerations": {
    "privacy_and_anonymity": "N/A",
    "data_licensing": "N/A",
    "consent_procedures": "N/A",
    "compliance_with_regulations": "N/A"
  }
}