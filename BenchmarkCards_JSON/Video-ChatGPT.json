{
  "benchmark_details": {
    "is_benchmark": true,
    "name": "Video-ChatGPT",
    "abbreviation": "N/A",
    "overview": "Introduces Video-ChatGPT (a multimodal video conversation model) and (1) a new dataset of 100,000 video-instruction pairs acquired via a manual and semi-automatic pipeline, and (2) a quantitative evaluation framework for video-based dialogue models that evaluates capabilities such as correctness of information, detail orientation, contextual understanding, temporal understanding, and consistency.",
    "data_type": "video (video-instruction / question-answer pairs)",
    "domains": [
      "Computer Vision",
      "Natural Language Processing"
    ],
    "languages": null,
    "similar_benchmarks": [
      "ActivityNet-200",
      "MSRVTT-QA",
      "MSVD-QA",
      "TGIF-QA",
      "ActivityNet-QA",
      "Video Chat",
      "Video-LLaMA",
      "LLaMA Adapter"
    ],
    "resources": [
      "https://github.com/mbzuai-oryx/Video-ChatGPT",
      "https://github.com/keplerlab/katna",
      "https://openai.com/policies/terms-of-use",
      "https://chat.openai.com"
    ]
  },
  "purpose_and_intended_users": {
    "goal": "Create a multimodal video conversation model (Video-ChatGPT), provide a large video-instruction dataset of 100,000 pairs for training, and develop a quantitative video conversation evaluation framework for benchmarking video conversation models.",
    "audience": [],
    "tasks": [
      "Question Answering",
      "Text Generation (open-ended video descriptions)",
      "Video Conversation",
      "Temporal Reasoning",
      "Spatial Understanding",
      "Action Recognition",
      "Localization",
      "Object Detection",
      "Segmentation",
      "Retrieval",
      "Tracking"
    ],
    "limitations": "Model struggles to understand subtle temporal relationships in long videos (>2 minutes) and has difficulty recognizing details of small objects.",
    "out_of_scope_uses": []
  },
  "data": {
    "source": "Curated from a subset of the ActivityNet-200 dataset and other video-caption datasets; enriched using off-the-shelf models BLIP-2, GRiT, Tag2Text, and GPT-3.5 via a semi-automatic pipeline plus human-assisted annotation.",
    "size": "100,000 video-instruction pairs",
    "format": "N/A",
    "annotation": "Human-assisted annotation (expert annotators enrich captions) and semi-automatic annotation (frame-level captions from BLIP-2 and GRiT filtered with Tag2Text tags and merged; GPT-3.5 used to generate coherent video-level captions and question-answer pairs)."
  },
  "methodology": {
    "methods": [
      "Instruction tuning / fine-tuning of LLM on video-instruction pairs (linear adapter layer trained; rest frozen)",
      "Video-based Generative Performance Benchmarking (text-generation evaluation)",
      "Zero-Shot Question-Answer Evaluation on existing QA datasets",
      "Automated GPT-3.5-based evaluation pipeline (and replication with Vicuna-1.5 for accessibility checks)",
      "Ablation studies comparing human-only, automatic-only, and combined data"
    ],
    "metrics": [
      "Accuracy",
      "Relative score (1-5) for: Correctness of Information, Detail Orientation, Contextual Understanding, Temporal Understanding, Consistency"
    ],
    "calculation": "For generative evaluations, a GPT-3.5-based evaluation pipeline assigns a relative score from 1-5 to model outputs along five aspects (Correctness of Information, Detail Orientation, Contextual Understanding, Temporal Understanding, Consistency). For question-answer datasets (MSVD-QA, MSRVTT-QA, TGIF-QA, ActivityNet-QA), standard Accuracy scores are reported.",
    "interpretation": "Higher Accuracy indicates better QA performance. Higher relative scores (on a 1-5 scale) indicate better performance for each aspect (1 = poor, 5 = excellent). The paper interprets higher values as better performance; no additional numeric thresholds are provided.",
    "baseline_results": "Zero-shot QA Accuracy (Table 2): Video-ChatGPT: MSVD-QA 64.9, MSRVTT-QA 49.3, TGIF-QA 51.4, ActivityNet-QA 35.2. Generative aspect scores (Table 1): Video-ChatGPT - Correctness 2.40, Detail Orientation 2.52, Contextual Understanding 2.62, Temporal Understanding 1.98, Consistency 2.37. Comparative baselines include Video Chat, LLaMA Adapter, Video-LLaMA, and FrozenBiLM (specific scores reported in paper tables).",
    "validation": "Validated via: (1) Blind test distinguishing human vs semi-automatic annotations on 50 sampled videos (52% accuracy in distinguishing sources) to demonstrate consistency of semi-automatic pipeline; (2) Evaluation replication using open-source Vicuna-1.5 (13B) showing similar trends to GPT-3.5 evaluation; (3) Ablation studies showing combined human+automatic data yields best performance."
  },
  "targeted_risks": {
    "risk_categories": [
      "Bias",
      "Privacy",
      "Misuse",
      "Fairness",
      "Transparency"
    ],
    "atlas_risks": {
      "risks": null
    },
    "demographic_analysis": null,
    "harm": [
      "Residual bias may persist in the dataset and influence the model's understanding and responses.",
      "Potential for misuse if model is not handled with caution (explicitly stated)."
    ]
  },
  "ethical_and_legal_considerations": {
    "privacy_and_anonymity": "N/A",
    "data_licensing": "The subset of ActivityNet-200 used is distributed under MIT LICENSE; the authors state they will release all datasets created in this work under MIT LICENSE.",
    "consent_procedures": "N/A",
    "compliance_with_regulations": "N/A"
  }
}