{
  "benchmark_details": {
    "is_benchmark": true,
    "name": "Cama (Benchmarking framework for Code LLMs in Android Malware Analysis)",
    "abbreviation": "Cama",
    "overview": "Cama is a benchmarking framework designed to systematically evaluate the effectiveness of Code LLMs in Android malware analysis. It specifies structured model outputs to support key malware analysis tasks, including malicious function identification and malware purpose summarization, and integrates evaluation metrics of consistency, fidelity, and semantic relevance.",
    "data_type": "malware samples comprising over 7.5 million distinct functions",
    "domains": [
      "Cybersecurity"
    ],
    "languages": [
      "English"
    ],
    "similar_benchmarks": [],
    "resources": [
      "https://zenodo.org/records/15155917"
    ]
  },
  "purpose_and_intended_users": {
    "goal": "The primary objective of Cama is to provide a structured framework for the evaluation of Code LLMs in Android malware analysis, facilitating comparison of their performance across different tasks.",
    "audience": [
      "ML Researchers",
      "Cybersecurity Analysts"
    ],
    "tasks": [
      "Malicious Function Identification",
      "Malware Purpose Summarization"
    ],
    "limitations": "N/A",
    "out_of_scope_uses": []
  },
  "data": {
    "source": "Collected from various Android malware families using Androguard for decompilation.",
    "size": "118 Android malware samples covering over 7.5 million distinct functions",
    "format": "JSON",
    "annotation": "Manually inspected and labeled according to their maliciousness."
  },
  "methodology": {
    "methods": [
      "Human evaluation",
      "Automated metrics"
    ],
    "metrics": [
      "Consistency",
      "Fidelity",
      "Semantic Relevance (BLEU, METEOR, ROUGE-L)"
    ],
    "calculation": "Metrics are calculated based on the structured outputs generated by the Code LLMs, comparing them against ground truth labels.",
    "interpretation": "The framework allows for the evaluation of how well Code LLMs output structured representations relevant for malware analysis.",
    "baseline_results": "N/A",
    "validation": "The effectiveness of the framework and models is validated through benchmarking studies involving multiple Code LLMs."
  },
  "targeted_risks": {
    "risk_categories": [
      "Accuracy",
      "Fairness"
    ],
    "atlas_risks": {
      "risks": [
        {
          "category": "Accuracy",
          "subcategory": [
            "Poor model accuracy"
          ]
        },
        {
          "category": "Fairness",
          "subcategory": [
            "Data bias"
          ]
        }
      ]
    },
    "demographic_analysis": "N/A",
    "harm": []
  },
  "ethical_and_legal_considerations": {
    "privacy_and_anonymity": "N/A",
    "data_licensing": "N/A",
    "consent_procedures": "N/A",
    "compliance_with_regulations": "N/A"
  }
}