{
  "benchmark_details": {
    "name": "CONFAIDE",
    "overview": "A benchmark designed to test the privacy implications of language models through the lens of contextual integrity theory. It evaluates the ability of models to appropriately manage the flow of sensitive information in interactive settings.",
    "data_type": "N/A",
    "domains": [
      "privacy",
      "social reasoning",
      "language models"
    ],
    "languages": null,
    "similar_benchmarks": null,
    "resources": [
      "https://confaide.github.io"
    ]
  },
  "purpose_and_intended_users": {
    "goal": "To evaluate the contextual privacy reasoning capabilities of language models.",
    "audience": [
      "researchers",
      "benchmark developers",
      "AI ethics practitioners"
    ],
    "tasks": [
      "assess privacy reasoning capabilities",
      "evaluate contextual integrity"
    ],
    "limitations": "N/A",
    "out_of_scope_uses": null
  },
  "data": {
    "source": "Madden, 2014; Martin & Nissenbaum, 2016; GPT-4 generated text",
    "size": "N/A",
    "format": "N/A",
    "annotation": "Human annotations were collected using Amazon Mechanical Turk."
  },
  "methodology": {
    "methods": [
      "Information type sensitivity assessment",
      "Vignette surveys based on contextual factors",
      "Scenario generation involving theory of mind"
    ],
    "metrics": [
      "Sensitivity Score",
      "Leakage Rate",
      "Binary Control Question Error Rate",
      "Omitting Public Information Rate"
    ],
    "calculation": "Measures are calculated based on model responses and compared against human annotations.",
    "interpretation": "Higher sensitivity scores indicate less willingness to share information; lower leakage rates indicate better privacy preservation.",
    "baseline_results": null,
    "validation": "Results validated through comparison with human judgments."
  },
  "targeted_risks": {
    "risk_categories": [
      "Privacy breaches",
      "Misuse of information",
      "Lack of appropriate contextual responses"
    ],
    "atlas_risks": {
      "risks": [
        {
          "category": "Privacy",
          "subcategory": [
            "Personal information in data",
            "Data privacy rights alignment",
            "Reidentification"
          ]
        },
        {
          "category": "Robustness",
          "subcategory": [
            "Prompt leaking",
            "Data poisoning"
          ]
        }
      ]
    },
    "demographic_analysis": null,
    "harm": [
      "Privacy violations in sensitive contexts",
      "Exposing personal information",
      "Inappropriately managing information flow"
    ]
  },
  "ethical_and_legal_considerations": {
    "privacy_and_anonymity": "The benchmark assesses privacy expectations based on contextual integrity.",
    "data_licensing": "N/A",
    "consent_procedures": "N/A",
    "compliance_with_regulations": "The study is exempt under US federal regulation 45 CFR 46."
  }
}