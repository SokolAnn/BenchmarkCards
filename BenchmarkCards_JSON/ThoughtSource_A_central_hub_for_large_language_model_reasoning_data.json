{
  "benchmark_details": {
    "is_benchmark": true,
    "name": "ThoughtSource: A central hub for large language model reasoning data",
    "abbreviation": "N/A",
    "overview": "ThoughtSource is a meta-dataset and software library for chain-of-thought (CoT) reasoning. The goal of ThoughtSource is to improve future artificial intelligence systems by facilitating qualitative understanding of CoTs, enabling empirical evaluations, and providing training data. This first release integrates seven scientific/medical, three general-domain and five math word question answering datasets.",
    "data_type": "question-answering pairs (text with chain-of-thought explanations)",
    "domains": [
      "Natural Language Processing",
      "Healthcare",
      "Mathematics",
      "Education"
    ],
    "languages": [
      "English"
    ],
    "similar_benchmarks": [
      "SuperGLUE",
      "BigBIO",
      "BLURB",
      "Pile",
      "ROOTS",
      "PromptSource"
    ],
    "resources": [
      "https://github.com/OpenBioLink/ThoughtSource",
      "https://doi.org/10.5281/zenodo.8155593",
      "https://github.com/hwchase17/langchain/"
    ]
  },
  "purpose_and_intended_users": {
    "goal": "Facilitate qualitative understanding of CoTs generated by LLMs under various conditions, enable empirical and quantitative evaluation, and provide a library of diverse CoT training data for improving performance, robustness, explainability and value-alignment of future LLM-based AI systems.",
    "audience": [],
    "tasks": [
      "Question Answering",
      "Natural Language Inference",
      "Chain-of-Thought Generation"
    ],
    "limitations": "AI-generated CoTs may contain faulty reasoning. Reference CoTs are not provided for some datasets (e.g., MedQA and PubmedQA for this release; reference CoTs are also not provided for MMLU medical subsets).",
    "out_of_scope_uses": []
  },
  "data": {
    "source": "Integrates the following datasets (as listed in the paper): WorldTree V2; EntailmentBank; OpenBookQA; MedQA (USMLE); Open-ended questions (converted variants); MedMCQA; PubmedQA; MMLU (medical subsets); CommonsenseQA; StrategyQA; QED; AQUA-RAT; ASDiv; GSM8K; MAWPS; SVAMP.",
    "size": "Per-dataset example counts (from Table 3): AQUA-RAT 97,975 examples; ASDiv 1,218 examples; CommonsenseQA 12,102 examples; EntailmentBank 1,840 examples; GSM8K 8,792 examples; MAWPS 1,921 examples; MedQA (USMLE) 12,723 examples; MedMCQA 193,155 examples; MMLU (medical) 1,242 examples; OpenBookQA 5,957 examples; PubmedQA 1,000 examples; QED 6,175 examples; StrategyQA 2,780 examples; SVAMP 1,000 examples; WorldTree V2 4,367 examples.",
    "format": "JSON following the ThoughtSource schema (sample, generated_cot, answer, annotation objects); source dataset schemas preserved for source-format data; data loaders compatible with the Hugging Face datasets library.",
    "annotation": "Reference CoTs originate from human expert rationales or crowdsourced rationales depending on the dataset; AI-generated CoTs were produced by calling LLM APIs. Annotations can be added programmatically or via the provided web-based Annotator tool."
  },
  "methodology": {
    "methods": [
      "Dataset conversion and preprocessing into a standardized ThoughtSource schema",
      "Model-based generation of CoTs via external LLM APIs",
      "Automated evaluation (answer extraction and accuracy calculation)",
      "Human annotation and inspection via the web-based Annotator"
    ],
    "metrics": [
      "Accuracy",
      "Szymkiewicz–Simpson coefficient (n-gram overlap, n=3)"
    ],
    "calculation": "Accuracy is computed by extracting the model-generated answer and comparing it to the gold answer (correct_answer boolean). Overlap between datasets is quantified using the Szymkiewicz–Simpson coefficient computed on mutual n-gram overlap with n=3.",
    "interpretation": "Accuracy is interpreted as the proportion of extracted answers equal to gold answers. The Szymkiewicz–Simpson overlap coefficient is interpreted as the proportion of n-grams of the smaller dataset that are contained in the larger dataset.",
    "baseline_results": null,
    "validation": "Datasets were reviewed by three team members; issues were tracked on the GitHub issue tracker. Overlap analyses (n-gram overlap) and distribution analyses (question and CoT length) were performed as technical validation."
  },
  "targeted_risks": {
    "risk_categories": [
      "Accuracy",
      "Explainability",
      "Robustness",
      "Fairness",
      "Value Alignment"
    ],
    "atlas_risks": {
      "risks": [
        {
          "category": "Accuracy",
          "subcategory": [
            "Poor model accuracy"
          ]
        },
        {
          "category": "Explainability",
          "subcategory": [
            "Unexplainable output"
          ]
        },
        {
          "category": "Robustness",
          "subcategory": [
            "Hallucination"
          ]
        },
        {
          "category": "Fairness",
          "subcategory": [
            "Data bias"
          ]
        }
      ]
    },
    "demographic_analysis": null,
    "harm": "The benchmark aims to help detect and mitigate issues that can lead to critical failures and biases when LLMs are deployed in practice."
  },
  "ethical_and_legal_considerations": {
    "privacy_and_anonymity": "N/A",
    "data_licensing": "Code and ThoughtSource data are licensed under an MIT license. Data adapted from existing datasets are available under the licenses of their respective sources. The ThoughtSource-33 generated CoTs subset is noted as licensed MIT.",
    "consent_procedures": "N/A",
    "compliance_with_regulations": "N/A"
  }
}