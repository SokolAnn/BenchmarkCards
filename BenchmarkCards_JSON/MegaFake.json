{
  "benchmark_details": {
    "is_benchmark": true,
    "name": "MegaFake",
    "abbreviation": "N/A",
    "overview": "The MegaFake dataset is a comprehensive dataset of fake news generated by large language models, developed using a novel theoretical framework known as LLM-Fake Theory. It comprises 46,096 instances of fake news and 17,871 instances of legitimate news, providing valuable resources for research in the detection and governance of fake news generated by LLMs.",
    "data_type": "text",
    "domains": [
      "Natural Language Processing"
    ],
    "languages": [
      "English"
    ],
    "similar_benchmarks": [
      "FakeNewsNet"
    ],
    "resources": [
      "https://arxiv.org/abs/2408.11871"
    ]
  },
  "purpose_and_intended_users": {
    "goal": "To advance research in the detection of LLM-generated fake news and understand the mechanisms behind its creation.",
    "audience": [
      "ML Researchers",
      "Industry Practitioners",
      "Model Developers",
      "Domain Experts"
    ],
    "tasks": [
      "Fake News Detection",
      "Text Classification"
    ],
    "limitations": "The dataset is constrained as it solely relies on the GossipCop dataset and currently supports only binary classification for fake or legitimate news.",
    "out_of_scope_uses": []
  },
  "data": {
    "source": "The MegaFake dataset was built upon the GossipCop dataset from FakeNewsNet, which includes human-generated fake and legitimate news articles as a foundation for generating LLM-generated content.",
    "size": "63,967 instances",
    "format": "JSON",
    "annotation": "Automatically generated by large language models using a novel generation pipeline informed by social psychology theory."
  },
  "methodology": {
    "methods": [
      "Human evaluation",
      "Automated metrics"
    ],
    "metrics": [
      "Accuracy",
      "F1 Score",
      "Precision",
      "Recall"
    ],
    "calculation": "Metrics are calculated based on the performance of various NLG and NLU models tasked with classifying the categories of news in the dataset.",
    "interpretation": "Higher scores indicate better model performance at identifying the true nature of news articles as legitimate or fake.",
    "baseline_results": null,
    "validation": "Cross-experiments were conducted to assess the robustness of the dataset against existing benchmarks."
  },
  "targeted_risks": {
    "risk_categories": [
      "Bias",
      "Safety",
      "Accuracy"
    ],
    "atlas_risks": {
      "risks": [
        {
          "category": "Fairness",
          "subcategory": [
            "Data bias"
          ]
        },
        {
          "category": "Accuracy",
          "subcategory": [
            "Unrepresentative data"
          ]
        }
      ]
    },
    "demographic_analysis": "The dataset does not include demographic breakdowns or fairness across groups, as it focuses on LLM-generated content.",
    "harm": "The dataset involves the generation of potentially harmful misinformation, necessitating careful consideration of its use."
  },
  "ethical_and_legal_considerations": {
    "privacy_and_anonymity": "N/A",
    "data_licensing": "N/A",
    "consent_procedures": "N/A",
    "compliance_with_regulations": "N/A"
  }
}