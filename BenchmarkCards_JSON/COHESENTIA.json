{
  "benchmark_details": {
    "is_benchmark": true,
    "name": "COHESENTIA: A Novel Benchmark of Incremental versus Holistic Assessment of Coherence in Generated Texts",
    "abbreviation": "COHESENTIA",
    "overview": "COHESENTIA is a new benchmark for human assessment of the coherence of automatically generated texts, where text coherence is evaluated through both holistic and incremental scoring methods, allowing for insights into factors contributing to coherence.",
    "data_type": "text",
    "domains": [
      "Natural Language Processing"
    ],
    "languages": [
      "English"
    ],
    "similar_benchmarks": [
      "GCDC (Grocery Coherence Dataset)"
    ],
    "resources": [
      "https://github.com/AviyaMn/CoheSentiaprotocol"
    ]
  },
  "purpose_and_intended_users": {
    "goal": "To provide a benchmark for automatic coherence assessment of generated texts and explore factors affecting coherence.",
    "audience": [
      "ML Researchers",
      "Model Developers"
    ],
    "tasks": [
      "Coherence Scoring",
      "Coherence Reasoning"
    ],
    "limitations": "The benchmark is composed of stories generated from GPT-3 and may not generalize to other models or languages.",
    "out_of_scope_uses": []
  },
  "data": {
    "source": "500 human-annotated paragraphs generated by GPT-3",
    "size": "500 paragraphs",
    "format": "JSON",
    "annotation": "Human annotations via Amazon Mechanical Turk"
  },
  "methodology": {
    "methods": [
      "Human evaluation",
      "Automated metrics"
    ],
    "metrics": [
      "F1 Score",
      "Accuracy"
    ],
    "calculation": "Coherence scores were averaged from multiple raters and clustered into defined groups.",
    "interpretation": "A higher score indicates better coherence as rated by human annotators.",
    "baseline_results": null,
    "validation": "Inter-annotator agreement was assessed to ensure reliability of the scoring methods."
  },
  "targeted_risks": {
    "risk_categories": [
      "Accuracy",
      "Fairness"
    ],
    "atlas_risks": {
      "risks": [
        {
          "category": "Accuracy",
          "subcategory": [
            "Unrepresentative data"
          ]
        },
        {
          "category": "Fairness",
          "subcategory": [
            "Data bias"
          ]
        }
      ]
    },
    "demographic_analysis": null,
    "harm": []
  },
  "ethical_and_legal_considerations": {
    "privacy_and_anonymity": "The generated texts were created without checking for potential harmful content, and annotators were chosen without biases.",
    "data_licensing": "N/A",
    "consent_procedures": "N/A",
    "compliance_with_regulations": "N/A"
  }
}