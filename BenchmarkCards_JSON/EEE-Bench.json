{
  "benchmark_details": {
    "is_benchmark": true,
    "name": "EEE-Bench (Electrical and Electronics Engineering Benchmark)",
    "abbreviation": "EEE-Bench",
    "overview": "EEE-Bench is a pioneering multimodal benchmark designed for assessing the reasoning abilities of Large Multimodal Models (LMMs) in electrical and electronics engineering (EEE) problems. It consists of 2860 samples spanning 10 essential subjects, featuring a diverse range of visual contexts such as electric and digital circuits, system diagrams, and more.",
    "data_type": "multiple-choice and free-form questions",
    "domains": [
      "Natural Language Processing",
      "Computer Vision"
    ],
    "languages": [
      "English"
    ],
    "similar_benchmarks": [],
    "resources": [
      "https://arxiv.org/abs/2411.01492"
    ]
  },
  "purpose_and_intended_users": {
    "goal": "To evaluate the abilities of LLMs and LMMs in their capability to tackle rigorous engineering reasoning tasks and to highlight limitations of current foundation models in solving EEE problems.",
    "audience": [
      "ML Researchers",
      "Industry Practitioners",
      "Model Developers",
      "Domain Experts"
    ],
    "tasks": [
      "Problem Solving",
      "Reasoning Evaluation"
    ],
    "limitations": "N/A",
    "out_of_scope_uses": []
  },
  "data": {
    "source": "Collection of problems from official EEE exams and verified online resources.",
    "size": "2,860 questions",
    "format": "JSON",
    "annotation": "Manual curation and selection of high-quality questions with visual aspects."
  },
  "methodology": {
    "methods": [
      "Automated metrics",
      "Human evaluations"
    ],
    "metrics": [
      "Accuracy"
    ],
    "calculation": "The accuracy of models was calculated as the proportion of correctly answered questions out of the total.",
    "interpretation": "An accuracy of 50% or above indicates satisfactory performance, while anything below suggests significant room for improvement.",
    "baseline_results": null,
    "validation": "Extensive experiments carried out using various models to cross-validate results."
  },
  "targeted_risks": {
    "risk_categories": [
      "Accuracy",
      "Bias",
      "Fairness",
      "Robustness"
    ],
    "atlas_risks": {
      "risks": [
        {
          "category": "Accuracy",
          "subcategory": [
            "Unrepresentative data"
          ]
        },
        {
          "category": "Fairness",
          "subcategory": [
            "Data bias"
          ]
        },
        {
          "category": "Robustness",
          "subcategory": [
            "Prompt injection attack"
          ]
        }
      ]
    },
    "demographic_analysis": "N/A",
    "harm": "N/A"
  },
  "ethical_and_legal_considerations": {
    "privacy_and_anonymity": "N/A",
    "data_licensing": "N/A",
    "consent_procedures": "N/A",
    "compliance_with_regulations": "N/A"
  }
}