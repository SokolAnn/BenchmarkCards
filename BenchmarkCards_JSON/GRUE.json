{
  "benchmark_details": {
    "is_benchmark": true,
    "name": "GRUE (General Reinforced-language Understanding Evaluation)",
    "abbreviation": "GRUE",
    "overview": "GRUE (General Reinforced-language Understanding Evaluation) benchmark, a set of 6 language generation tasks which are supervised not by target strings, but by reward functions which capture automated measures of human preference. GRUE is the first leaderboard-style evaluation of RL algorithms for NLP tasks.",
    "data_type": "text (generative text: continuations, summaries, translations, data-to-text, QA answers, dialogue responses)",
    "domains": [
      "Natural Language Processing"
    ],
    "languages": [
      "English"
    ],
    "similar_benchmarks": [],
    "resources": [
      "https://github.com/allenai/RL4LMs",
      "https://rl4lms.apps.allenai.org/",
      "https://arxiv.org/abs/2210.01241"
    ]
  },
  "purpose_and_intended_users": {
    "goal": "Align pre-trained large language models (LMs) with human preferences by evaluating RL algorithms for LM-based generation using reward functions that capture automated measures of human preference.",
    "audience": [
      "ML Researchers",
      "Research Community"
    ],
    "tasks": [
      "Text Continuation",
      "Summarization",
      "Data-to-Text Generation",
      "Machine Translation",
      "Question Answering (abstractive)",
      "Dialogue Generation",
      "Generative Commonsense"
    ],
    "limitations": "N/A",
    "out_of_scope_uses": []
  },
  "data": {
    "source": "IMDB (Maas et al., 2011); CommonGen (Lin et al., 2020); CNN/Daily Mail (Hermann et al., 2015); ToTTo (Parikh et al., 2020); WMT-16 (en-de) (Bojar et al., 2016); NarrativeQA (Kočiskỳ et al., 2018); DailyDialog (Li et al., 2017).",
    "size": "IMDB: 25,000 training, 5,000 validation, 5,000 test; CNN/Daily Mail: 287,000 training, 13,000 validation, 11,000 test; other dataset sizes: N/A (not specified in paper).",
    "format": "N/A",
    "annotation": "Human judgments collected via Amazon Mechanical Turk for five tasks (3 unique human raters per sample); a DistilBERT classifier trained on sentiment labels for IMDB used as a reward model; pairwise human preference data collected for Commongen to train a T5-11B preference model (train/val/test splits described in Appendix)."
  },
  "methodology": {
    "methods": [
      "Automated metrics",
      "Human evaluation (crowdsourced Likert ratings and pairwise preference collection)",
      "Leaderboard-style evaluation"
    ],
    "metrics": [
      "Perplexity",
      "Learned Sentiment Classifier (sentiment score)",
      "CIDER",
      "ROUGE-1",
      "ROUGE-2",
      "ROUGE-L",
      "BLEU (n=3, n=4)",
      "METEOR",
      "Coverage",
      "SPICE",
      "SummaCZS",
      "SacreBLEU",
      "PARENT",
      "BLEURT",
      "TER",
      "chrF",
      "BertScore",
      "Mean Segmented Type Token Ratio (MSSTR)",
      "Shannon entropy (unigrams and bigrams)",
      "Distinct-1",
      "Distinct-2",
      "Krippendorf's alpha",
      "ROC AUC"
    ],
    "calculation": "Each task is evaluated at test time using a task-specific mix of metrics (task preference metrics and naturalness metrics). During training models often optimize a single reward function; test-time scores are averaged across the task-specific metric mix. Results are averaged over multiple random seeds (exact counts in Appendix B).",
    "interpretation": "Higher task-metric values indicate better task-specific performance; naturalness is assessed via perplexity and human Likert fluency judgments. The paper emphasizes balancing reward optimization with a KL-based naturalness penalty (removing KL penalty yields higher reward but worse perplexity / naturalness). NLPO is presented as better balancing reward and naturalness than prior policy-gradient methods.",
    "baseline_results": "IMDB (sentiment / perplexity): Zero-shot 0.489 / 32.171; Supervised 0.539 / 35.472; PPO 0.602 / 33.816; NLPO 0.611 / 33.832; Supervised+PPO 0.626 / 35.049; Supervised+NLPO 0.620 / 34.816. Additionally: a 220 million parameter model trained with supervision+NLPO outperforms a 3 billion parameter supervised model (reported in paper).",
    "validation": "Validation follows original dataset train/val/test splits. Human validation: crowd-sourced human judgments (3 annotators per sample), inter-annotator agreement measured via Krippendorf's alpha, statistical testing with one-way ANOVA and post-hoc Tukey HSD to assess significance; results averaged over multiple seeds; further experimental details and exact counts in Appendix B."
  },
  "targeted_risks": {
    "risk_categories": [
      "Robustness",
      "Accuracy",
      "Governance"
    ],
    "atlas_risks": {
      "risks": [
        {
          "category": "Robustness",
          "subcategory": [
            "Hallucination"
          ]
        },
        {
          "category": "Accuracy",
          "subcategory": [
            "Unrepresentative data"
          ]
        },
        {
          "category": "Governance",
          "subcategory": [
            "Lack of testing diversity"
          ]
        }
      ]
    },
    "demographic_analysis": "Human evaluation annotators for qualification were recruited from Australia, Canada, New Zealand, Great Britain, and the United States (qualification round); further annotator qualification and per-task details are provided in Appendix B.",
    "harm": [
      "Detecting and preventing reward hacking (nonsense outputs that exploit automated metrics)",
      "Detecting and reducing factual inconsistency / hallucination in generated outputs (e.g., summarization)"
    ]
  },
  "ethical_and_legal_considerations": {
    "privacy_and_anonymity": "N/A",
    "data_licensing": "N/A",
    "consent_procedures": "N/A",
    "compliance_with_regulations": "N/A"
  }
}