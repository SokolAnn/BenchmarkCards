{
  "benchmark_details": {
    "is_benchmark": true,
    "name": "Red Teaming Multimodal Language Models: Evaluating Harm Across Prompt Modalities and Models",
    "abbreviation": "N/A",
    "overview": "This paper introduces a novel adversarial benchmarking dataset including 726 prompts (half text-only, half multimodal) authored by 26 expert red teamers. The study evaluates the responses of four commercial MLLMs using human ratings from 17 annotators across 2,904 outputs, yielding over 47,000 annotations.",
    "data_type": "adversarial prompts",
    "domains": [
      "Natural Language Processing"
    ],
    "languages": [
      "English"
    ],
    "similar_benchmarks": [
      "RealToxicityPrompts",
      "MultiStAR"
    ],
    "resources": [
      "https://doi.org/10.48550/arXiv.2410.07073",
      "https://www.anthropic.com/news/claude-3-5-sonnet",
      "https://doi.org/10.48550/arXiv.2308.12966"
    ]
  },
  "purpose_and_intended_users": {
    "goal": "To systematically evaluate the robustness of four leading multimodal large language models (MLLMs) to adversarial prompts across text-only and multimodal formats.",
    "audience": [
      "ML Researchers",
      "Safety Engineers",
      "AI Developers"
    ],
    "tasks": [
      "Adversarial Text Generation"
    ],
    "limitations": "The adversarial prompt pool targeted only three harm categories (illegal activity, disinformation, and unethical behaviour). Language and cultural context may influence harm judgments.",
    "out_of_scope_uses": []
  },
  "data": {
    "source": "Generated by a team of 26 red teamers using established scenarios and guidelines.",
    "size": "726 prompts",
    "format": "N/A",
    "annotation": "Rated by 17 trained annotators on a 5-point scale of harmfulness."
  },
  "methodology": {
    "methods": [
      "Human evaluation",
      "Statistical analysis"
    ],
    "metrics": [
      "Attack Success Rate (ASR)",
      "Harmfulness ratings"
    ],
    "calculation": "Calculated using generalized linear mixed models (GLMMs) and ratings from annotators.",
    "interpretation": "Higher ASR indicates a greater vulnerability to adversarial prompts.",
    "baseline_results": "Pixtral 12B showed a 62% ASR, while Claude Sonnet 3.5 showed a 10% ASR.",
    "validation": "Validated through statistical analysis of ASR and annotator reliability."
  },
  "targeted_risks": {
    "risk_categories": [
      "Safety",
      "Robustness"
    ],
    "atlas_risks": {
      "risks": [
        {
          "category": "Robustness",
          "subcategory": [
            "Evasion attack"
          ]
        },
        {
          "category": "Fairness",
          "subcategory": [
            "Output bias"
          ]
        }
      ]
    },
    "demographic_analysis": "N/A",
    "harm": [
      "Harmfulness from unsafe model outputs"
    ]
  },
  "ethical_and_legal_considerations": {
    "privacy_and_anonymity": "All outputs were contained within secure research environments and not released publicly.",
    "data_licensing": "N/A",
    "consent_procedures": "N/A",
    "compliance_with_regulations": "N/A"
  }
}