{
  "benchmark_details": {
    "is_benchmark": true,
    "name": "Greatest Good Benchmark (GGB)",
    "abbreviation": "GGB",
    "overview": "The GGB is a novel framework designed to evaluate the moral judgments of Large Language Models (LLMs) by adapting the Oxford Utilitarianism Scale (OUS) and incorporates an extended dataset that is ten times larger than the original. It allows for direct comparison between the moral preferences of LLMs and humans.",
    "data_type": "utilitarian moral dilemmas",
    "domains": [
      "Natural Language Processing"
    ],
    "languages": [
      "English"
    ],
    "similar_benchmarks": [],
    "resources": [
      "https://github.com/noehsueh/greatest-good-benchmarkevaluation"
    ]
  },
  "purpose_and_intended_users": {
    "goal": "To evaluate LLMs' moral judgments using utilitarian dilemmas and provide insights into their alignment with human moral values.",
    "audience": [
      "ML Researchers",
      "Model Developers",
      "Ethics Researchers"
    ],
    "tasks": [
      "Moral Reasoning Evaluation"
    ],
    "limitations": "The benchmark primarily uses English and may not be representative of all languages or populations.",
    "out_of_scope_uses": []
  },
  "data": {
    "source": "Synthetic dataset derived from the Oxford Utilitarianism Scale (OUS), expanded with expert involvement.",
    "size": "5940 prompts generated across multiple variations and statements",
    "format": "JSON",
    "annotation": "Expert evaluation and synthetic creation of moral dilemma statements."
  },
  "methodology": {
    "methods": [
      "Human evaluation",
      "Automated modeling tests"
    ],
    "metrics": [
      "Statistical significance tests",
      "Effect sizes"
    ],
    "calculation": "Responses averaged across multiple variations of moral dilemma prompts to gauge consistency.",
    "interpretation": "Responses are interpreted based on mean values in comparison to human lay population standards.",
    "baseline_results": null,
    "validation": "Models were evaluated based on consistency of moral preferences across multiple prompts and iterations."
  },
  "targeted_risks": {
    "risk_categories": [
      "Bias",
      "Fairness"
    ],
    "atlas_risks": {
      "risks": []
    },
    "demographic_analysis": "Future research could include analysis across different demographics and languages.",
    "harm": "Potentially biased moral judgments reflected in model training data."
  },
  "ethical_and_legal_considerations": {
    "privacy_and_anonymity": "No personal data from individuals were used; scenarios analyzed are fictional.",
    "data_licensing": "N/A",
    "consent_procedures": "N/A",
    "compliance_with_regulations": "N/A"
  }
}