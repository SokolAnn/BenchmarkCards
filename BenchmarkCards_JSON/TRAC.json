{
  "benchmark_details": {
    "is_benchmark": true,
    "name": "TRAC (Textual Reasoning about Actions and Change)",
    "abbreviation": "TRAC",
    "overview": "We propose four essential RAC tasks as a comprehensive textual benchmark and generate problems in a way that minimizes the influence of other linguistic requirements (e.g., grounding) to focus on RAC. The resulting benchmark, TRAC, encompassing problems of various complexities, facilitates a more granular evaluation of LMs, precisely targeting the structural generalization ability much needed for RAC.",
    "data_type": "text classification pairs (context-query pairs with true/false labels)",
    "domains": [
      "Natural Language Processing",
      "Artificial Intelligence"
    ],
    "languages": [
      "English"
    ],
    "similar_benchmarks": [
      "SQuAD",
      "Winograd Schema Challenge",
      "bAbI",
      "ProPara",
      "SuperGLUE"
    ],
    "resources": [
      "https://arxiv.org/abs/2211.13930"
    ]
  },
  "purpose_and_intended_users": {
    "goal": "To provide a comprehensive textual benchmark of four RAC tasks (Projection, Executability, Planning, Goal-Recognition) to evaluate language models' ability to reason about actions and change and to test structural generalization.",
    "audience": [],
    "tasks": [
      "Projection",
      "Executability",
      "Planning",
      "Goal-Recognition"
    ],
    "limitations": "Action domains are limited to deterministic and noise-free settings; goals are limited to either a literal (an atom or its negation) or a conjunction of two; the provided proof-of-concept uses the blocks world domain which is simplistic (one kind of object, three types of actions, three predicates).",
    "out_of_scope_uses": [
      "Grounding and language variance are avoided and thus out-of-scope for this benchmark (clean-room evaluation)."
    ]
  },
  "data": {
    "source": "Synthetic datasets generated by a framework: symbolic problems generated from a STRIPS-based blocks world action domain and then translated into textual form in English via handcrafted templates.",
    "size": "Twelve primary datasets (four tasks × three action-length variants), each dataset contains 15,000 label-balanced examples (12,000 training examples, of which 2,000 are used as a dev set, and 3,000 testing examples). Additionally, twenty more datasets were created for generalization experiments.",
    "format": "N/A",
    "annotation": "Automatically generated labels via symbolic generation following STRIPS semantics (queries labeled true/false based on symbolic execution); datasets are label-balanced."
  },
  "methodology": {
    "methods": [
      "Automated metrics",
      "Model-based evaluation (baseline transformer models: RoBERTa, GPT-2, T5)"
    ],
    "metrics": [
      "Accuracy",
      "Mean and standard deviation"
    ],
    "calculation": "Models are trained to predict the truth of the query using cross-entropy loss; evaluation reports Accuracy (percentage) and the mean and standard deviation over five repeated runs with different seeds.",
    "interpretation": "Accuracy is used to evaluate model performance; the paper reports that baselines require at least 3,000 training samples to reach acceptable accuracies (above 80%) on standard datasets; higher accuracy indicates better ability to solve TRAC tasks and generalize structurally.",
    "baseline_results": "Baseline mean accuracies (percent) from Table 2: RoBERTa — Projection 87.36 (std 0.0396), Executability 99.73 (std 0.0013), Planning 87.63 (std 0.0158), Goal-Recognition 96.82 (std 0.0044). GPT-2 — Projection 85.13 (std 0.0336), Executability 99.37 (std 0.0037), Planning 90.09 (std 0.0157), Goal-Recognition 97.44 (std 0.0021). T5 — Projection 82.99 (std 0.0227), Executability 98.83 (std 0.0024), Planning 87.73 (std 0.0110), Goal-Recognition 94.04 (std 0.0082).",
    "validation": "Each dataset split into 12,000 training examples (including 2,000 dev) and 3,000 test examples; experiments repeated five times to compute mean and standard deviation; additional out-of-distribution generalization evaluations (GE1-GE4) were used to validate structural generalization."
  },
  "targeted_risks": {
    "risk_categories": [],
    "atlas_risks": {
      "risks": null
    },
    "demographic_analysis": null,
    "harm": null
  },
  "ethical_and_legal_considerations": {
    "privacy_and_anonymity": "N/A",
    "data_licensing": "Datasets and code released under the CRAPL license (the Community Research and Academic Programming License).",
    "consent_procedures": "N/A",
    "compliance_with_regulations": "N/A"
  }
}