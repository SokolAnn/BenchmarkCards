{
  "benchmark_details": {
    "is_benchmark": true,
    "name": "ExpliCa",
    "abbreviation": "N/A",
    "overview": "ExpliCa is a dataset designed to evaluate Large Language Models (LLMs) on explicit causal reasoning through pairwise causal discovery (PCD) tasks. It comprises sentence pairs annotated with causal and temporal relations expressed via linguistic connectives, along with human acceptability ratings for each connective.",
    "data_type": "sentence pairs with causal and temporal relations",
    "domains": [
      "Natural Language Processing"
    ],
    "languages": [
      "English"
    ],
    "similar_benchmarks": [
      "COPA",
      "Causal TimeBank",
      "CaTeRS",
      "Event Story Line",
      "BeCause Corpus"
    ],
    "resources": [
      "https://github.com/Unipisa/explica"
    ]
  },
  "purpose_and_intended_users": {
    "goal": "To evaluate LLMs' capabilities in commonsense causal reasoning through explicit causal and temporal relations.",
    "audience": [
      "ML Researchers",
      "Natural Language Processing Practitioners",
      "Model Developers"
    ],
    "tasks": [
      "Pairwise Causal Discovery",
      "Acceptability Rating",
      "Cloze Test",
      "Multiple-Choice Task"
    ],
    "limitations": "The dataset design excludes certain cues, focusing evaluation on deeper understanding, rather than superficial linguistic correlations.",
    "out_of_scope_uses": []
  },
  "data": {
    "source": "Annotated by native English speakers via crowdsourcing.",
    "size": "4,800 sentence pairs",
    "format": "N/A",
    "annotation": "Crowdsourced human acceptability ratings"
  },
  "methodology": {
    "methods": [
      "Human evaluation",
      "Prompting tasks",
      "Perplexity evaluation"
    ],
    "metrics": [
      "Accuracy",
      "Perplexity"
    ],
    "calculation": "Accuracy is calculated based on model responses compared to human acceptability ratings.",
    "interpretation": "Higher accuracy indicates better causal reasoning capabilities among models.",
    "baseline_results": "Top-performing LLMs struggled to reach 0.80 accuracy.",
    "validation": "The dataset was validated to ensure uniform statistical relations across categories."
  },
  "targeted_risks": {
    "risk_categories": [
      "Bias",
      "Fairness",
      "Accuracy"
    ],
    "atlas_risks": {
      "risks": []
    },
    "demographic_analysis": "N/A",
    "harm": []
  },
  "ethical_and_legal_considerations": {
    "privacy_and_anonymity": "N/A",
    "data_licensing": "Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International (CC BY-NC-SA 4.0)",
    "consent_procedures": "Participants provided formal consent to take part in the study.",
    "compliance_with_regulations": "N/A"
  }
}