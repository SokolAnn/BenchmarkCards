# FACTOID: Factual entailment for hallucination detection

## üìä Benchmark Details

**Name**: FACTOID: Factual entailment for hallucination detection

**Overview**: FACTOID is a benchmark dataset for detecting factual inaccuracies in content generated by large language models (LLMs), specifically focusing on the identification of hallucinations.

**Data Type**: Synthetic text pairs

**Domains**:
- Natural Language Processing
- Artificial Intelligence

**Languages**:
- English

**Similar Benchmarks**:
- SNLI
- MNLI
- HILT

**Resources**:
- [Resource](Dataset available at: https://huggingface.co/datasets/aisafe/FACTOID)
- [Resource](Demo available at: https://huggingface.co/spaces/aisafe/FACTOID)

## üéØ Purpose and Intended Users

**Goal**: To automatically detect factual inaccuracies and hallucinations in content generated by large language models.

**Target Audience**:
- NLP researchers
- AI developers

**Tasks**:
- Detecting hallucinations
- Evaluating LLMs based on hallucination vulnerabilities

**Limitations**: Temporal issues pose the greatest challenge in detection.

**Out of Scope Uses**:
- Applications outside of hallucination detection
- Usage outside of NLP tasks

## üíæ Data

**Source**: Synthetic extension of the HILT dataset.

**Size**: 2 million text pairs

**Format**: Text pairs with annotations for factual accuracy.

**Annotation**: Human-annotated and synthetically generated.

## üî¨ Methodology

**Methods**:
- Multi-task learning (MTL)
- Long text embeddings (e5-mistral-7b-instruct, GPT-3, SpanBERT, RoFormer)

**Metrics**:
- Accuracy
- Hallucination detection rate

**Calculation**: Average accuracy improvement of 30-40% over state-of-the-art TE methods.

**Interpretation**: Results indicate improved detection of factual inaccuracies compared to traditional TE.

**Validation**: Validated against real-world LLM generated text.

## ‚ö†Ô∏è Targeted Risks

**Risk Categories**:
- Misuse of AI
- Inaccurate information generation

**Atlas Risks**:
- **Accuracy**: Poor model accuracy
- **Fairness**: Data bias
- **Robustness**: Prompt injection attack
- **Transparency**: Lack of training data transparency

**Demographic Analysis**: N/A

**Potential Harm**: Potential spread of inaccurate information from LLMs.

## üîí Ethical and Legal Considerations

**Privacy And Anonymity**: Data anonymization was considered during dataset creation.

**Data Licensing**: Open access for research use.

**Consent Procedures**: Not Applicable

**Compliance With Regulations**: Not Applicable
