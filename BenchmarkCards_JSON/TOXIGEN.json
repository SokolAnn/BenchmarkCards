{
  "benchmark_details": {
    "is_benchmark": true,
    "name": "TOXIGEN: A Large-Scale Machine-Generated Dataset for Adversarial and Implicit Hate Speech Detection",
    "abbreviation": "TOXIGEN",
    "overview": "We create TOXIGEN, a new large-scale and machine-generated dataset of 274k toxic and benign statements about 13 minority groups. We develop a demonstration-based prompting framework and an adversarial classifier-in-the-loop decoding method (ALICE) to generate subtly toxic and benign text with a massive pretrained language model (GPT-3). TOXIGEN is almost entirely implicit (98.2% implicit) and balanced between toxic and benign statements for each group. We release our code and data at https://github.com/microsoft/ToxiGen.",
    "data_type": "text (single-sentence statements mentioning minority groups)",
    "domains": [
      "Natural Language Processing"
    ],
    "languages": null,
    "similar_benchmarks": [
      "ImplicitHateCorpus",
      "SocialBiasFrames",
      "DynaHate",
      "TweetBLM",
      "Breitfeller et al. (2019)",
      "Founta et al. (2018)",
      "Davidson et al. (2017)"
    ],
    "resources": [
      "https://github.com/microsoft/ToxiGen",
      "https://arxiv.org/abs/2203.09509"
    ]
  },
  "purpose_and_intended_users": {
    "goal": "Provide a large-scale, balanced, and largely-implicit dataset of statements mentioning 13 minority groups to improve evaluation and training of toxic language detectors, particularly for implicitly toxic language and adversarial robustness.",
    "audience": [
      "ML Researchers",
      "Model Developers",
      "Industry Practitioners",
      "Domain Experts in Toxicity/Hate Speech Detection"
    ],
    "tasks": [
      "Toxicity Detection",
      "Hate Speech Detection",
      "Implicit Hate / Microaggression Detection",
      "Robustness Evaluation of Toxicity Classifiers"
    ],
    "limitations": "The dataset only captures implicit toxicity for 13 identified minority groups and can naturally be noisy due to large-scale machine generation. Annotations may not capture the full complexity of human experiences and toxicity is subjective.",
    "out_of_scope_uses": []
  },
  "data": {
    "source": "Generated by GPT-3 (Brown et al., 2020) using demonstration-based prompting and an adversarial classifier-in-the-loop constrained beam search decoding method (ALICE); includes samples generated with top-k decoding and with ALICE.",
    "size": "274,186 examples (statements); covers over 135k toxic and over 135k benign statements. ALICE subset: 14,174 examples; top-k subset: 260,012 examples.",
    "format": "Dataframe (released with fields: prompt, generation, generation_method, prompt_label, group, roberta_prediction).",
    "annotation": "Human validation via Amazon Mechanical Turk. A human-validated test set (TOXIGEN-HUMAN VAL) of 792 statements rated by 3 annotators each (from a prequalified pool of 156 workers). Additional human annotations: 8,960 randomly sampled training examples. Harmfulness rated on a 1-5 Likert scale; mapping to classes uses the max of HARMFUL_IF_AI and HARMFUL_IF_HUMAN: scores <3 => non-toxic, =3 => ambiguous, >3 => toxic."
  },
  "methodology": {
    "methods": [
      "Demonstration-based prompting",
      "Top-k decoding (baseline generation)",
      "Adversarial classifier-in-the-loop constrained beam search decoding (ALICE)",
      "Human validation via Amazon Mechanical Turk",
      "Fine-tuning toxicity classifiers (HateBERT, ToxDectRoBERTa)"
    ],
    "metrics": [
      "Area Under ROC Curve (AUC)",
      "Fleiss' kappa",
      "Krippendorff's alpha",
      "Human annotation Likert-scale mean (1-5)",
      "Percentage mistaken for human-written (%)"
    ],
    "calculation": "For human annotation mapping: take the maximum of the HARMFUL_IF_AI and HARMFUL_IF_HUMAN scores and map to classes (<3 non-toxic, =3 ambiguous, >3 toxic). AUC reported for classifier evaluations on external human-written datasets and TOXIGEN-HUMAN VAL. Inter-annotator agreement reported as Fleiss' kappa and Krippendorff's alpha.",
    "interpretation": "Higher AUC indicates better classifier performance. Fine-tuning classifiers on TOXIGEN improves performance on human-written implicit toxic datasets (reported improvements of +7â€“19%). Human-evaluation statistics (e.g., percent mistaken for human) indicate how human-like generated examples are.",
    "baseline_results": "HateBERT (AUC): zero-shot on SocialBiasFrames 0.60; fine-tuned on ALICE 0.66; top-k 0.65; ALICE+top-k 0.71. On ImplicitHateCorpus: 0.60 (none), 0.60 (ALICE), 0.61 (top-k), 0.67 (ALICE+top-k). On DynaHate: 0.47 (none), 0.54 (ALICE), 0.59 (top-k), 0.66 (ALICE+top-k). On TOXIGEN-VAL: 0.57 (none), 0.93 (ALICE), 0.88 (top-k), 0.96 (ALICE+top-k). RoBERTa (AUC): SocialBiasFrames 0.65 (none), 0.70 (ALICE), 0.67 (top-k), 0.70 (ALICE+top-k); ImplicitHateCorpus 0.57, 0.64, 0.63, 0.66; DynaHate 0.49, 0.51, 0.50, 0.54; TOXIGEN-VAL 0.57, 0.87, 0.85, 0.93.",
    "validation": "Human validation: TOXIGEN-HUMAN VAL (792 statements) annotated by 3 annotators each; ensured no training statement had cosine similarity above 0.7 with any test statement. Inter-annotator agreement: Fleiss' kappa = 0.46; Krippendorff's alpha = 0.64. Additional large-scale human validation performed on ~8,960 training samples."
  },
  "targeted_risks": {
    "risk_categories": [
      "Fairness",
      "Robustness",
      "Misuse",
      "Accuracy",
      "Societal Impact"
    ],
    "atlas_risks": {
      "risks": [
        {
          "category": "Fairness",
          "subcategory": [
            "Data bias"
          ]
        },
        {
          "category": "Robustness",
          "subcategory": [
            "Evasion attack"
          ]
        },
        {
          "category": "Misuse",
          "subcategory": [
            "Spreading toxicity"
          ]
        },
        {
          "category": "Accuracy",
          "subcategory": [
            "Poor model accuracy"
          ]
        },
        {
          "category": "Societal Impact",
          "subcategory": [
            "Impact on affected communities"
          ]
        }
      ]
    },
    "demographic_analysis": "TOXIGEN includes mentions of 13 minority identity groups (per-group counts and statistics reported in Table 2). Human annotator demographics (from a survey of MTurk workers): 56.9% identify as White, 9.8% as Black, 3.9% as Hispanic, 3.9% as Asian, 5.9% as Other; 45.1% female, 37.3% male, 2% non-binary; majority aged 25-45 (58.8%).",
    "harm": [
      "Detecting implicit toxic language / implicit hate speech (stereotyping, microaggressions)",
      "Preventing over-censoring or marginalization of minority groups by improving classifier robustness and reducing spurious identity-toxicity correlations",
      "Identifying adversarially-generated toxicity that can harm targeted communities"
    ]
  },
  "ethical_and_legal_considerations": {
    "privacy_and_anonymity": "Annotators provided signed consent before viewing any text (annotation interface includes a strong warning and required signed consent). No additional anonymization procedures for data instances are specified in the paper.",
    "data_licensing": "N/A",
    "consent_procedures": "For human validation, annotators were required to provide signed consent before any text was shown. Workers were prequalified; a demographic survey was optional and collected.",
    "compliance_with_regulations": "N/A"
  }
}