{
  "benchmark_details": {
    "is_benchmark": true,
    "name": "MatPlotBench",
    "abbreviation": "N/A",
    "overview": "MatPlotBench is a high-quality benchmark consisting of 100 human-verified test cases (query, raw data, ground-truth figure) designed to quantitatively evaluate AI methods for scientific data visualization. The benchmark supports an automatic scoring approach based on GPT-4V and is intended to enable automatic quantitative evaluation of AI methods in this task.",
    "data_type": "multimodal (user queries: text; raw data: tabular/CSV; ground-truth figures: images)",
    "domains": [
      "Scientific Data Visualization"
    ],
    "languages": [],
    "similar_benchmarks": [
      "DS-1000",
      "Qwen-Agent Code Interpreter Benchmark"
    ],
    "resources": [
      "https://github.com/thunlp/MatPlotAgent"
    ]
  },
  "purpose_and_intended_users": {
    "goal": "Enable automatic quantitative evaluation of AI methods designed for scientific data visualization.",
    "audience": [],
    "tasks": [
      "Scientific Data Visualization",
      "Figure Generation"
    ],
    "limitations": "MatPlotBench is developed for general scientific data visualization and may not encompass all domain-specific requirements, potentially restricting its applicability to certain fields.",
    "out_of_scope_uses": []
  },
  "data": {
    "source": "75 original examples from the Matplotlib Gallery and 25 original examples from the OriginLab GraphGallery; examples were modified (data replacement for Matplotlib examples), converted to (query, raw data, ground-truth figure) triples, and human-verified.",
    "size": "100 examples",
    "format": "Raw data: CSV files; Ground-truth figures: images (extracted from OriginLab or plotted and saved as PNG); Queries: text",
    "annotation": "Preliminary queries generated by LLMs and revised by humans; human modification performed by annotators with a minimum of three years of coding/NLP experience, each query refined by two independent annotators; final verification by three NLP researchers."
  },
  "methodology": {
    "methods": [
      "Automated evaluation using GPT-4V (scoring 0-100)",
      "Human evaluation (human annotators scoring generated plots)"
    ],
    "metrics": [
      "Score (0-100)",
      "Pearson correlation coefficient",
      "p-value",
      "Accuracy of Code Execution Results (%)"
    ],
    "calculation": "Automatic score: GPT-4V is prompted to give a score from 0 to 100 comparing the model-generated plot and the ground-truth figure. For correlation with human evaluation: for each model, sample subsets of size n=25, repeat k=100 times to obtain average automatic scores A and average human scores H for each subset; compute Pearson correlation coefficient r and p-value using scipy.",
    "interpretation": "Higher score (0-100) indicates closer match to the ground truth. Human evaluation guide maps score ranges to qualitative categories (Exact Match 90-100, High Resemblance 70-89, Moderate Resemblance 50-69, Low Resemblance 30-49, Poor Match 10-29, No Resemblance 1-9, Failure to Generate 0). Correlation interpretation: r > 0.8 and p < 0.05 indicates a strong correlation between automatic and human evaluation.",
    "baseline_results": "Direct decoding baselines on MatPlotBench: GPT-4: 48.86; GPT-3.5: 38.03; Magicoder-S-DS-6.7B: 38.49; Deepseek-coder-6.7B-instruct: 31.53; CodeLlama-34B-Instruct: 16.54; Deepseek-coder-33B-instruct: 30.88; WizardCoder-Python-33B-V1.1: 36.94.",
    "validation": "Human verification of (query, raw data, ground-truth) triples by three NLP researchers; human evaluation performed by recruited annotators; automatic evaluation validated by computing Pearson correlation between GPT-4V scores and human-annotated scores (for GPT-4: r=0.876, p=7.41e-33; for GPT-3.5: r=0.836, p=2.67e-27)."
  },
  "targeted_risks": {
    "risk_categories": [],
    "atlas_risks": {
      "risks": null
    },
    "demographic_analysis": "N/A",
    "harm": "N/A"
  },
  "ethical_and_legal_considerations": {
    "privacy_and_anonymity": "Human annotators were informed that the collected data will be used solely for academic research purposes and their personal information will not be disclosed.",
    "data_licensing": "N/A",
    "consent_procedures": "Human annotators were informed about the research use of collected data; annotators were compensated for their work.",
    "compliance_with_regulations": "N/A"
  }
}