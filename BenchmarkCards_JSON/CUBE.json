{
  "benchmark_details": {
    "is_benchmark": true,
    "name": "Rubrik’s CUBE (Cognitive Understanding of BERT-based Explanations)",
    "abbreviation": "CUBE",
    "overview": "Rubrik’s CUBE is a task-independent evaluation rubric and dataset aimed at assessing the quality of explanations generated by Large Language Models (LLMs). It includes 26,000 explanations generated across different tasks and evaluated using the proposed rubric.",
    "data_type": "explanation pairs",
    "domains": [
      "Natural Language Processing"
    ],
    "languages": [
      "English"
    ],
    "similar_benchmarks": [],
    "resources": [
      "https://github.com/RubriksCube/rubriks_cube"
    ]
  },
  "purpose_and_intended_users": {
    "goal": "To systematically evaluate the quality of LLM-generated explanations based on a hierarchical rubric.",
    "audience": [
      "ML Researchers",
      "NLP Practitioners"
    ],
    "tasks": [
      "Explanatory Evaluation"
    ],
    "limitations": "N/A",
    "out_of_scope_uses": []
  },
  "data": {
    "source": "The dataset comprised explanations generated by humans and various LLMs across four tasks.",
    "size": "26,000 explanations",
    "format": "JSON",
    "annotation": "Quality annotated by humans and LLMs using the Rubrik."
  },
  "methodology": {
    "methods": [
      "Human evaluation",
      "LLM evaluation"
    ],
    "metrics": [
      "Inter-rater Agreement",
      "Quality Assessment Metrics"
    ],
    "calculation": "Evaluations are scored based on a hierarchical system where different dimensions are assessed for each explanation.",
    "interpretation": "Good explanations are those which meet all evaluative criteria established in the rubric.",
    "baseline_results": "N/A",
    "validation": "The rubric's effectiveness was validated through inter-rater reliability metrics."
  },
  "targeted_risks": {
    "risk_categories": [
      "Bias",
      "Fairness",
      "Transparency"
    ],
    "atlas_risks": {
      "risks": [
        {
          "category": "Fairness",
          "subcategory": [
            "Data bias"
          ]
        },
        {
          "category": "Transparency",
          "subcategory": [
            "Lack of training data transparency"
          ]
        }
      ]
    },
    "demographic_analysis": "N/A",
    "harm": [
      "Potential for misleading explanations impacting decision making"
    ]
  },
  "ethical_and_legal_considerations": {
    "privacy_and_anonymity": "Informed consent was obtained from all participants.",
    "data_licensing": "N/A",
    "consent_procedures": "All participants were informed of the study's aims and procedures prior to their involvement.",
    "compliance_with_regulations": "N/A"
  }
}