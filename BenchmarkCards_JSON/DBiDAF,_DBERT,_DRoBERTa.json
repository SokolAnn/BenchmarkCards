{
  "benchmark_details": {
    "is_benchmark": true,
    "name": "DBiDAF, DBERT, DRoBERTa",
    "abbreviation": "DBiDAF, DBERT, DRoBERTa",
    "overview": "Investigation of a model-in-the-loop adversarial annotation methodology for Reading Comprehension (RC). The authors apply the methodology with three progressively stronger models in the annotation loop, collecting three datasets of 12,000 samples each (36,000 total), to study reproducibility of the adversarial effect, transfer between datasets collected with different model-in-the-loop strengths, and generalisation to non-adversarially collected data.",
    "data_type": "question-answering pairs (text)",
    "domains": [
      "Natural Language Processing"
    ],
    "languages": [
      "N/A"
    ],
    "similar_benchmarks": [
      "SQuAD1.1",
      "DROP",
      "NaturalQuestions",
      "Quoref",
      "AdversarialNLI",
      "CODAH",
      "SWAG",
      "HellaSWAG",
      "ReCoRD",
      "HotpotQA"
    ],
    "resources": [
      "https://arxiv.org/abs/2002.00293"
    ]
  },
  "purpose_and_intended_users": {
    "goal": "Investigate the model-in-the-loop adversarial annotation paradigm for Reading Comprehension: study reproducibility of adversarial examples, transfer between datasets collected with different adversary strengths, and generalisation to non-adversarial datasets.",
    "audience": [
      "ML Researchers",
      "Dataset creators",
      "Natural Language Processing researchers"
    ],
    "tasks": [
      "Question Answering",
      "Reading Comprehension"
    ],
    "limitations": "Datasets constructed with weaker models in the loop may become outdated as stronger models emerge; the validation procedure can result in valid questions being discarded (the authors explicitly note this when enforcing human answerability); some models (e.g., BiDAF) may be unable to learn from adversarially collected samples.",
    "out_of_scope_uses": []
  },
  "data": {
    "source": "Passages sourced from SQuAD1.1; adversarial question-answer pairs crowdsourced via Amazon Mechanical Turk with an in-the-loop model (BiDAF, BERT LARGE, or RoBERTa LARGE) guiding annotation. Resulting datasets are named DBiDAF, DBERT, and DRoBERTa. A modified version of SQuAD is referred to as DSQuAD.",
    "size": "DBiDAF: 10,000 training examples, 1,000 validation examples, 1,000 test examples (12,000 total); DBERT: 10,000 training, 1,000 validation, 1,000 test (12,000 total); DRoBERTa: 10,000 training, 1,000 validation, 1,000 test (12,000 total); Total adversarially collected samples: 36,000 examples. DSQuAD: 87,599 training examples, 5,278 dev examples, 5,292 test examples (as reported in Table 3).",
    "format": "N/A",
    "annotation": "Crowdsourced via Amazon Mechanical Turk (workers based in Canada, the UK, or the US; HIT Approval Rate >98%; at least 1,000 previously completed HITs). Annotators generate a question q for a passage p and highlight the human answer span ah. The (p,q) pair is sent to the model-in-the-loop which returns a predicted answer am. A word-overlap F1 between ah and am is computed; a threshold of 40% F1 is used to determine a model 'win' (if F1 > 40% the model wins and the question is rejected). Only questions where the model fails to answer correctly are retained. Workers complete qualification tasks; manual worker validation is applied (workers falling below an 80% success threshold are disqualified). Answerability is checked by at least three additional non-expert validators; questions without at least one matching validator answer are discarded from validation and test sets. Payment: USD 2.00 per question-generation HIT."
  },
  "methodology": {
    "methods": [
      "Crowdsourcing via Amazon Mechanical Turk",
      "Model-in-the-loop adversarial annotation",
      "Human validation (multi-validator answerability checks)",
      "Manual worker training and qualification",
      "Model training and evaluation (retraining experiments and cross-dataset evaluation)",
      "Qualitative annotation of comprehension requirement types"
    ],
    "metrics": [
      "Exact Match (EM)",
      "Word-overlap F1 (F1)"
    ],
    "calculation": "Word-overlap F1 between human answer ah and model answer am is computed; a threshold of 40% F1 determines whether the model 'wins' during annotation (if model F1 > 40% the question is rejected). Evaluation metrics for models are Exact Match (EM) and word-overlap F1 computed as in SQuAD.",
    "interpretation": "Higher EM and F1 indicate better model performance on the dataset/tasks. Human answerability is defined as at least one of three non-expert validators providing an answer matching the original. The authors interpret progressive decreases in EM/F1 when evaluating on datasets constructed with stronger model-in-the-loop adversaries as indicating increased difficulty and distributional shift.",
    "baseline_results": "Models (on SQuAD1.1 validation) achieve EM/F1: BiDAF 65.5/77.5, BERT LARGE 82.7/90.3, RoBERTa LARGE 86.9/93.6. Non-expert human performance (Table 2) on validation: DBiDAF 63.0 EM / 76.9 F1; DBERT 59.2 EM / 74.3 F1; DRoBERTa 58.1 EM / 72.0 F1. The paper reports numerous model training and cross-evaluation results across DBiDAF, DBERT, DRoBERTa, DSQuAD, DDROP, and DNQ (see Tables 5-8).",
    "validation": "The existing SQuAD1.1 validation set was split in half (stratified by document title) to create an official test set. Passage consistency was maintained across splits. Worker outputs were manually screened during qualification; samples of every worker's HITs were manually reviewed after each annotation batch and workers below an 80% success threshold were disqualified and their work discarded. Answerability checks were performed on validation and test sets using at least three validators; only questions with at least one matching validator answer were retained. The authors report answerability scores of 87.95% (DBiDAF), 85.41% (DBERT), and 82.63% (DRoBERTa) for validation/test."
  },
  "targeted_risks": {
    "risk_categories": [
      "Bias",
      "Robustness"
    ],
    "atlas_risks": {
      "risks": [
        {
          "category": "Fairness",
          "subcategory": [
            "Data bias"
          ]
        },
        {
          "category": "Governance",
          "subcategory": [
            "Unrepresentative risk testing",
            "Lack of testing diversity"
          ]
        }
      ]
    },
    "demographic_analysis": "N/A",
    "harm": "Detecting model blind spots and dataset biases; probing model robustness to adversarially authored questions."
  },
  "ethical_and_legal_considerations": {
    "privacy_and_anonymity": "N/A",
    "data_licensing": "N/A",
    "consent_procedures": "N/A",
    "compliance_with_regulations": "N/A"
  }
}