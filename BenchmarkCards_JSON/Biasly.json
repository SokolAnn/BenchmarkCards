{
  "benchmark_details": {
    "is_benchmark": true,
    "name": "Biasly (Expert-Annotated Dataset for Misogyny Detection and Mitigation)",
    "abbreviation": "Biasly",
    "overview": "Biasly is an expert-annotated dataset capturing subtle forms of misogyny in North American movie subtitles. It is designed for tasks including misogyny classification, severity score regression, and mitigation through text rewriting, fostering responsible AI development for social good.",
    "data_type": "text",
    "domains": [
      "Natural Language Processing"
    ],
    "languages": [
      "English"
    ],
    "similar_benchmarks": [
      "EDOS",
      "APPDIA",
      "ParaDetox"
    ],
    "resources": [
      "N/A"
    ]
  },
  "purpose_and_intended_users": {
    "goal": "To develop a comprehensive dataset for detecting and mitigating subtle forms of misogyny in language while promoting responsible AI development.",
    "audience": [
      "ML Researchers",
      "Social Scientists",
      "NLP Practitioners",
      "Bias Detection Researchers"
    ],
    "tasks": [
      "Text Classification",
      "Severity Score Regression",
      "Text Generation"
    ],
    "limitations": "N/A",
    "out_of_scope_uses": []
  },
  "data": {
    "source": "Expert annotations of contemporary movie subtitles from a corpus available through English-corpora.org.",
    "size": "10,000 datapoints",
    "format": "N/A",
    "annotation": "Expert annotations by diverse annotators from gender studies and linguistics fields."
  },
  "methodology": {
    "methods": [
      "Human evaluation",
      "Automated metrics"
    ],
    "metrics": [
      "F1 Score",
      "Accuracy",
      "BLEU Score"
    ],
    "calculation": "Metrics are calculated based on binary classifications and regression analysis of the annotations.",
    "interpretation": "Good performance is indicated by high scores on F1 and accuracy metrics, while BLEU score reflects text generation quality.",
    "baseline_results": "Models used include BERT, RoBERTa, and BART with baseline F1 scores reported in the paper.",
    "validation": "Inter-annotator agreement measured, and methodologies followed for comprehensive evaluation."
  },
  "targeted_risks": {
    "risk_categories": [
      "Fairness",
      "Bias",
      "Safety"
    ],
    "atlas_risks": {
      "risks": [
        {
          "category": "Fairness",
          "subcategory": [
            "Data bias",
            "Output bias"
          ]
        },
        {
          "category": "Accuracy",
          "subcategory": [
            "Poor model accuracy"
          ]
        }
      ]
    },
    "demographic_analysis": "N/A",
    "harm": "Potential misuse of the dataset for generating subtle misogynistic content."
  },
  "ethical_and_legal_considerations": {
    "privacy_and_anonymity": "N/A",
    "data_licensing": "N/A",
    "consent_procedures": "N/A",
    "compliance_with_regulations": "N/A"
  }
}