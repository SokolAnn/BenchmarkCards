{
  "benchmark_details": {
    "is_benchmark": true,
    "name": "ScoNe: Scoped Negation Benchmark (ScoNe-NLI and ScoNe-NLG)",
    "abbreviation": "ScoNe",
    "overview": "ScoNe introduces the Scoped Negation Natural Language Inference benchmark (ScoNe-NLI), an English-language benchmark in which each of the 1,202 examples is a contrast set with six examples where zero, one, or two negations may or may not affect the NLI label; and ScoNe-NLG, a sentence-completion test set containing 74 contrasting triplets that embed negation reasoning in short narratives. The benchmarks are intended to assess whether models have learned how negation morphemes semantically scope, and to evaluate fine-tuning and in-context learning strategies.",
    "data_type": "question-answering pairs (premise-hypothesis NLI pairs) for ScoNe-NLI; short narrative sentence-completion triplets for ScoNe-NLG (natural language generation examples)",
    "domains": [
      "Natural Language Processing"
    ],
    "languages": [
      "English"
    ],
    "similar_benchmarks": [
      "Monotonicity NLI (MoNLI)",
      "SNLI",
      "MNLI",
      "RTE",
      "MED",
      "Negation-NLI",
      "CondaQA",
      "ANLI",
      "Fever-NLI"
    ],
    "resources": [
      "https://github.com/selenashe/ScoNe",
      "https://arxiv.org/abs/2305.19426",
      "https://huggingface.co/roberta-large-mnli",
      "https://huggingface.co/MoritzLaurer/DeBERTa-v3-base-mnli-fever-anli"
    ]
  },
  "purpose_and_intended_users": {
    "goal": "Provide a diagnostic benchmark for assessing whether models can reason accurately about natural language negation and semantic scope, and to evaluate fine-tuning and in-context learning strategies.",
    "audience": [
      "ML Researchers",
      "Model Developers"
    ],
    "tasks": [
      "Natural Language Inference",
      "Natural Language Generation"
    ],
    "limitations": "ScoNe is intended as a diagnostic tool for controlled scientific experiments and is focused on English. It restricts to negation in the context of lexical entailment and mostly uses \"not\" as the negative morpheme. The authors note ScoNe may inherit undesirable biases and artifacts from SNLI and related sources and has not been audited for real-world applications.",
    "out_of_scope_uses": [
      "Uncritical use of ScoNe for real-world applications (authors advise against this)"
    ]
  },
  "data": {
    "source": "ScoNe-NLI is an extension of Monotonicity NLI (MoNLI), which itself was generated from SNLI; ScoNe-NLI was generated by using each example of NMoNLI to create a contrast set of six examples. ScoNe-NLG contains contrastive triplets of half-completed naturalistic narratives. The ScoNe work uses the train–test split of MoNLI where substituted lexical items are disjoint across training and testing data.",
    "size": "ScoNe-NLI: 1,202 contrast sets (each contrast set contains six examples). ScoNe-NLG: 74 contrasting triplets.",
    "format": "N/A",
    "annotation": "ScoNe-NLI uses gold NLI labels impacted by the scope of negation (derived from MoNLI). ScoNe-NLG model outputs were annotated by authors for coherence/compatibility; annotators agreed on 216/222 zero-shot responses (Fleiss kappa 0.84) and 220/222 few-shot responses (Fleiss kappa 0.91); the authors evaluated only the cases where annotators agreed."
  },
  "methodology": {
    "methods": [
      "Fine-tuning",
      "In-context learning",
      "Human evaluation (annotator agreement for ScoNe-NLG outputs)"
    ],
    "metrics": [
      "Accuracy",
      "Fleiss kappa (annotator agreement)"
    ],
    "calculation": "For InstructGPT experiments, if the model response contains \"yes\" (case-insensitive) the example is predicted entailment, otherwise neutral. Accuracy is reported as percent correct on the test sections. Fleiss kappa is reported for annotator agreement on ScoNe-NLG evaluations.",
    "interpretation": "Fine-tuning (many-shot) on ScoNe-NLI training data leads to near-perfect performance, indicating models can learn negation reasoning with appropriate fine-tuning. In-context learning with InstructGPT shows high performance only on sections where negation can be ignored and systematically fails on conditions where exactly one negation scopes; ScoNe-NLG results show InstructGPT can succeed on narrative-style completions aligned with pretraining.",
    "baseline_results": "DeBERTa-v3-base fine-tuned on MAF-NLI+MoNLI+ScoNe-NLI: 100.0% across ScoNe-NLI sections (Table 2). RoBERTa fine-tuned on MAF-NLI+MoNLI+ScoNe-NLI: 100.0% across sections (Table 6). In-context learning with InstructGPT (davinci-003) few-shot reasoning prompt: overall accuracy 0.82 (82%) on ScoNe-NLI (Table 4). ScoNe-NLG with davinci-003: few-shot accuracy 0.95 (95%), zero-shot accuracy 0.92 (92%) (Table 5).",
    "validation": "Train–test split of MoNLI used where substituted lexical items are disjoint across training and testing. ScoNe-NLG human annotations report Fleiss kappa values (0.84 zero-shot, 0.91 few-shot) and the authors evaluated only examples with annotator agreement."
  },
  "targeted_risks": {
    "risk_categories": [
      "Fairness",
      "Accuracy",
      "Robustness"
    ],
    "atlas_risks": {
      "risks": [
        {
          "category": "Fairness",
          "subcategory": [
            "Data bias"
          ]
        }
      ]
    },
    "demographic_analysis": "N/A",
    "harm": [
      "Undesirable dataset/model biases inherited from SNLI and related sources (explicitly noted by authors)"
    ]
  },
  "ethical_and_legal_considerations": {
    "privacy_and_anonymity": "N/A",
    "data_licensing": "Appendix C lists licenses for referenced models and datasets including MIT license and Creative Commons Attribution-ShareAlike licenses (explicitly stated for RoBERTa, DeBERTa, MoNLI, MED, Negation-NLI and others in the appendix).",
    "consent_procedures": "N/A",
    "compliance_with_regulations": "N/A"
  }
}