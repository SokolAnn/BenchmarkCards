{
  "benchmark_details": {
    "is_benchmark": true,
    "name": "MultiNRC (Multilingual Native Reasoning Challenge)",
    "abbreviation": "MultiNRC",
    "overview": "MultiNRC is a benchmark designed to assess LLMs on more than 1,000 native, linguistic and culturally grounded reasoning questions written by native speakers in French, Spanish, and Chinese. It covers four core reasoning categories: language-specific linguistic reasoning, wordplay & riddles, cultural/tradition reasoning, and math reasoning with cultural relevance.",
    "data_type": "question-answering pairs",
    "domains": [
      "Natural Language Processing"
    ],
    "languages": [
      "French",
      "Spanish",
      "Chinese"
    ],
    "similar_benchmarks": [
      "MMLU (Massive Multitask Language Understanding)",
      "HellaSwag"
    ],
    "resources": [
      "https://huggingface.co/datasets/ScaleAI/MultiNRC"
    ]
  },
  "purpose_and_intended_users": {
    "goal": "To evaluate the multilingual abilities of LLMs through linguistically and culturally-nuanced reasoning questions.",
    "audience": [
      "ML Researchers",
      "Industry Practitioners",
      "Model Developers"
    ],
    "tasks": [
      "Text Classification",
      "Question Answering"
    ],
    "limitations": "N/A",
    "out_of_scope_uses": []
  },
  "data": {
    "source": "Questions were created by native speakers for each target language, supplemented with automatic evaluation.",
    "size": "1,055 examples",
    "format": "JSON",
    "annotation": "Manual annotation by native speakers"
  },
  "methodology": {
    "methods": [
      "Human evaluation",
      "Automated metrics"
    ],
    "metrics": [
      "Accuracy"
    ],
    "calculation": "Metrics are calculated based on alignment with human judgment.",
    "interpretation": "Scores above 50% indicate reasonable performance, while lower scores highlight significant challenges for the models.",
    "baseline_results": null,
    "validation": "Data was validated through review by native speakers who assessed the quality and difficulty of questions."
  },
  "targeted_risks": {
    "risk_categories": [
      "Bias",
      "Accuracy"
    ],
    "atlas_risks": {
      "risks": [
        {
          "category": "Fairness",
          "subcategory": [
            "Data bias"
          ]
        },
        {
          "category": "Accuracy",
          "subcategory": [
            "Poor model accuracy"
          ]
        }
      ]
    },
    "demographic_analysis": null,
    "harm": []
  },
  "ethical_and_legal_considerations": {
    "privacy_and_anonymity": "N/A",
    "data_licensing": "N/A",
    "consent_procedures": "N/A",
    "compliance_with_regulations": "N/A"
  }
}