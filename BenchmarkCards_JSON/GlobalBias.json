{
  "benchmark_details": {
    "is_benchmark": true,
    "name": "GlobalBias",
    "abbreviation": "N/A",
    "overview": "GlobalBias is a dataset of 876,000 sentences that incorporates 40 distinct gender-by-ethnicity groups to study harmful stereotypes and evaluate language models' outputs.",
    "data_type": "text",
    "domains": [
      "Natural Language Processing"
    ],
    "languages": [
      "English"
    ],
    "similar_benchmarks": [
      "StereoSet",
      "HOLISTICBIAS"
    ],
    "resources": [
      "https://github.com/groovychoons/GlobalBias"
    ]
  },
  "purpose_and_intended_users": {
    "goal": "To conduct a comprehensive study of intersectional stereotypes and evaluate how these stereotypes are represented in language models.",
    "audience": [
      "ML Researchers",
      "Ethics in AI Developers"
    ],
    "tasks": [
      "Bias Analysis",
      "Stereotype Evaluation"
    ],
    "limitations": "The dataset excludes critical aspects such as age, disability, and socioeconomic status and focuses on a fixed set of gender-by-ethnicity groups.",
    "out_of_scope_uses": []
  },
  "data": {
    "source": "Generated from clustering techniques applied on a seed dataset of names annotated with ethnicity and gender.",
    "size": "876,000 sentences",
    "format": "JSON",
    "annotation": "Automatically generated using clustering techniques."
  },
  "methodology": {
    "methods": [
      "Evaluation of language models using perplexity and a novel metric Adjusted Perplexity across Descriptors (APX)."
    ],
    "metrics": [
      "Perplexity",
      "Adjusted Perplexity"
    ],
    "calculation": "Perplexity is calculated using tokenized sentences or pseudo-likelihood for masked language models.",
    "interpretation": "Lower perplexity scores indicate strong likelihood and more consistent stereotype associations within model outputs.",
    "baseline_results": null,
    "validation": "Validation through human evaluation of outputs against expected stereotypical narratives."
  },
  "targeted_risks": {
    "risk_categories": [
      "Fairness",
      "Accuracy",
      "Social Bias"
    ],
    "atlas_risks": {
      "risks": [
        {
          "category": "Fairness",
          "subcategory": [
            "Data bias"
          ]
        },
        {
          "category": "Accuracy",
          "subcategory": [
            "Unrepresentative data"
          ]
        },
        {
          "category": "Societal Impact",
          "subcategory": [
            "Human exploitation"
          ]
        }
      ]
    },
    "demographic_analysis": "The dataset examines biases across various intersectional demographic groups.",
    "harm": "Potential reinforcement of stereotypes could exacerbate societal inequalities."
  },
  "ethical_and_legal_considerations": {
    "privacy_and_anonymity": "N/A",
    "data_licensing": "N/A",
    "consent_procedures": "N/A",
    "compliance_with_regulations": "N/A"
  }
}