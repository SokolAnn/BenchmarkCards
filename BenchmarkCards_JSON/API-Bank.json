{
  "benchmark_details": {
    "is_benchmark": true,
    "name": "API-Bank: A Comprehensive Benchmark for Tool-Augmented LLMs",
    "abbreviation": "API-Bank",
    "overview": "API-Bank is a benchmark specifically designed for tool-augmented LLMs that (1) provides a runnable evaluation system consisting of 73 API tools and 314 manually annotated tool-use dialogues with 753 API calls for evaluation, and (2) provides a comprehensive training set containing 1,888 tool-use dialogues from 2,138 APIs across 1,000 domains to assess LLMs' planning, retrieving, and calling API tools.",
    "data_type": "text (multi-turn dialogues with API call and response annotations)",
    "domains": [
      "Account Management",
      "Information Query and Processing",
      "Health Management",
      "Schedule Management",
      "Smart Home",
      "Finance Management",
      "Entertainment",
      "Travel",
      "Mental Health Hotline and Support",
      "Dental Procedure Cost Estimate",
      "Nutrition Planning"
    ],
    "languages": [
      "English"
    ],
    "similar_benchmarks": [
      "DATESET (Schick et al., 2023)",
      "APIBench (Patil et al., 2023)",
      "ToolAlpaca (Tang et al., 2023)",
      "ToolBench1 (Qin et al., 2023b)",
      "ToolBench2 (Xu et al., 2023)",
      "ToolQA (Zhuang et al., 2023)"
    ],
    "resources": [
      "https://github.com/AlibabaResearch/DAMO-ConvAI/tree/main/api-bank",
      "https://github.com/public-apis/public-apis",
      "https://arxiv.org/abs/2304.08244v2"
    ]
  },
  "purpose_and_intended_users": {
    "goal": "To evaluate LLMs' effectiveness in utilizing external API tools (measuring planning, retrieving, and calling abilities), provide a high-quality training dataset to improve tool-augmented LLMs, and identify obstacles to effective tool usage.",
    "audience": [
      "ML Researchers"
    ],
    "tasks": [
      "Call (API Calling)",
      "Retrieval+Call (API Retrieval and Calling)",
      "Plan+Retrieval+Call (Planning, API Retrieval, and Calling)"
    ],
    "limitations": "API-Bank focuses solely on English; only Lynx-7B was fine-tuned on API-Bank in this work and larger-scale models were not explored; results from a commercially viable internal model are not reported due to anonymity reasons.",
    "out_of_scope_uses": []
  },
  "data": {
    "source": "Training data generated automatically by the proposed Multi-agent method (five collaborative agents implemented via ChatGPT prompts). Evaluation data manually annotated by human annotators using the implemented runnable evaluation system of 73 APIs.",
    "size": "Overall: 2,202 dialogues, 2,211 APIs, 1,008 domains, 6,135 turns. Training set: 1,888 dialogues, 2,138 APIs, 1,000 domains, 5,221 turns. Evaluation set: 314 dialogues, 73 APIs, 8 domains, 914 turns. (Also reported: training set total API calls 4,149; evaluation set total API calls 753.)",
    "format": "N/A",
    "annotation": "Evaluation set: manual annotation workflow involving discussion between two annotators per dialogue, with two additional annotators for quality checks; each evaluation instance reviewed by four annotators; average annotation cost per dialogue $8; 21.5% of an initial 400 dialogues were discarded, resulting in 314 retained dialogues. Training set: automatically generated by Multi-agent (five agents) with an automatic tester agent that discarded ~35% of generated instances; Multi-agent generation cost ~ $0.1 per dialogue (claimed 98% cost reduction vs manual)."
  },
  "methodology": {
    "methods": [
      "Runnable execution-based evaluation (execute predicted API calls in evaluation system)",
      "Manual annotation and human review",
      "Automated metrics-based evaluation (Accuracy and ROUGE-L)"
    ],
    "metrics": [
      "Accuracy",
      "ROUGE-L"
    ],
    "calculation": "API call evaluation: Accuracy = number of correct predictions divided by total number of predictions; consistency is defined as whether the same database queries or modifications are performed and whether the returned results are the same. Response evaluation: ROUGE-L computed between model responses and annotated expected responses.",
    "interpretation": "Higher Accuracy indicates more correct API calls (correct database queries/modifications and matching returned results). Higher ROUGE-L indicates better quality of LLM responses after API execution. Performance generally decreases with increasing required ability (Call -> Retrieval+Call -> Plan+Retrieval+Call).",
    "baseline_results": "Main results on the API-Bank evaluation system (Table 3): Zero-shot Alpaca-7B total correctness 15.19%, ROUGE-L 0.0318; ChatGLM-6B total correctness 16.42%, ROUGE-L 0.2191; GPT-3 Davinci total correctness 0.57%, ROUGE-L 0.0814; GPT-3.5-turbo total correctness 47.16%, ROUGE-L 0.4267; GPT-4 total correctness 60.24%, ROUGE-L 0.3910; Fine-tuned Lynx-7B total correctness 39.58%, ROUGE-L 0.3794.",
    "validation": "Evaluation set: each instance reviewed by four annotators. Training set: randomly sampled 100 generated training samples were reviewed by annotators yielding a 94% available rate; the tester agent automatically filtered data and discarded ~35% of instances during generation; authors report that 78% of data automatically filtered by the tester agent did not adhere to design principles."
  },
  "targeted_risks": {
    "risk_categories": [
      "Robustness",
      "Accuracy"
    ],
    "atlas_risks": {
      "risks": [
        {
          "category": "Robustness",
          "subcategory": [
            "Hallucination"
          ]
        },
        {
          "category": "Accuracy",
          "subcategory": [
            "Poor model accuracy"
          ]
        }
      ]
    },
    "demographic_analysis": "N/A",
    "harm": null
  },
  "ethical_and_legal_considerations": {
    "privacy_and_anonymity": "During interviews participants were informed in advance that their feedback would be used for product development and potentially published, and no personal private information was disclosed.",
    "data_licensing": "N/A",
    "consent_procedures": "Participants in interviews were informed about use of their feedback; annotators were hired and paid $15 US per hour (reported as higher than local statutory minimum wage).",
    "compliance_with_regulations": "N/A"
  }
}