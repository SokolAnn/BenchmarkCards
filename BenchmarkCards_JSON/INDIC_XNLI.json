{
  "benchmark_details": {
    "is_benchmark": true,
    "name": "INDIC XNLI",
    "abbreviation": "INDIC XNLI",
    "overview": "We introduce INDIC XNLI, an NLI dataset for 11 Indic languages. It has been created by high-quality machine translation of the original English XNLI dataset and our analysis attests to the quality of INDIC XNLI. By fine-tuning different pre-trained LMs on this INDIC XNLI, we analyze various cross-lingual transfer techniques with respect to the impact of the choice of language models, languages, multi-linguality, mix-language input, etc.",
    "data_type": "text (premise-hypothesis sentence pairs for Natural Language Inference)",
    "domains": [
      "Natural Language Processing"
    ],
    "languages": [
      "Assamese",
      "Gujarati",
      "Kannada",
      "Malayalam",
      "Marathi",
      "Odia",
      "Punjabi",
      "Tamil",
      "Telugu",
      "Hindi",
      "Bengali"
    ],
    "similar_benchmarks": [
      "XNLI",
      "XQUAD",
      "MLQA",
      "PAWS-X",
      "XTREME",
      "XTREME-R",
      "XGLUE",
      "IndicGLUE"
    ],
    "resources": [
      "https://github.com/divyanshuaggarwal/indicxnli"
    ]
  },
  "purpose_and_intended_users": {
    "goal": "Provide an NLI benchmark dataset for eleven prominent Indic languages by translating the English XNLI dataset, and to analyze cross-lingual transfer and training strategies for multilingual language models on these languages.",
    "audience": [
      "ML Researchers",
      "Model Developers",
      "Natural Language Processing Researchers"
    ],
    "tasks": [
      "Natural Language Inference"
    ],
    "limitations": "INDIC XNLI is created via machine translation and the authors note machine translation is \"prone to contain some 'translationese' errors\"; the ideal strategy of creating an NLI dataset from scratch (human-authored in each Indic language) is noted as significantly more expensive and time-consuming.",
    "out_of_scope_uses": []
  },
  "data": {
    "source": "Translated from English XNLI using IndicTrans (Ramesh et al., 2021). IndicTrans is trained on the Samanantar parallel corpus.",
    "size": "392,702 training examples; 2,490 validation examples; 5,010 evaluation examples (per language)",
    "format": "N/A",
    "annotation": "Manual human validation: 2 annotators per language on 100 diverse sampled test instances using Determinantal Point Process sampling; annotators scored sentence pairs 0-5 (normalized to 0-1). Automatic validation: BERTScore comparisons (EngTrans and Multilingual strategies)."
  },
  "methodology": {
    "methods": [
      "Fine-tuning pre-trained multilingual language models (Indic-specific and generic)",
      "Human evaluation (manual validation of translations)",
      "Automated metrics evaluation (BERTScore)",
      "Cross-lingual transfer experiments (zero-shot, translate-train, translate-test, mix-language evaluation)"
    ],
    "metrics": [
      "Accuracy",
      "BERTScore (F1-based)",
      "Pearson correlation",
      "Spearman correlation"
    ],
    "calculation": "Accuracy computed as in the original XNLI paper. Human validation scores: annotators assigned integer scores 0-5 which were normalized to a 0-1 probability range and averaged over sampled instances. BERTScore computed between English and translated sentences (both EngTrans and multilingual strategies as described). Pearson and Spearman correlations computed over annotation scores.",
    "interpretation": "Higher Accuracy indicates better NLI performance. BERTScore is used as an automatic proxy that correlates with human judgment. Human validation average scores > 0.85 and Pearson > 0.7 and Spearman > 0.8 are reported as supporting high translation quality. The authors report English+Indic Train strategy yields best performance, and MuRIL performs best across Indic languages in many settings.",
    "baseline_results": "Authors report comparative results across models and strategies: MuRIL generally performs best across Indic languages (except some English-eval scenarios where XLM-RoBERTa outperforms). English + Indic Train and Train All strategies give the best results; IndicBERT performs worse than mBERT attributed to smaller model size.",
    "validation": "Validation includes (a) human validation: 2 expert annotators per language scored 100 DPP-sampled test instances following SemEval-2016 Task-I guidelines, with Pearson and Spearman correlations reported; (b) automatic validation using BERTScore via back-translation (EngTrans) and multilingual comparison (mBERT)."
  },
  "targeted_risks": {
    "risk_categories": [
      "Accuracy"
    ],
    "atlas_risks": {
      "risks": [
        {
          "category": "Accuracy",
          "subcategory": [
            "Poor model accuracy"
          ]
        }
      ]
    },
    "demographic_analysis": "N/A",
    "harm": "N/A"
  },
  "ethical_and_legal_considerations": {
    "privacy_and_anonymity": "N/A",
    "data_licensing": "N/A",
    "consent_procedures": "N/A",
    "compliance_with_regulations": "N/A"
  }
}