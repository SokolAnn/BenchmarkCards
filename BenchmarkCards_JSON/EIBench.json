{
  "benchmark_details": {
    "is_benchmark": true,
    "name": "EIBench",
    "abbreviation": "N/A",
    "overview": "EIBench is a curated benchmark for Emotion Interpretation (EI), featuring 1615 basic EI samples and 50 complex EI samples to evaluate the emotional triggers of affective states based on explicit and implicit factors.",
    "data_type": "text",
    "domains": [
      "Natural Language Processing"
    ],
    "languages": [
      "English"
    ],
    "similar_benchmarks": [],
    "resources": [
      "https://github.com/Lum1104/EIBench"
    ]
  },
  "purpose_and_intended_users": {
    "goal": "To provide a dataset that helps in understanding the causative factors behind emotional responses rather than just recognizing emotion labels.",
    "audience": [
      "ML Researchers",
      "Industry Practitioners",
      "Model Developers"
    ],
    "tasks": [
      "Emotion Interpretation"
    ],
    "limitations": "N/A",
    "out_of_scope_uses": []
  },
  "data": {
    "source": "The dataset is derived from CAER-S and EmoSet.",
    "size": "1,665 samples",
    "format": "N/A",
    "annotation": "Coarse-to-Fine Self-Ask (CFSA) annotation method involving multiple rounds of questioning."
  },
  "methodology": {
    "methods": [
      "Human evaluation",
      "Automated metrics"
    ],
    "metrics": [
      "Emotional Trigger Recall",
      "Long-Term Coherence"
    ],
    "calculation": "Scores and evaluations are based on overlaps between predicted and actual emotional triggers.",
    "interpretation": "Higher scores indicate better identification of emotional triggers.",
    "baseline_results": null,
    "validation": "The accuracy of annotations is validated by human evaluators achieving high confidence scores."
  },
  "targeted_risks": {
    "risk_categories": [
      "Bias",
      "Accuracy"
    ],
    "atlas_risks": {
      "risks": [
        {
          "category": "Accuracy",
          "subcategory": [
            "Unrepresentative data"
          ]
        },
        {
          "category": "Fairness",
          "subcategory": [
            "Data bias"
          ]
        }
      ]
    },
    "demographic_analysis": "N/A",
    "harm": []
  },
  "ethical_and_legal_considerations": {
    "privacy_and_anonymity": "N/A",
    "data_licensing": "N/A",
    "consent_procedures": "N/A",
    "compliance_with_regulations": "N/A"
  }
}