{
  "benchmark_details": {
    "is_benchmark": true,
    "name": "ProactiveVideoQA",
    "abbreviation": "N/A",
    "overview": "ProactiveVideoQA is the first comprehensive benchmark designed to evaluate the proactive interaction capabilities of Multimodal Large Language Models (MLLMs) during video playback. It focuses on systems' abilities to engage users by determining when to respond as relevant information appears over time.",
    "data_type": "video",
    "domains": [
      "Natural Language Processing"
    ],
    "languages": [
      "English"
    ],
    "similar_benchmarks": [
      "VideoQA",
      "TVQA",
      "Ego4D Goalstep"
    ],
    "resources": [
      "https://github.com/yellow-binary-tree/ProactiveVideoQA"
    ]
  },
  "purpose_and_intended_users": {
    "goal": "To evaluate and promote the development of proactive interaction capabilities in video multimodal systems.",
    "audience": [
      "ML Researchers",
      "Industry Practitioners",
      "Model Developers"
    ],
    "tasks": [
      "Question Answering",
      "Video Understanding"
    ],
    "limitations": "N/A",
    "out_of_scope_uses": []
  },
  "data": {
    "source": "Collected from multiple datasets including Shot2story-MAGQA-39k, Ego4D Goalstep, TVQA, and UCF-Crime.",
    "size": "1,377 videos",
    "format": "N/A",
    "annotation": "Annotations provided include questions, answers, and relevant timestamps. Generated from dense captions and manual descriptions."
  },
  "methodology": {
    "methods": [
      "Automated metrics",
      "User study"
    ],
    "metrics": [
      "PAUC (Proactive Area Under Curve)"
    ],
    "calculation": "PAUC is calculated based on the temporal evolution of response quality and is designed to account for timing and accuracy of model outputs.",
    "interpretation": "Higher PAUC scores indicate better alignment with user experience in proactive interactions.",
    "baseline_results": "N/A",
    "validation": "Validated through comparison of model performance against human preferences."
  },
  "targeted_risks": {
    "risk_categories": [
      "Bias",
      "Accuracy"
    ],
    "atlas_risks": {
      "risks": [
        {
          "category": "Accuracy",
          "subcategory": [
            "Unrepresentative data"
          ]
        }
      ]
    },
    "demographic_analysis": "N/A",
    "harm": "N/A"
  },
  "ethical_and_legal_considerations": {
    "privacy_and_anonymity": "N/A",
    "data_licensing": "N/A",
    "consent_procedures": "N/A",
    "compliance_with_regulations": "N/A"
  }
}