{
  "benchmark_details": {
    "is_benchmark": true,
    "name": "PalmX 2025 Shared Task",
    "abbreviation": "N/A",
    "overview": "PalmX 2025 is the first shared task designed to benchmark the cultural competence of Large Language Models (LLMs) in Arabic and Islamic cultures through multiple-choice questions on various cultural topics.",
    "data_type": "multiple-choice questions (MCQs)",
    "domains": [
      "Natural Language Processing"
    ],
    "languages": [
      "Arabic"
    ],
    "similar_benchmarks": [],
    "resources": [
      "https://palmx.dlnlp.ai/",
      "https://github.com/UBC-NLP/palmx_2025"
    ]
  },
  "purpose_and_intended_users": {
    "goal": "To systematically evaluate and compare the cultural competence of LLMs in understanding Arabic and Islamic cultures.",
    "audience": [
      "ML Researchers",
      "Industry Practitioners",
      "Model Developers"
    ],
    "tasks": [
      "Cultural Competence Evaluation",
      "Question Answering"
    ],
    "limitations": "There are inherent dataset imbalances and an evaluation design that limits the assessment to multiple-choice questions in Modern Standard Arabic, which may not capture broader aspects of cultural and linguistic competence.",
    "out_of_scope_uses": []
  },
  "data": {
    "source": "Data collected from various cultural and Islamic sources, including web crawls and human-created questions.",
    "size": "8,000 questions total (2,000 training, 500 dev, 2,000 test for General Arabic Culture; 600 training, 300 dev, 1,000 test for General Islamic Culture)",
    "format": "JSON",
    "annotation": "Questions were created and annotated by language experts."
  },
  "methodology": {
    "methods": [
      "Human evaluation",
      "Automated metrics"
    ],
    "metrics": [
      "Accuracy"
    ],
    "calculation": "The model's predicted label is compared against the ground-truth label for each question, with overall accuracy calculated as the percentage of correctly answered questions.",
    "interpretation": "Accuracy measures how well the model predicts the correct answer among multiple choices.",
    "baseline_results": "Baseline accuracy for Subtask 1 was 67.55% and for Subtask 2 was 75.12% using the NileChat-3B model without fine-tuning.",
    "validation": "The evaluation is blind, ensuring that participants had no prior access to the test data."
  },
  "targeted_risks": {
    "risk_categories": [
      "Bias",
      "Fairness",
      "Accuracy"
    ],
    "atlas_risks": {
      "risks": [
        {
          "category": "Fairness",
          "subcategory": [
            "Data bias"
          ]
        },
        {
          "category": "Accuracy",
          "subcategory": [
            "Unrepresentative data"
          ]
        }
      ]
    },
    "demographic_analysis": "Analysis of cultural representation across 22 Arab countries, with attention to representation imbalances.",
    "harm": []
  },
  "ethical_and_legal_considerations": {
    "privacy_and_anonymity": "All data sources used are publicly available or properly licensed with no personal information collected.",
    "data_licensing": "N/A.",
    "consent_procedures": "N/A.",
    "compliance_with_regulations": "N/A."
  }
}