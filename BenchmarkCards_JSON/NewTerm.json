{
  "benchmark_details": {
    "is_benchmark": true,
    "name": "NewTerm",
    "abbreviation": "N/A",
    "overview": "NewTerm is an adaptive benchmark for real-time evaluation of new terms in large language models (LLMs), allowing for annual updates. It focuses on LLM performance related to new words, phrases, and meanings, and seeks to address the challenges faced by LLMs due to knowledge cutoffs.",
    "data_type": "question-answering pairs",
    "domains": [
      "Natural Language Processing"
    ],
    "languages": [
      "English"
    ],
    "similar_benchmarks": [
      "N/A"
    ],
    "resources": [
      "https://github.com/hexuandeng/NewTerm"
    ]
  },
  "purpose_and_intended_users": {
    "goal": "To evaluate the ability of LLMs to understand and integrate new terms into their language processing capabilities over time.",
    "audience": [
      "ML Researchers",
      "Model Developers"
    ],
    "tasks": [
      "Natural Language Understanding"
    ],
    "limitations": "N/A",
    "out_of_scope_uses": []
  },
  "data": {
    "source": "Collected new terms from online dictionaries such as Cambridge, Collins, and Oxford.",
    "size": "744 questions for NewTerm 2022, 715 questions for NewTerm 2023, covering 600 new terms.",
    "format": "JSON",
    "annotation": "Automatically generated with human filtering for quality assurance."
  },
  "methodology": {
    "methods": [
      "Automated metrics",
      "Human evaluation"
    ],
    "metrics": [
      "Accuracy"
    ],
    "calculation": "Accuracy is measured based on model performance in identifying correct answers to benchmark questions.",
    "interpretation": "Higher accuracy indicates better understanding and integration of new terms by LLMs.",
    "baseline_results": "N/A",
    "validation": "Questions were filtered through a combination of automated methods and human verification."
  },
  "targeted_risks": {
    "risk_categories": [
      "Accuracy",
      "Fairness"
    ],
    "atlas_risks": {
      "risks": [
        {
          "category": "Accuracy",
          "subcategory": [
            "Poor model accuracy"
          ]
        },
        {
          "category": "Fairness",
          "subcategory": [
            "Data bias"
          ]
        }
      ]
    },
    "demographic_analysis": "N/A",
    "harm": "N/A"
  },
  "ethical_and_legal_considerations": {
    "privacy_and_anonymity": "N/A",
    "data_licensing": "Creative Commons Attribution 4.0 International license (CC BY 4.0)",
    "consent_procedures": "N/A",
    "compliance_with_regulations": "N/A"
  }
}