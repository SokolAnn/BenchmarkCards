{
  "benchmark_details": {
    "is_benchmark": true,
    "name": "UrbanPlanBench",
    "abbreviation": "N/A",
    "overview": "UrbanPlanBench is a comprehensive benchmark designed to evaluate the efficacy of Large Language Models (LLMs) in urban planning. It measures performance across fundamental principles, professional knowledge, and management and regulations aligned with the qualifications expected of human planners.",
    "data_type": "multiple-choice questions",
    "domains": [
      "Urban Planning"
    ],
    "languages": [
      "Chinese",
      "English"
    ],
    "similar_benchmarks": [
      "SuperGLUE",
      "BIG-Bench"
    ],
    "resources": [
      "https://github.com/tsinghua-fib-lab/PlanBench"
    ]
  },
  "purpose_and_intended_users": {
    "goal": "The primary objective of UrbanPlanBench is to assess the capabilities of LLMs in urban planning and catalyze their integration into practical workflows by providing a standardized evaluation framework.",
    "audience": [
      "ML Researchers",
      "Urban Planners",
      "AI Practitioners"
    ],
    "tasks": [
      "Question Answering"
    ],
    "limitations": "N/A",
    "out_of_scope_uses": []
  },
  "data": {
    "source": "Generated from urban planning textbooks and past urban planning exams.",
    "size": "30,000 instruction pairs",
    "format": "CSV",
    "annotation": "Manually curated and transformed into structured multiple-choice format."
  },
  "methodology": {
    "methods": [
      "Automated metrics",
      "Human evaluation"
    ],
    "metrics": [
      "Accuracy"
    ],
    "calculation": "Metrics are calculated based on the proportion of correctly answered questions in UrbanPlanBench.",
    "interpretation": "Higher accuracy indicates better performance in understanding urban planning concepts.",
    "baseline_results": "Current LLMs generally perform below the certification bar of professional urban planners.",
    "validation": "System-level validation against the urban planner qualification examination standards."
  },
  "targeted_risks": {
    "risk_categories": [
      "Accuracy"
    ],
    "atlas_risks": {
      "risks": [
        {
          "category": "Accuracy",
          "subcategory": [
            "Poor model accuracy",
            "Unrepresentative data"
          ]
        }
      ]
    },
    "demographic_analysis": "N/A",
    "harm": "N/A"
  },
  "ethical_and_legal_considerations": {
    "privacy_and_anonymity": "N/A",
    "data_licensing": "N/A",
    "consent_procedures": "N/A",
    "compliance_with_regulations": "N/A"
  }
}