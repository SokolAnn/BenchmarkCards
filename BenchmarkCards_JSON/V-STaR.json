{
  "benchmark_details": {
    "is_benchmark": true,
    "name": "V-STaR (Video Spatio-Temporal Reasoning)",
    "abbreviation": "V-STaR",
    "overview": "V-STaR is introduced to evaluate Video Large Language Models (Video-LLMs) on their spatio-temporal reasoning capabilities for answering video-based questions that involve understanding 'when', 'where', and 'what'. The benchmark includes a dataset constructed with semi-automated GPT-4-powered pipeline, featuring coarse-to-fine Chain-of-Thought (CoT) questions.",
    "data_type": "video",
    "domains": [
      "Computer Vision"
    ],
    "languages": [
      "English"
    ],
    "similar_benchmarks": [
      "TVQA",
      "QAEgo4D",
      "Next-GQA",
      "E.T. Bench"
    ],
    "resources": [
      "https://V-STaR-Bench.github.io/"
    ]
  },
  "purpose_and_intended_users": {
    "goal": "To provide a comprehensive assessment of the spatio-temporal reasoning ability of Video-LLMs in understanding video content through structured reasoning chains.",
    "audience": [
      "ML Researchers",
      "Model Developers",
      "Industry Practitioners"
    ],
    "tasks": [
      "Video Question Answering",
      "Spatio-Temporal Reasoning"
    ],
    "limitations": "N/A",
    "out_of_scope_uses": []
  },
  "data": {
    "source": "Videos collected from existing datasets (VidSTG, TVQA+, GOT-10k) and YouTube.",
    "size": "2,094 videos with a total of 64.12 hours of footage",
    "format": "Various video formats",
    "annotation": "Coarse-to-fine Chain-of-Thought (CoT) questions and spatial-temporal labels generated via a semi-automated process."
  },
  "methodology": {
    "methods": [
      "Automated metrics",
      "Human evaluation through CoT reasoning"
    ],
    "metrics": [
      "Accuracy",
      "Mean Temporal IoU (mtIoU)",
      "Mean Visual IoU (mvIoU)",
      "Logarithmic Geometric Mean (LGM)",
      "Arithmetic Mean (AM)"
    ],
    "calculation": "Accuracy is calculated based on the correctness of answers; mtIoU and mvIoU are used for evaluating temporal and spatial grounding metrics respectively. The LGM aggregates the performance across the reasoning chain.",
    "interpretation": "A higher LGM indicates better overall performance in spatio-temporal reasoning.",
    "baseline_results": "Experiments conducted on 14 contemporary Video-LLMs reveal significant gaps between models and expected performance on the V-STaR benchmark.",
    "validation": "The benchmark's validation involves assessing the reasoning capabilities across multiple models under structured question chains."
  },
  "targeted_risks": {
    "risk_categories": [
      "Fairness",
      "Accuracy",
      "Robustness"
    ],
    "atlas_risks": {
      "risks": [
        {
          "category": "Fairness",
          "subcategory": [
            "Data bias"
          ]
        },
        {
          "category": "Accuracy",
          "subcategory": [
            "Poor model accuracy"
          ]
        },
        {
          "category": "Robustness",
          "subcategory": []
        }
      ]
    },
    "demographic_analysis": "N/A",
    "harm": "N/A"
  },
  "ethical_and_legal_considerations": {
    "privacy_and_anonymity": "N/A",
    "data_licensing": "N/A",
    "consent_procedures": "N/A",
    "compliance_with_regulations": "N/A"
  }
}