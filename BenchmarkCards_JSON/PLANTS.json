{
  "benchmark_details": {
    "is_benchmark": true,
    "name": "PLANTS (Planning-like Tasks)",
    "abbreviation": "PLANTS",
    "overview": "The PLANTS dataset is specifically designed for plan summarization tasks, encompassing diverse domains such as automated plans, recipes, and travel plans. It aims to generate concise and coherent summaries of action sequences that achieve specific goals.",
    "data_type": "text",
    "domains": [
      "Natural Language Processing"
    ],
    "languages": [
      "English"
    ],
    "similar_benchmarks": [],
    "resources": [
      "https://github.com/VishalPallagani/PLANTS-benchmark"
    ]
  },
  "purpose_and_intended_users": {
    "goal": "To advance research in planning task summarization by providing a dataset specifically designed for summarizing planning-like (PL) tasks.",
    "audience": [
      "ML Researchers",
      "Industry Practitioners",
      "Model Developers",
      "Domain Experts"
    ],
    "tasks": [
      "Text Summarization"
    ],
    "limitations": "The dataset includes only 10 problems per domain, which may not fully capture the variability and complexity of real-world planning tasks.",
    "out_of_scope_uses": []
  },
  "data": {
    "source": "The PLANTS dataset was created from automated plans sourced from the downward-benchmarks, recipes sourced from the Recipe1M+ dataset, and travel routes generated using the OpenStreetMap API.",
    "size": "130 plans",
    "format": "N/A",
    "annotation": "Data was manually curated and selected based on specific criteria to ensure diversity in the planning tasks."
  },
  "methodology": {
    "methods": [
      "User studies",
      "Quantitative metrics"
    ],
    "metrics": [
      "Ease of understanding",
      "User preference scores"
    ],
    "calculation": "Metrics were calculated based on user ratings and performance measures during evaluations.",
    "interpretation": "Higher ratings in ease of understanding indicate better comprehension and quicker decision-making for the summaries.",
    "baseline_results": "GPT-4o provided the highest ratings for ease of understanding across all PL tasks.",
    "validation": "The human evaluation was conducted with inter-annotator agreement calculated using Cohenâ€™s kappa coefficient."
  },
  "targeted_risks": {
    "risk_categories": [
      "Bias",
      "Accuracy"
    ],
    "atlas_risks": {
      "risks": []
    },
    "demographic_analysis": "N/A",
    "harm": "N/A"
  },
  "ethical_and_legal_considerations": {
    "privacy_and_anonymity": "All data were sourced from publicly available repositories, ensuring compliance with usage terms and privacy regulations.",
    "data_licensing": "N/A",
    "consent_procedures": "Human evaluators participated voluntarily and provided informed consent.",
    "compliance_with_regulations": "N/A"
  }
}