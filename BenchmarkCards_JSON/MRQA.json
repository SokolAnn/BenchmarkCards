{
  "benchmark_details": {
    "is_benchmark": true,
    "name": "MRQA 2019 Shared Task: Evaluating Generalization in Reading Comprehension",
    "abbreviation": "MRQA",
    "overview": "The MRQA 2019 Shared Task evaluates the generalization capabilities of reading comprehension systems by adapting and unifying 18 distinct question answering datasets into a single, unified extractive QA format and testing systems on held-out out-of-domain datasets.",
    "data_type": "text (extractive question-answering pairs: question, context passage, answer span)",
    "domains": [
      "Natural Language Processing",
      "Healthcare",
      "Education"
    ],
    "languages": [
      "English"
    ],
    "similar_benchmarks": [
      "SQuAD",
      "NewsQA",
      "TriviaQA",
      "SearchQA",
      "HotpotQA",
      "Natural Questions",
      "BioASQ",
      "DROP",
      "DuoRC",
      "RACE",
      "RelationExtraction",
      "TextbookQA",
      "BioProcess",
      "ComplexWebQuestions",
      "MCTest",
      "QAMR",
      "QAST",
      "TREC"
    ],
    "resources": [
      "https://github.com/mrqa/MRQA-Shared-Task-2019",
      "https://worksheets.codalab.org",
      "http://catalog.elra.info",
      "https://arxiv.org/abs/1910.09753"
    ]
  },
  "purpose_and_intended_users": {
    "goal": "Test extractive question answering models on their ability to generalize to data distributions different from the distribution on which they were trained (evaluate out-of-domain generalization).",
    "audience": [
      "Machine Learning Researchers",
      "Model Developers"
    ],
    "tasks": [
      "Question Answering",
      "Reading Comprehension",
      "Extractive Question Answering"
    ],
    "limitations": "The shared task is restricted to English-language extractive question answering; passages are concatenated and truncated to the first 800 tokens; the task does not test unanswerable, multi-turn, or open-domain question types.",
    "out_of_scope_uses": [
      "Unanswerable question evaluation",
      "Multi-turn question answering",
      "Open-domain question answering"
    ]
  },
  "data": {
    "source": "Adapted and unified 18 sub-domain QA datasets (see Table 1): SQuAD, NewsQA, TriviaQA, SearchQA, HotpotQA, Natural Questions, BioASQ, DROP, DuoRC, RACE, RelationExtraction, TextbookQA, BioProcess, ComplexWebQuestions, MCTest, QAMR, QAST, TREC.",
    "size": "Varies by sub-domain (per Table 1). Examples: SQuAD: 86,588 train / 10,507 dev; NewsQA: 74,160 train / 4,212 dev; TriviaQA: 61,688 train / 7,785 dev; SearchQA: 117,384 train / 16,980 dev; HotpotQA: 72,928 train / 5,904 dev; Natural Questions: 104,071 train / 12,836 dev. Testing portions of Splits II and III were balanced to 1,500 examples per sub-domain.",
    "format": "Unified extractive QA format (question, context, answer string/span). Contexts concatenated and truncated to first 800 tokens; documents separated with [DOC], titles with [TLE], paragraphs with [PAR].",
    "annotation": "Uses original dataset annotations where available. For datasets without labeled answer spans, all occurrences of the answer string in the context are provided. Multiple-choice datasets: keep correct answer if contained in context and discard other options; random examples manually verified for quality."
  },
  "methodology": {
    "methods": [
      "Automated evaluation metrics (Exact Match, word-level F1 Score)",
      "Macro-averaged test F1 ranking across 12 held-out test datasets",
      "Baseline multi-task BERT-based model evaluation",
      "Meta-analysis of submitted systems (techniques included data sampling, multi-task learning, adversarial training, ensembling)"
    ],
    "metrics": [
      "Exact Match (EM)",
      "Word-level F1 Score (F1)",
      "Macro-averaged F1 across the 12 hidden test datasets (used for final ranking)"
    ],
    "calculation": "Follow SQuAD evaluation normalization rules (ignore articles and punctuation). EM requires exact answer string match; F1 measures token-level overlap with gold answer(s). Scores are macro-averaged across the 12 test datasets for final ranking. Scoring is based on answer string match rather than token index match in the context.",
    "interpretation": "Higher EM and F1 indicate better extractive QA performance. Systems are ranked by macro-averaged F1 across the 12 held-out test datasets to measure out-of-domain generalization. The paper reports absolute improvements over the provided BERT baselines (e.g., best system improved by 10.7 F1 over baseline).",
    "baseline_results": "Ours: BERT-Large baseline achieved 61.8 macro-averaged F1 on the combined Split II + III test (12 datasets). Ours: BERT-Base baseline achieved 58.5 macro-averaged F1 on Split II + III. The top submitted system (D-Net) achieved 72.5 macro-averaged F1 on Split II + III.",
    "validation": "Participants trained on a fixed training split of six datasets and were evaluated on 12 held-out datasets (six with dev data, six fully hidden). Test splits for Splits II and III were balanced to 1,500 examples per sub-domain; partitioning was done by context so no context is shared across development and testing portions to avoid leakage."
  },
  "targeted_risks": {
    "risk_categories": [],
    "atlas_risks": {
      "risks": null
    },
    "demographic_analysis": "N/A",
    "harm": "N/A"
  },
  "ethical_and_legal_considerations": {
    "privacy_and_anonymity": "N/A",
    "data_licensing": "N/A",
    "consent_procedures": "N/A",
    "compliance_with_regulations": "N/A"
  }
}