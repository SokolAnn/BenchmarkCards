{
  "benchmark_details": {
    "name": "RAGTruth",
    "overview": "RAGTruth is a corpus tailored for analyzing word-level hallucinations in various domains and tasks within the standard RAG frameworks for LLM applications. The dataset comprises nearly 18,000 naturally generated responses from diverse LLMs using RAG and includes meticulous manual annotations of hallucination intensity.",
    "data_type": "Text",
    "domains": [
      "Question Answering",
      "Data-to-text Writing",
      "News Summarization"
    ],
    "languages": [
      "English"
    ],
    "similar_benchmarks": [
      "HaluEval",
      "FELM",
      "RefChecker"
    ],
    "resources": [
      "Label Studio"
    ]
  },
  "purpose_and_intended_users": {
    "goal": "To develop effective hallucination prevention strategies under the retrieval-augmented generation (RAG) framework.",
    "audience": [
      "Researchers",
      "Developers",
      "Academics"
    ],
    "tasks": [
      "Evaluate hallucination detection methods",
      "Fine-tune LLMs for hallucination detection"
    ],
    "limitations": "N/A",
    "out_of_scope_uses": [
      "Applications outside of LLMs"
    ]
  },
  "data": {
    "source": "Generated responses from various LLMs",
    "size": "Approximately 18,000 responses",
    "format": "Text interaction with annotations",
    "annotation": "Manual annotations at both individual case and word levels"
  },
  "methodology": {
    "methods": [
      "Hallucination Detection Prompt",
      "SelfCheckGPT",
      "LMvLM",
      "Finetuning Llama-2-13B"
    ],
    "metrics": [
      "Precision",
      "Recall",
      "F1 Score"
    ],
    "calculation": "Measured through response-level and span-level evaluation of hallucination detection methods",
    "interpretation": "Evaluated based on performance across various tasks using the RAGTruth dataset",
    "baseline_results": null,
    "validation": "Evaluated using a held-out test set of 450 instances"
  },
  "targeted_risks": {
    "risk_categories": [
      "Accuracy",
      "Transparency",
      "Fairness"
    ],
    "atlas_risks": {
      "risks": [
        {
          "category": "Accuracy",
          "subcategory": [
            "Poor model accuracy"
          ]
        },
        {
          "category": "Transparency",
          "subcategory": [
            "Lack of training data transparency"
          ]
        },
        {
          "category": "Fairness",
          "subcategory": [
            "Data bias"
          ]
        }
      ]
    },
    "demographic_analysis": "N/A",
    "harm": "N/A"
  },
  "ethical_and_legal_considerations": {
    "privacy_and_anonymity": "Responses generated by LLMs may contain inaccuracies.",
    "data_licensing": "N/A",
    "consent_procedures": "N/A",
    "compliance_with_regulations": "In compliance with the Ethics Policy of the ACL."
  }
}