{
  "benchmark_details": {
    "is_benchmark": true,
    "name": "Mitigating Biases to Embrace Diversity: A Comprehensive Annotation Benchmark for Toxic Language",
    "abbreviation": "N/A",
    "overview": "This study introduces a prescriptive annotation benchmark grounded in humanities research to ensure consistent, unbiased labeling of offensive language, particularly for casual and non-mainstream language uses. It contributes two newly annotated datasets to achieve higher inter-annotator agreement between human and language model annotations compared to original datasets based on descriptive instructions.",
    "data_type": "text",
    "domains": [
      "Natural Language Processing"
    ],
    "languages": [
      "English"
    ],
    "similar_benchmarks": [],
    "resources": [
      "https://github.com/Paparare/toxic_benchmark_2024"
    ]
  },
  "purpose_and_intended_users": {
    "goal": "To enable consistent offensive language data labeling with high reliability while preventing biases against language minorities, hence protecting natural language diversity.",
    "audience": [
      "ML Researchers",
      "Industry Practitioners",
      "Model Developers",
      "Domain Experts"
    ],
    "tasks": [
      "Offensive Language Detection"
    ],
    "limitations": "The classifications of aggressive expressions are not definitive and may shift over time with evolving cultural norms.",
    "out_of_scope_uses": []
  },
  "data": {
    "source": "Two newly annotated datasets based on the proposed annotation benchmark, including various social media platforms.",
    "size": "1,942 pieces",
    "format": "N/A",
    "annotation": "Annotated based on prescriptive criteria to ensure high inter-rater reliability."
  },
  "methodology": {
    "methods": [
      "Statistical analysis",
      "LLM-based annotation",
      "Human annotation"
    ],
    "metrics": [
      "Inter-Annotator Reliability",
      "Accuracy of Annotation"
    ],
    "calculation": "Inter-Annotator reliability was calculated using Cohen’s Kappa and Gwet’s AC1.",
    "interpretation": "Higher scores indicate better agreement among annotators, reflecting consistency in labeling.",
    "baseline_results": null,
    "validation": "Validation was conducted through inter-rater reliability checks using multiple annotators."
  },
  "targeted_risks": {
    "risk_categories": [
      "Bias",
      "Accuracy"
    ],
    "atlas_risks": {
      "risks": [
        {
          "category": "Accuracy",
          "subcategory": [
            "Unrepresentative data",
            "Poor model accuracy"
          ]
        },
        {
          "category": "Fairness",
          "subcategory": [
            "Data bias"
          ]
        }
      ]
    },
    "demographic_analysis": "Analysis of bias in relation to different language varieties including casual and non-mainstream uses.",
    "harm": []
  },
  "ethical_and_legal_considerations": {
    "privacy_and_anonymity": "N/A",
    "data_licensing": "N/A",
    "consent_procedures": "N/A",
    "compliance_with_regulations": "N/A"
  }
}