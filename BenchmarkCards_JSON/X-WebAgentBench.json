{
  "benchmark_details": {
    "is_benchmark": true,
    "name": "X-WebAgentBench: A Multilingual Interactive Web Benchmark for Evaluating Global Agentic System",
    "abbreviation": "X-WebAgentBench",
    "overview": "X-WebAgentBench is a novel multilingual agent benchmark in an interactive web environment, which evaluates the planning and interaction performance of language agents across multiple languages, thereby contributing to the advancement of global agent intelligence.",
    "data_type": "text instructions",
    "domains": [
      "Natural Language Processing"
    ],
    "languages": [
      "English",
      "Chinese",
      "French",
      "Spanish",
      "German",
      "Greek",
      "Bulgarian",
      "Russian",
      "Turkish",
      "Arabic",
      "Vietnamese",
      "Thai",
      "Hindi",
      "Swahili",
      "Urdu"
    ],
    "similar_benchmarks": [],
    "resources": [
      "https://github.com/WPENGxs/X-WebAgentBench"
    ]
  },
  "purpose_and_intended_users": {
    "goal": "To fill the gap in evaluating language agents in multilingual contexts and provide actionable insights and recommendations to enhance language model performance in these settings.",
    "audience": [
      "ML Researchers",
      "Industry Practitioners",
      "Model Developers"
    ],
    "tasks": [
      "Interactive Agent Evaluation"
    ],
    "limitations": "The quality cannot be completely equivalent to that of experienced translators.",
    "out_of_scope_uses": []
  },
  "data": {
    "source": "WebShop Dataset",
    "size": "2,800 multilingual instructions and 589,946 product entries",
    "format": "JSON",
    "annotation": "Manual translation and quality checks by multilingual experts"
  },
  "methodology": {
    "methods": [
      "Human evaluation",
      "Automated metrics"
    ],
    "metrics": [
      "Task Score"
    ],
    "calculation": "Task scores are computed based on performance on interactive tasks.",
    "interpretation": "Higher task scores indicate better interactive performance of language agents in multilingual contexts.",
    "baseline_results": null,
    "validation": "Quality checks through manual rechecks and assessments using LLM metrics."
  },
  "targeted_risks": {
    "risk_categories": [
      "Fairness",
      "Privacy"
    ],
    "atlas_risks": {
      "risks": [
        {
          "category": "Fairness",
          "subcategory": [
            "Data bias"
          ]
        },
        {
          "category": "Privacy",
          "subcategory": [
            "Personal information in data"
          ]
        }
      ]
    },
    "demographic_analysis": "The benchmark includes diverse demographics across 14 languages.",
    "harm": [
      "Misrepresentation of multilingual capabilities"
    ]
  },
  "ethical_and_legal_considerations": {
    "privacy_and_anonymity": "Data is sourced from open-access WebShop and does not violate privacy regulations.",
    "data_licensing": "Data is open-source and freely accessible for academic research.",
    "consent_procedures": "Informed consent obtained from all participating annotators.",
    "compliance_with_regulations": "N/A"
  }
}