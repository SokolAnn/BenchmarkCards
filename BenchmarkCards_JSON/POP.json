{
  "benchmark_details": {
    "is_benchmark": true,
    "name": "Prompt Optimization Preference (POP) dataset",
    "abbreviation": "POP",
    "overview": "A large-scale Prompt Optimization Preference (POP) dataset of 30,000 prompt optimization preference examples, constructed by querying GPT-3.5-turbo and GPT-4 on naive prompts sampled from the Alpaca dataset and cross-validated by an external alignment model, GPT-4 self-check, and human experts. The dataset is intended to enable training of local LLM-based optimizers (FIPO) for model-agnostic prompt optimization.",
    "data_type": "text (naive prompts, optional naive responses, optional ground truth responses, and contrastive optimized prompt pairs)",
    "domains": [
      "Natural Language Processing"
    ],
    "languages": null,
    "similar_benchmarks": [
      "Alpaca dataset"
    ],
    "resources": [
      "https://github.com/LuJunru/FIPO_Project",
      "https://arxiv.org/abs/2402.11811"
    ]
  },
  "purpose_and_intended_users": {
    "goal": "To collect a large-scale prompt optimization preference dataset (POP) and develop Free-form Instruction-oriented Prompt Optimization (FIPO), a locally trained, model-agnostic optimizer that produces refined prompts to improve downstream model answer quality.",
    "audience": [],
    "tasks": [
      "Text Generation",
      "Question Answering",
      "Multiple-choice Question Answering",
      "Mathematical Reasoning",
      "Commonsense Reasoning"
    ],
    "limitations": "1) Overwhelmed Cheating Notes: FIPO sometimes provides overly detailed instructions ('cheating notes'), especially on mathematical questions. 2) Evaluation Metrics: Current evaluation primarily focuses on accuracy; interpretability, fairness, and ethical implications are not addressed. 3) Optimization of In-context Exemplars: FIPO does not optimize in-context examples and focuses only on task instructions.",
    "out_of_scope_uses": [
      "Optimization of in-context exemplars (FIPO focuses on optimizing task instructions only)",
      "Consideration of interpretability, fairness, and broader ethical implications (these are noted as future work)"
    ]
  },
  "data": {
    "source": "Naive prompts sampled from the Alpaca dataset (52K diverse instructions and corresponding responses from Text-davinci-003). Contrastive POP data collected by sending naive prompt, naive response, and ground truth response to GPT-3.5-turbo and GPT-4 to obtain chosen and rejected optimized prompts (xo+, xoâˆ’). GPT-4 responses are treated as ground truth responses when official human ground truth is unavailable.",
    "size": "30,000 examples",
    "format": "N/A",
    "annotation": "Optimized prompts generated by GPT-3.5-turbo and GPT-4; dataset quality cross-validated using an external alignment model UltraRM, GPT-4 self-judgement, and manual checking by human experts (reported win rates)."
  },
  "methodology": {
    "methods": [
      "Supervised Fine-tuning (SFT)",
      "Direct Preference Optimization (DPO)",
      "Identity Preference Optimization (IPO)",
      "Iterative Preference Learning (IPL)",
      "Cross-validation with UltraRM (alignment model), GPT-4 self-check, and human expert checking",
      "Automated evaluation on downstream benchmarks"
    ],
    "metrics": [
      "Accuracy",
      "Win Rate (percentage)"
    ],
    "calculation": "Accuracy is reported for downstream benchmarks in few-shot format using strict answering templates (correct answers / total). Dataset quality is measured by win-rate percentages indicating proportions where GPT-4-generated response or GPT-4-optimized prompt is better than alternatives (reported from UltraRM, GPT-4 self-check, and human experts).",
    "interpretation": "Higher accuracy on downstream benchmarks indicates more effective prompt optimization. Dataset quality is interpreted via win rates (the paper reports average win rates exceeding ~85% for response and prompt categories).",
    "baseline_results": "The paper compares FIPO to APE and PromptAgent and reports superior results on most downstream tests. Example results from Table 2: Llama2-7B weighted average improved from 41.73 (Naive Prompt) to 48.10 (FIPO Optimizer); Tulu2-13B from 52.53 to 54.79; Baichuan2-13B from 52.36 to 54.35. Detailed per-benchmark tables are provided in the paper.",
    "validation": "POP dataset validated via three cross-validation methods: critiques from external alignment model UltraRM, GPT-4 self-judgement, and manual checking by human experts (Table 1 reports win rates: UltraRM, GPT-4 self-check, Human Expert; average win rates exceed 85%)."
  },
  "targeted_risks": {
    "risk_categories": [
      "Privacy",
      "Accuracy",
      "Misuse",
      "Governance"
    ],
    "atlas_risks": {
      "risks": [
        {
          "category": "Privacy",
          "subcategory": [
            "Exposing personal information"
          ]
        },
        {
          "category": "Accuracy",
          "subcategory": [
            "Poor model accuracy"
          ]
        },
        {
          "category": "Misuse",
          "subcategory": [
            "Improper usage"
          ]
        },
        {
          "category": "Governance",
          "subcategory": [
            "Incomplete usage definition"
          ]
        }
      ]
    },
    "demographic_analysis": null,
    "harm": [
      "Privacy risk from exposing sensitive information to third-party LLM services (noted as a drawback of online ad-hoc APO that FIPO aims to mitigate)",
      "Provision of overly detailed 'cheating notes' in optimized prompts (noted as a limitation that may misalign with intended use)"
    ]
  },
  "ethical_and_legal_considerations": {
    "privacy_and_anonymity": "The paper notes privacy risks of online ad-hoc APO relying on external LLM services (exposing sensitive information to third-party systems) and states FIPO eliminates dependence on in-box model generators to reduce this privacy risk.",
    "data_licensing": "N/A",
    "consent_procedures": "N/A",
    "compliance_with_regulations": "N/A"
  }
}