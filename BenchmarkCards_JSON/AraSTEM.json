{
  "benchmark_details": {
    "is_benchmark": true,
    "name": "AraSTEM: A Native Arabic Multiple Choice Question Benchmark for Evaluating LLMs Knowledge In STEM Subjects",
    "abbreviation": "AraSTEM",
    "overview": "AraSTEM is a new Arabic multiple-choice question dataset aimed at evaluating LLMs knowledge in STEM subjects including Math, Science, Physics, Biology, Chemistry, Computer Science, and Medicine. It contains 11,637 questions across elementary to professional levels, requiring a deep understanding of scientific Arabic.",
    "data_type": "multiple-choice questions",
    "domains": [
      "Natural Language Processing",
      "Education"
    ],
    "languages": [
      "Arabic"
    ],
    "similar_benchmarks": [
      "Multilingual Massive Multitask Language Understanding (MMMLU)",
      "ArabicMMLU"
    ],
    "resources": [
      "https://huggingface.co/datasets/AraSTEM"
    ]
  },
  "purpose_and_intended_users": {
    "goal": "To provide a benchmark for evaluating the knowledge of Large Language Models (LLMs) in the Arabic language across STEM subjects.",
    "audience": [
      "ML Researchers",
      "Language Model Developers",
      "Educators"
    ],
    "tasks": [
      "Question Answering"
    ],
    "limitations": "N/A",
    "out_of_scope_uses": []
  },
  "data": {
    "source": "Multiple sources including scraping from MCQ websites, manual extraction from books, and LLM extraction from PDF files.",
    "size": "11,637 questions",
    "format": "N/A",
    "annotation": "Questions were manually annotated and sourced from credible educational materials."
  },
  "methodology": {
    "methods": [
      "Zero-shot evaluation",
      "Human evaluation"
    ],
    "metrics": [
      "Accuracy"
    ],
    "calculation": "Accuracy is calculated based on the proportion of correctly answered questions by the models across different subjects.",
    "interpretation": "Models are expected to demonstrate an understanding of scientific concepts in Arabic, with a higher accuracy indicating better performance.",
    "baseline_results": "The best-performing model achieved an accuracy of 56% on average.",
    "validation": "N/A"
  },
  "targeted_risks": {
    "risk_categories": [
      "Bias",
      "Safety"
    ],
    "atlas_risks": {
      "risks": []
    },
    "demographic_analysis": "N/A",
    "harm": []
  },
  "ethical_and_legal_considerations": {
    "privacy_and_anonymity": "N/A",
    "data_licensing": "N/A",
    "consent_procedures": "N/A",
    "compliance_with_regulations": "N/A"
  }
}