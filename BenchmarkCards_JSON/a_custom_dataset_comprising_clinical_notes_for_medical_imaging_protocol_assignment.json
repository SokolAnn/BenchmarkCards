{
  "benchmark_details": {
    "is_benchmark": true,
    "name": "a custom dataset comprising clinical notes for medical imaging protocol assignment",
    "abbreviation": "N/A",
    "overview": "We present an approach to evaluate the performance and trustworthiness of a GPT3.5 model for medical image protocol assignment. Our evaluation dataset consists of 4,700 physician entries across 11 imaging protocol classes spanning the entire head.",
    "data_type": "clinical physician order text (reason for exam)",
    "domains": [
      "Medical Imaging",
      "Natural Language Processing"
    ],
    "languages": [],
    "similar_benchmarks": [],
    "resources": []
  },
  "purpose_and_intended_users": {
    "goal": "Evaluate the performance and trustworthiness of GPT3.5 for medical image protocol assignment, compare it with fine-tuned models (BERT, BioBERT, RoBERTa) and a radiologist, analyze explanations and word importance, and assess calibration.",
    "audience": [],
    "tasks": [
      "Text Classification",
      "Explainability",
      "Model Calibration"
    ],
    "limitations": "Dataset comprised of neuroradiologic orders from a single center and may be limited in its representation of the racial, social, and ethnic diversity of other regions; limited number of protocols (the ten/eleven most commonly used protocols) which may not capture the full breadth of clinical protocols; protocols assigned by multiple radiologists with varying experience (inter-operator variability); dataset not publicly available; evaluation dataset size (4,730 entries) may still limit model performance.",
    "out_of_scope_uses": []
  },
  "data": {
    "source": "Deidentified order entries and assigned protocols for magnetic resonance (MR) neuroradiology studies conducted at the authors' institution between June 2018 and July 2021. Each row includes the 'reason for exam', patient age and gender, and the protocol assigned by the radiologist.",
    "size": "4,730 recorded notes",
    "format": "N/A",
    "annotation": "Expert-annotated imaging protocols reviewed by an experienced radiologist (ET) with 10 years of experience."
  },
  "methodology": {
    "methods": [
      "Prompting GPT3.5 via OpenAI API to output predicted protocol and explanation",
      "Fine-tuning and evaluation of pretrained models (BERT, BioBERT, RoBERTa) using HuggingFace Transformers",
      "Integrated gradients for word importance attribution for BERT",
      "Radiologist (human) word-importance scoring",
      "Model calibration evaluation using log probabilities and binning",
      "Error analysis via review of GPT3.5 explanations by a radiologist"
    ],
    "metrics": [
      "Precision",
      "Recall",
      "F1 Score",
      "Accuracy",
      "Calibration (accuracy per confidence bin using log probabilities)"
    ],
    "calculation": "Predicted protocol from each model was compared against ground truth to measure precision, recall, and accuracy for each class. Log probabilities from GPT3.5 were transformed into a confidence value and entries were divided into bins in [0,1] with intervals of 0.1; for each bin accuracy was calculated. Integrated gradients aggregated average attribution values across texts per protocol; GPT3.5 word importance used a weighted average (1, 0.66, 0.33 for top three words). Words appearing in fewer than five texts were filtered out for reliability.",
    "interpretation": "Higher F1 scores and accuracy indicate better protocol assignment performance. Calibration curves closer to the diagonal indicate better alignment between predicted probabilities and observed accuracy; well-calibrated models give more reliable confidence estimates. Agreement of top words with radiologist indicates better interpretability/clinical relevance.",
    "baseline_results": "Weighted average F1 scores reported in the paper: BERT 0.89, BioBERT 0.92, RoBERTa 0.90, GPT3.5 0.72. Per-class F1 scores and accuracies are provided (e.g., per-table values; GPT3.5 generally underperformed compared to fine-tuned models).",
    "validation": "Radiologist review by an experienced radiologist (ET) for data quality and for reviewing GPT3.5 explanations; model outputs compared to radiologist-assigned ground truth; calibration evaluated via confidence bins; word-importance lists filtered to exclude words appearing in fewer than five texts."
  },
  "targeted_risks": {
    "risk_categories": [
      "Accuracy",
      "Fairness",
      "Privacy",
      "Explainability",
      "Robustness",
      "Value Alignment"
    ],
    "atlas_risks": {
      "risks": [
        {
          "category": "Accuracy",
          "subcategory": [
            "Data contamination",
            "Unrepresentative data"
          ]
        },
        {
          "category": "Fairness",
          "subcategory": [
            "Data bias"
          ]
        },
        {
          "category": "Privacy",
          "subcategory": [
            "Data privacy rights alignment"
          ]
        },
        {
          "category": "Explainability",
          "subcategory": [
            "Unexplainable output"
          ]
        },
        {
          "category": "Robustness",
          "subcategory": [
            "Hallucination"
          ]
        },
        {
          "category": "Value Alignment",
          "subcategory": [
            "Over- or under-reliance"
          ]
        }
      ]
    },
    "demographic_analysis": "The dataset comprised of neuroradiologic orders from a single center and may be limited in its representation of the racial, social, and ethnic diversity of other regions.",
    "harm": [
      "Safety risks in clinical settings due to systematic errors (incomplete understanding of protocols, anatomy, medical terminology)",
      "Overconfidence in model explanations leading to reliance on incorrect predictions"
    ]
  },
  "ethical_and_legal_considerations": {
    "privacy_and_anonymity": "The datasets utilized during this study are not publicly available due to reasonable privacy and security concerns. The data is not easily redistributable to researchers other than those engaged in Institutional Review Board-approved research collaborations with Stanford University.",
    "data_licensing": "N/A",
    "consent_procedures": "Study conducted with approval of the Stanford Institutional Review Board and under a waiver of informed consent.",
    "compliance_with_regulations": "Study approved by Stanford Institutional Review Board (IRB); data sharing restricted to IRB-approved collaborations."
  }
}