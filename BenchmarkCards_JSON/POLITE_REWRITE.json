{
  "benchmark_details": {
    "is_benchmark": true,
    "name": "POLITE REWRITE",
    "abbreviation": "N/A",
    "overview": "POLITE REWRITE â€“ a dataset for polite language rewrite. The released dataset has 10K polite sentence rewrites annotated collaboratively by GPT-3.5 and human, which can be used as gold standard for training, validation and test; and 100K high-quality polite sentence rewrites by GPT-3.5 without human review.",
    "data_type": "text (source sentence, polite rewrite sentence pairs)",
    "domains": [
      "Natural Language Processing"
    ],
    "languages": [
      "English"
    ],
    "similar_benchmarks": [
      "GYAFC"
    ],
    "resources": [
      "N/A"
    ]
  },
  "purpose_and_intended_users": {
    "goal": "To provide a novel dataset for polite language rewrite, a challenging sentence rewrite task to guide a generation model against verbal abuse and offensive expression.",
    "audience": [
      "ML Researchers",
      "Model Developers"
    ],
    "tasks": [
      "Sentence Rewrite",
      "Style Transfer"
    ],
    "limitations": "N/A",
    "out_of_scope_uses": []
  },
  "data": {
    "source": "Impolite sentences selected from web using a sentiment analyzer (a fine-tuned UNILM on SST-2). Candidate sentences are screened with GPT-3.5 (8-shot in-context learning). Rewrites are generated by GPT-3.5 and then reviewed/edited by human annotators in a multi-stage human-in-the-loop procedure (Stage 0: GPT-3.5 8-shot seed; Stage 1: human accept/modify; Stage 2: GPT-3.5 fine-tuning (davinci-002) on human edits; Stage 3: GPT-3.5 generation). The released dataset contains 10K gold-standard sentence pairs (human+GPT-3.5 collaborative annotation) and 100K silver-standard pairs (GPT-3.5 without human review).",
    "size": "10,000 sentence pairs (gold); 100,000 sentence pairs (silver)",
    "format": "N/A",
    "annotation": "Human-GPT-3.5 collaborative annotation: GPT-3.5 generates rewrites (8-shot and fine-tuned davinci-002); human annotators accept, modify, or rewrite GPT-3.5 outputs. Gold: 10K reviewed by humans; Silver: 100K generated by GPT-3.5 without human review."
  },
  "methodology": {
    "methods": [
      "Human-GPT-3.5 collaborative annotation",
      "Automated metric evaluation (BLEU)",
      "Human evaluation (English native speakers scoring outputs 1-5)",
      "Neural model baselines (Transformer, BART) trained/fine-tuned on dataset"
    ],
    "metrics": [
      "BLEU",
      "Human evaluation score (1-5)",
      "Pearson Correlation"
    ],
    "calculation": "BLEU reported (sentence-level BLEU as referenced by 'Human-(sentence-level)BLEU'). Human judges assign scores from 1 to 5 (5 best, 1 worst). Pearson correlation computed between human-human judges and between human scores and sentence-level BLEU.",
    "interpretation": "Higher BLEU indicates closer match to reference polite rewrites. Human judge score: 5 is best polite rewrite, 1 is worst. Pearson correlation is used to assess agreement between judges and between human judgments and BLEU.",
    "baseline_results": "Transformer (Gold): BLEU 6.2, Human Eval 1.8; Transformer (Silver+Gold): BLEU 23.9, Human Eval 2.8; BART (Gold): BLEU 34.9, Human Eval 3.2; BART (Silver+Gold): BLEU 37.5, Human Eval 3.9.",
    "validation": "10K gold data randomly split into 5K train / 3K development / 2K test. Silver data (100K) used as additional training data. Quality validation: 500 impolite sentences from the 10K were annotated by an additional human and another English native speaker attempted to distinguish purely-human vs collaborative annotations; over 90% of cases were indistinguishable. Human evaluation on 100 sampled test sentences by two native speakers reported correlations (human-human 0.84, human-BLEU 0.68)."
  },
  "targeted_risks": {
    "risk_categories": [
      "Safety"
    ],
    "atlas_risks": {
      "risks": [
        {
          "category": "Value Alignment",
          "subcategory": [
            "Toxic output"
          ]
        }
      ]
    },
    "demographic_analysis": null,
    "harm": [
      "Verbal abuse",
      "Offensive expression"
    ]
  },
  "ethical_and_legal_considerations": {
    "privacy_and_anonymity": "N/A",
    "data_licensing": "N/A",
    "consent_procedures": "N/A",
    "compliance_with_regulations": "N/A"
  }
}