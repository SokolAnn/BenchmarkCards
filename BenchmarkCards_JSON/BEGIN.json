{
  "benchmark_details": {
    "is_benchmark": true,
    "name": "Benchmark for Evaluation of Grounded INteraction (BEGIN)",
    "abbreviation": "BEGIN",
    "overview": "BEGIN is a benchmark for assessing attribution in knowledge-based dialog systems, comprising 12k dialogue turns generated by neural dialogue systems. It collects human annotations on the attribution of generated responses to the given background information and evaluates multiple existing automatic metrics.",
    "data_type": "dialogue turns",
    "domains": [
      "Natural Language Processing"
    ],
    "languages": [
      "English"
    ],
    "similar_benchmarks": [],
    "resources": [
      "https://github.com/google/BEGIN-dataset"
    ]
  },
  "purpose_and_intended_users": {
    "goal": "To assess attribution in knowledge-based dialog systems and to promote the development of robust evaluation metrics.",
    "audience": [
      "ML Researchers",
      "Domain Experts"
    ],
    "tasks": [
      "Dialogue Generation",
      "Attribution Assessment"
    ],
    "limitations": "N/A",
    "out_of_scope_uses": []
  },
  "data": {
    "source": "Responses generated by four language-model-based dialogue systems trained on three different knowledge-grounded dialogue corpora.",
    "size": "12,000 dialogue turns",
    "format": "N/A",
    "annotation": "Human annotations assessing attribution of responses."
  },
  "methodology": {
    "methods": [
      "Human evaluation",
      "Automatic metrics evaluation"
    ],
    "metrics": [
      "BLEU Score",
      "ROUGE-L",
      "F1 Score",
      "BERTScore",
      "BARTScore",
      "Q2"
    ],
    "calculation": "Metrics are calculated based on the correspondence of generated responses to the dialogue knowledge.",
    "interpretation": "Higher scores indicate better attribution; however, many metrics fail to adequately distinguish between different response types.",
    "baseline_results": "N/A",
    "validation": "Used majority vote from three annotators for annotation quality control."
  },
  "targeted_risks": {
    "risk_categories": [
      "Accuracy",
      "Fairness",
      "Robustness"
    ],
    "atlas_risks": {
      "risks": [
        {
          "category": "Accuracy",
          "subcategory": [
            "Unrepresentative data"
          ]
        },
        {
          "category": "Fairness",
          "subcategory": [
            "Data bias"
          ]
        },
        {
          "category": "Robustness",
          "subcategory": [
            "Evasion attack"
          ]
        }
      ]
    },
    "demographic_analysis": "N/A",
    "harm": []
  },
  "ethical_and_legal_considerations": {
    "privacy_and_anonymity": "N/A",
    "data_licensing": "N/A",
    "consent_procedures": "N/A",
    "compliance_with_regulations": "N/A"
  }
}