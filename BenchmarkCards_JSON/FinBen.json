{
  "benchmark_details": {
    "is_benchmark": true,
    "name": "FinBen: An Holistic Financial Benchmark for Large Language Models",
    "abbreviation": "FinBen",
    "overview": "FinBen is the first extensive open-source evaluation benchmark for LLMs in the financial domain, including 36 datasets spanning 24 financial tasks and covering seven aspects: information extraction (IE), textual analysis (TA), question answering (QA), text generation (TG), risk management (RM), forecasting (FO), and decision-making (DM). FinBen introduces agent-based and Retrieval-Augmented Generation (RAG) evaluation, and three novel open-source datasets for text summarization, question answering, and stock trading.",
    "data_type": "question-answering pairs; text summarization; text classification; time-series (historical stock prices); multimodal (news text + time-series)",
    "domains": [
      "Natural Language Processing",
      "Finance"
    ],
    "languages": [
      "English"
    ],
    "similar_benchmarks": [
      "PIXIU",
      "FinanceBench",
      "BizBench",
      "CFBenchmark",
      "Fin-Eva"
    ],
    "resources": [
      "https://github.com/The-FinAI/PIXIU",
      "https://arxiv.org/abs/2402.12659",
      "https://sites.google.com/nlg.csie.ntu.edu.tw/finnlp-agentscen"
    ]
  },
  "purpose_and_intended_users": {
    "goal": "To provide a comprehensive open-source evaluation benchmark for LLMs in the financial domain, comprising 36 datasets across 24 tasks to assess LLM performance across seven aspects: information extraction, textual analysis, question answering, text generation, risk management, forecasting, and decision-making.",
    "audience": [
      "ML Researchers",
      "Model Developers",
      "Industry Practitioners",
      "Financial Domain Experts"
    ],
    "tasks": [
      "Named Entity Recognition",
      "Relation Extraction",
      "Causal Classification",
      "Causal Detection",
      "Numeric Labeling",
      "Textual Analogy Parsing",
      "Sentiment Analysis",
      "News Headline Classification",
      "Argument Unit Classification",
      "Argument Relation Detection",
      "Multi-class Classification",
      "Deal Completeness Classification",
      "ESG Issue Identification",
      "Question Answering",
      "Long-form Question Answering",
      "Multi-turn Question Answering",
      "Text Summarization",
      "Stock Movement Prediction",
      "Credit Scoring",
      "Fraud Detection",
      "Financial Distress Identification",
      "Claim Analysis",
      "Stock Trading (Agent-based Decision-Making)"
    ],
    "limitations": "Limited dataset sizes for some tasks; computational constraints limited evaluation (e.g., constrained to LLaMA 70B in some evaluations); tasks are based on American market data and English texts which may limit global applicability; potential misuse such as financial misinformation or unethical market influence.",
    "out_of_scope_uses": [
      "Not to be used as financial, legal, or investment advice"
    ]
  },
  "data": {
    "source": "Three primary sources: 1) open-sourced datasets from existing studies transformed into instruction-response pairs; 2) datasets from existing evaluation benchmarks (e.g., PIXIU) already transformed into instruction tuning format; 3) novel datasets introduced in this paper (EDTSum, FinTrade, Regulations).",
    "size": "36 datasets in total spanning 24 tasks. Examples: EDTSum: 2,000 news; FinTrade: 3,384 examples (stock trading); Regulations: 254 examples (long-form QA). Test set sizes for each dataset are provided in Table 2 of the paper.",
    "format": "Instruction-response pairs (evaluation uses test splits); datasets originate from various source formats and have been transformed into an instruction tuning / evaluation format for zero-shot evaluation.",
    "annotation": "Combination of dataset-specific annotations from source datasets and manual curation: EDTSum headlines were manually selected and cleaned; Regulations maps queries to relevant regulatory articles; other datasets retain original annotations from their sources. All datasets transformed into instruction-response format for evaluation."
  },
  "methodology": {
    "methods": [
      "Zero-shot evaluation",
      "Few-shot results (reported from previous papers)",
      "Agent-based evaluation",
      "Retrieval-Augmented Generation (RAG) evaluation",
      "Automated metrics (dataset-specific)"
    ],
    "metrics": [
      "F1 Score",
      "Entity F1 Score",
      "Exact Match Accuracy (EM Accuracy)",
      "Accuracy",
      "ROUGE (ROUGE-1, ROUGE-2, ROUGE-L)",
      "BERTScore",
      "BART Score",
      "Cumulative Return (CR)",
      "Sharpe Ratio (SR)",
      "Daily Volatility (DV)",
      "Annualized Volatility (AV)",
      "Maximum Drawdown (MD)",
      "Matthews Correlation Coefficient (MCC)",
      "RMSE",
      "Average F1"
    ],
    "calculation": "Evaluations are performed on test sets; most results reported as averages over three runs. Trading performance metrics are reported with 95% confidence intervals. Dataset-specific metric calculations follow standard definitions (e.g., F1, ROUGE, BERTScore, MCC) as indicated in Table 2.",
    "interpretation": "Higher values of F1, Entity F1, Accuracy, ROUGE, BERTScore, and BART Score indicate better performance on NLP tasks. For forecasting and trading, higher Cumulative Return and Sharpe Ratio and lower Daily/Annualized Volatility and Maximum Drawdown indicate better performance. Lower RMSE indicates better performance where applicable.",
    "baseline_results": "Baseline evaluations include 15 representative LLMs (e.g., GPT-4, ChatGPT, Gemini, LLaMA2, LLaMA3, FinMA-7B, FinGPT). Examples of reported results: GPT-4 NER EntityF1 0.83 (Table 3), GPT-4 FinQA EMAcc 0.63 (Table 3), GPT-4 trading performance Cumulative Return 28.19% ±25.27 and Sharpe Ratio 1.51 ±1.08 (Table 4). Detailed per-model per-dataset results are provided in Tables 3-5.",
    "validation": "Benchmark validated via systematic evaluation of 15 LLMs and by hosting the FinLLM shared task (FinNLP-AgentScen workshop at IJCAI-2024) with participating teams. A Data Leakage Test (DLT) was proposed for the shared task to detect potential data leakage; trading metrics reported with confidence intervals; authors provide code, data, and instructions to reproduce evaluations."
  },
  "targeted_risks": {
    "risk_categories": [
      "Misuse",
      "Legal Compliance",
      "Accuracy",
      "Governance"
    ],
    "atlas_risks": {
      "risks": [
        {
          "category": "Misuse",
          "subcategory": [
            "Spreading disinformation"
          ]
        },
        {
          "category": "Legal Compliance",
          "subcategory": [
            "Legal accountability"
          ]
        },
        {
          "category": "Accuracy",
          "subcategory": [
            "Data contamination"
          ]
        },
        {
          "category": "Governance",
          "subcategory": [
            "Unrepresentative risk testing",
            "Lack of testing diversity"
          ]
        }
      ]
    },
    "demographic_analysis": null,
    "harm": [
      "Financial misinformation",
      "Unethical market influence"
    ]
  },
  "ethical_and_legal_considerations": {
    "privacy_and_anonymity": "The authors state that all raw data used are publicly available and do not contain personal information; efforts were made to ensure privacy and anonymization where applicable.",
    "data_licensing": "Datasets compiled within FinBen are shared under the MIT license; Table 2 lists licenses for individual datasets (examples include CC BY-SA 3.0, CC BY-NC 4.0, MIT License, CC0 1.0, Public).",
    "consent_procedures": "N/A",
    "compliance_with_regulations": "N/A"
  }
}